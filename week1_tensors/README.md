# Week 1: Linear Regression â€” Teoriden PratiÄŸe

**NovaDev v1.0 - Pratik Sprint**

> "Teori bitti. Åžimdi kod zamanÄ±. 45 dakikada baÅŸtan sona, 5 gÃ¼nde kusursuz."

---

## ðŸŽ¯ Hedef

**Linear Regression**'u sÄ±fÄ±rdan yazÄ±p, eÄŸitim sÃ¼recini anlamak:
- âœ… Manuel gradient descent (autograd'Ä± Ã§Ä±plak gÃ¶rmek)
- âœ… nn.Module ile temiz kod
- âœ… Train/Val split + Early stopping
- âœ… L2 regularization
- âœ… **Val MSE < 0.5**

**Teori BaÄŸlantÄ±sÄ±:** Week 0'da Ã¶ÄŸrendiklerin artÄ±k KOD!

---

## ðŸ”¥ (A) BUGÃœN â€” 45 DakikalÄ±k HÄ±zlÄ± Pratik

**AmaÃ§:** Linear regression'u baÅŸtan sona koÅŸ, metrik kaydet, mini rapor Ã§Ä±kar.

### â±ï¸ Timeline

#### 0-5 dk: Setup
```bash
# Venv aktif et
source .venv/bin/activate
cd /Users/onur/code/novadev-protocol

# Week 1 klasÃ¶rÃ¼ne geÃ§
cd week1_tensors
```

#### 5-15 dk: Data & Manuel GD
```bash
# Sentetik veri Ã¼ret
python data_synth.py
# Output: data/week1_linreg.pt

# Manuel gradient descent
python linreg_manual.py
# Output: Manuel GD ile MSE hesabÄ±
```

**Ne Ã¶ÄŸreniyorsun:**
- Sentetik veri oluÅŸturma (y = wx + b + noise)
- Forward pass (Å· = Xw + b)
- Loss hesabÄ± (MSE)
- Backward pass (autograd)
- Manuel parametre gÃ¼ncelleme (w â† w - Î·âˆ‡L)

#### 15-30 dk: nn.Module + Training Loop
```bash
# nn.Module ile temiz kod
python linreg_module.py

# Train/Val split + Early stopping
python train.py
# Output: outputs/model.pt, outputs/metrics.json
```

**Ne Ã¶ÄŸreniyorsun:**
- nn.Module yapÄ±sÄ±
- DataLoader kullanÄ±mÄ±
- Train/Val split
- Early stopping implementasyonu
- L2 regularization (weight_decay)

#### 30-40 dk: Test & Lint
```bash
# Testleri koÅŸ
pytest -q tests/test_linreg.py

# Linter kontrol
ruff check week1_tensors/
```

**Ne Ã¶ÄŸreniyorsun:**
- Pytest ile model testi
- Convergence testi
- L2 regularization etkisi testi

#### 40-45 dk: Mini Rapor
```bash
# KÄ±sa rapor oluÅŸtur
cat > week1_summary.md << EOF
# Week 1 - HÄ±zlÄ± Sprint Ã–zeti

## KonfigÃ¼rasyon
- LR: 5e-3
- L2: 1e-3
- Batch: 32
- Patience: 5

## SonuÃ§lar
- Train MSE: 0.12
- Val MSE: 0.18
- Epochs: 25 (early stopped)

## GÃ¶zlem
- Early stopping Ã§alÄ±ÅŸtÄ± (epoch 25'te)
- L2 olmadan overfit baÅŸlÄ±yordu
- Val MSE < 0.5 âœ“
EOF

# Commit
git add .
git commit -m "day1: linreg end-to-end, val MSE=0.18"
```

### âœ… Definition of Done (BugÃ¼n)

```
â–¡ data/week1_linreg.pt oluÅŸturuldu
â–¡ Val MSE < 0.5
â–¡ pytest yeÅŸil
â–¡ week1_summary.md var
â–¡ Git commit yapÄ±ldÄ±
```

---

## ðŸš€ (B) Week 1 â€” 5 GÃ¼nlÃ¼k Sprint (2-3 saat/gÃ¼n)

### ðŸ“… GÃ¼n 1: E2E Temel + Metrik GÃ¼nlÃ¼ÄŸÃ¼

**Hedef:** TÃ¼m pipeline'Ä± baÅŸtan sona koÅŸ, kayÄ±t sistemi kur

**GÃ¶revler:**
```bash
# 1. Sentetik veri oluÅŸtur
python data_synth.py

# 2. Manuel GD
python linreg_manual.py

# 3. nn.Module
python linreg_module.py

# 4. Full training
python train.py
```

**KayÄ±t Sistemi:**
```bash
# outputs/exp_log.csv oluÅŸtur
echo "exp_id,lr,l2,batch,patience,train_mse,val_mse,epochs,stopped_early" > outputs/exp_log.csv
```

**Kabul Kriteri:**
- âœ… Val MSE < 0.5
- âœ… Testler yeÅŸil
- âœ… exp_log.csv ilk satÄ±rÄ± dolu

**Ã‡Ä±ktÄ±:**
```bash
git commit -m "day1: E2E pipeline + logging system"
```

---

### ðŸ“… GÃ¼n 2: Early Stopping + LR Schedule

**Hedef:** Regularization stratejilerini ekle ve etkilerini Ã¶lÃ§

**GÃ¶revler:**

1. **Early Stopping Ekle** (train.py'da)
```python
best_val_loss = float('inf')
patience_counter = 0
patience = 3  # â† Ekle

for epoch in range(epochs):
    # ... train loop
    
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        patience_counter = 0
        save_checkpoint()
    else:
        patience_counter += 1
        
    if patience_counter >= patience:
        print(f"Early stopping at epoch {epoch}")
        break
```

2. **LR Schedule Ekle**
```python
from torch.optim.lr_scheduler import ReduceLROnPlateau

scheduler = ReduceLROnPlateau(
    optimizer, 
    mode='min',
    factor=0.5,
    patience=5,
    verbose=True
)

# Training loop'ta
scheduler.step(val_loss)
```

3. **3 Deney KoÅŸ**
```bash
# Experiment 1: LR=1e-2
python train.py --lr 1e-2 --l2 1e-3 --exp-id exp001

# Experiment 2: LR=5e-3
python train.py --lr 5e-3 --l2 1e-3 --exp-id exp002

# Experiment 3: LR=1e-3
python train.py --lr 1e-3 --l2 1e-3 --exp-id exp003
```

**Kabul Kriteri:**
- âœ… Early stopping tetikliyor
- âœ… LR schedule Ã§alÄ±ÅŸÄ±yor (log'da gÃ¶rÃ¼nÃ¼yor)
- âœ… exp_log.csv'de 3 satÄ±r var
- âœ… En iyi config belirlendi

**Ã‡Ä±ktÄ±:**
```bash
git commit -m "day2: early stopping + LR schedule, best: lr=5e-3"
```

---

### ðŸ“… GÃ¼n 3: Ã–lÃ§ekleme Ablation'Ä±

**Hedef:** Feature scaling'in etkisini Ã¶lÃ§

**GÃ¶revler:**

1. **StandardScaler Ekle**
```python
from sklearn.preprocessing import StandardScaler

# Train'de Ã¶ÄŸren
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)

# Val/Test'e uygula (FIT deÄŸil!)
X_val_scaled = scaler.transform(X_val)
```

2. **Ä°ki KoÅŸu KarÅŸÄ±laÅŸtÄ±r**
```bash
# Scaling YOK
python train.py --no-scaling --exp-id exp_no_scale

# Scaling VAR
python train.py --scaling --exp-id exp_with_scale
```

3. **Analiz Yap**
```markdown
## Scaling Ablation

### SonuÃ§lar
| Config       | Train MSE | Val MSE | Epochs | Time  |
|--------------|-----------|---------|--------|-------|
| No Scaling   | 0.25      | 0.35    | 45     | 12s   |
| With Scaling | 0.12      | 0.18    | 25     | 8s    |

### GÃ¶zlem
- Scaling ile convergence **HIZLI** (45â†’25 epoch)
- MSE **DÃœÅžÃœK** (0.35â†’0.18)
- Training **STABIL** (loss eÄŸrisi smooth)

### Neden?
â†’ Week 0 theory: Loss yÃ¼zeyi yuvarlanÄ±r (condition number â†“)
â†’ GD zikzak yapmaz, doÄŸrudan iner
```

**Kabul Kriteri:**
- âœ… Her iki koÅŸu bitti
- âœ… KarÅŸÄ±laÅŸtÄ±rma tablosu var
- âœ… "Neden?" aÃ§Ä±klamasÄ± Week 0'a referans veriyor

**Ã‡Ä±ktÄ±:**
```bash
git commit -m "day3: scaling ablation, 2x faster convergence"
```

---

### ðŸ“… GÃ¼n 4: Hata EÄŸrisi + Over/Underfit TeÅŸhisi

**Hedef:** Train/Val loss gÃ¶rselleÅŸtir, overfit Ã¶rneÄŸi gÃ¶ster

**GÃ¶revler:**

1. **Loss Curve Plotter**
```python
import matplotlib.pyplot as plt

def plot_loss_curves(train_losses, val_losses, save_path):
    plt.figure(figsize=(10, 6))
    plt.plot(train_losses, label='Train Loss', linewidth=2)
    plt.plot(val_losses, label='Val Loss', linewidth=2)
    plt.xlabel('Epoch')
    plt.ylabel('MSE')
    plt.title('Training & Validation Loss')
    plt.legend()
    plt.grid(True, alpha=0.3)
    plt.savefig(save_path, dpi=150, bbox_inches='tight')
    plt.close()
```

2. **Overfit Ã–rneÄŸi Tetikle**
```bash
# L2 YOK, Epoch YÃœKSEk
python train.py --l2 0.0 --epochs 200 --exp-id exp_overfit

# L2 VAR (normal)
python train.py --l2 1e-3 --epochs 100 --exp-id exp_normal
```

3. **Grafikleri OluÅŸtur**
```bash
python visualize_losses.py
# Output: 
#   outputs/loss_curve_overfit.png
#   outputs/loss_curve_normal.png
```

**Beklenen Grafik (Overfit):**
```
MSE
  â”‚
  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•²___ Train (mÃ¼kemmel)
  â”‚            â•²
  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•±â”€â”€ Val (kÃ¶tÃ¼leÅŸiyor)
  â”‚             â†‘
  â”‚        Overfit baÅŸladÄ±
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Epoch
```

**Beklenen Grafik (Normal):**
```
MSE
  â”‚
  â”‚â•²___
  â”‚    â•²___ Train
  â”‚    â•²___
  â”‚        â•²___ Val (parallel)
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Epoch
```

**Kabul Kriteri:**
- âœ… 2 grafik oluÅŸturuldu (overfit + normal)
- âœ… Overfit Ã¶rneÄŸinde val loss yÃ¼kseliyor
- âœ… Normal Ã¶rnekte val loss stabil/dÃ¼ÅŸÃ¼yor
- âœ… Grafikler outputs/ klasÃ¶rÃ¼nde

**Ã‡Ä±ktÄ±:**
```bash
git commit -m "day4: loss curves + overfit example"
```

---

### ðŸ“… GÃ¼n 5: Rapor + Temizlik

**Hedef:** Profesyonel rapor yaz, test ekle

**GÃ¶revler:**

1. **KapsamlÄ± Rapor (week1_report.md)**

```markdown
# Week 1: Linear Regression â€” Final Rapor

## ðŸŽ¯ Hedef
Val MSE < 0.5 baÅŸarmak ve training sÃ¼recini optimize etmek.

## ðŸ“Š Deneyler

### Ã–zet Tablo
| Exp ID  | LR    | L2    | Scaling | Early Stop | Train MSE | Val MSE | Epochs |
|---------|-------|-------|---------|------------|-----------|---------|--------|
| exp001  | 1e-2  | 1e-3  | âœ“       | âœ“          | 0.15      | 0.22    | 35     |
| exp002  | 5e-3  | 1e-3  | âœ“       | âœ“          | 0.12      | 0.18    | 25     |
| exp003  | 1e-3  | 1e-3  | âœ“       | âœ“          | 0.18      | 0.25    | 50     |
| exp_ovf | 5e-3  | 0.0   | âœ“       | âœ—          | 0.05      | 0.45    | 200    |
| no_scl  | 5e-3  | 1e-3  | âœ—       | âœ“          | 0.25      | 0.35    | 45     |

### En Ä°yi KonfigÃ¼rasyon â­
```
LR: 5e-3
L2: 1e-3
Batch: 32
Patience: 5
Scaling: StandardScaler
```

**SonuÃ§:** Val MSE = 0.18 âœ“

## ðŸ§  Neden Ä°ÅŸe YaradÄ±? (Week 0 Teori BaÄŸlantÄ±sÄ±)

### 1. Feature Scaling
**Teori (theory_core_concepts.md, BÃ¶lÃ¼m 8):**
- FarklÄ± Ã¶lÃ§ekler â†’ Eliptik loss yÃ¼zeyi
- Standardizasyon â†’ Yuvarlak yÃ¼zey
- GD zikzak yapmaz â†’ HÄ±zlÄ± yakÄ±nsama

**Pratik:**
- Scaling ile 45â†’25 epoch (2x hÄ±zlÄ±)
- Val MSE: 0.35â†’0.18 (daha iyi)

### 2. L2 Regularization
**Teori (theory_core_concepts.md, BÃ¶lÃ¼m 11):**
- L2 â† Gaussian prior (MAP)
- BÃ¼yÃ¼k aÄŸÄ±rlÄ±klarÄ± cezalar
- Overfit'i Ã¶nler

**Pratik:**
- L2=0: Val MSE patladÄ± (0.45)
- L2=1e-3: Val MSE stabil (0.18)

### 3. Early Stopping
**Teori (theory_core_concepts.md, BÃ¶lÃ¼m 7):**
- Implicit regularization
- Val kÃ¶tÃ¼leÅŸince dur

**Pratik:**
- Patience=5 â†’ Epoch 25'te durdu
- Overfit Ã¶nlendi

### 4. Learning Rate
**Teori (theory_closure.md, BÃ¶lÃ¼m 3):**
- Ã‡ok bÃ¼yÃ¼k â†’ SalÄ±nÄ±m
- Ã‡ok kÃ¼Ã§Ã¼k â†’ YavaÅŸ
- Sweet spot: 5e-3

**Pratik:**
- 1e-2: SalÄ±nÄ±mlÄ± (val: 0.22)
- 5e-3: Optimal (val: 0.18) â­
- 1e-3: YavaÅŸ (val: 0.25)

## ðŸ“ˆ Grafikler

### Loss Curves
![Loss Curves](outputs/loss_curve_normal.png)

### Overfit Example
![Overfit](outputs/loss_curve_overfit.png)

**GÃ¶zlem:** L2=0 koÅŸusunda val loss epoch 50'den sonra yÃ¼kseliyor.

## ðŸ› KarÅŸÄ±laÅŸÄ±lan Sorunlar

### Problem 1: Ä°lk Denemede NaN
**Sebep:** LR Ã§ok bÃ¼yÃ¼k (0.1)
**Ã‡Ã¶zÃ¼m:** LR'Ä± 0.005'e indirdim
**Teori:** theory_closure.md BÃ¶lÃ¼m 3, "LR Ã§ok bÃ¼yÃ¼k" semptomlarÄ±

### Problem 2: Val Loss Unstable
**Sebep:** Batch size Ã§ok kÃ¼Ã§Ã¼k (8)
**Ã‡Ã¶zÃ¼m:** Batch'i 32'ye Ã§Ä±kardÄ±m
**Teori:** Mini-batch GD dengesi

### Problem 3: Convergence YavaÅŸ
**Sebep:** Feature scaling unutulmuÅŸ
**Ã‡Ã¶zÃ¼m:** StandardScaler eklendi
**Teori:** theory_core_concepts.md BÃ¶lÃ¼m 8

## ðŸ’­ Ã–ÄŸrendiklerim

1. **Theory â†’ Practice baÄŸlantÄ±sÄ± GÃœÃ‡LÃœ**
   - Week 0'da Ã¶ÄŸrendiÄŸim her kavramÄ± burada gÃ¶rdÃ¼m
   - MSE â† Gaussian MLE (gerÃ§ekten!)
   - L2 â† Gaussian prior (MAP)

2. **Debugging sistematik yapÄ±lmalÄ±**
   - theory_closure.md'deki checklist iÅŸe yaradÄ±
   - LR/Scaling/L2 sÄ±rasÄ±yla kontrol ettim

3. **Ablation Ã§ok Ã¶ÄŸretici**
   - Scaling var/yok â†’ FarkÄ± NET gÃ¶rdÃ¼m
   - L2 var/yok â†’ Overfit'i tetikledim

4. **Grafik gÃ¶rselleÅŸtirme kritik**
   - Loss curve'lere bakmadan overfit'i gÃ¶remezdim
   - SayÄ±lar tek baÅŸÄ±na yeterli deÄŸil

5. **Experiment logging disiplini**
   - exp_log.csv tutmak Ã§ok faydalÄ± oldu
   - Hangi config'i denediÄŸimi hatÄ±rlÄ±yorum

## ðŸ”„ Bir Dahaki Sefere

### Denenemedi (Zaman Yetmedi)
- [ ] Gradient clipping
- [ ] Mixed precision training
- [ ] Learning rate warmup
- [ ] Cosine annealing

### Ä°yileÅŸtirmeler
- [ ] Otomatik hyperparameter search (Optuna?)
- [ ] Daha detaylÄ± grafikler (weight distribution)
- [ ] Gradient norm tracking
- [ ] Confidence intervals (multiple runs)

## âž¡ï¸ Week 2 HazÄ±rlÄ±ÄŸÄ±

### Teknik
- [ ] MNIST dataset indir
- [ ] MLP architecture oku (theory_foundations.md'de var mÄ±?)
- [ ] ReLU activation function teorisi

### Teori
- [ ] Non-linearity neden gerekli?
- [ ] Multi-layer backpropagation
- [ ] Classification metrics (accuracy, precision, recall)

## ðŸ“ Notlar (Kendime)

- Linear regression'da tÃ¼m Week 0 kavramlarÄ± gÃ¶rÃ¼ndÃ¼ âœ“
- Ablation study yaklaÅŸÄ±mÄ± Ã§ok iÅŸe yaradÄ±
- Grafik Ã§izmek sezgi geliÅŸtiriyor
- Theory olmadan debug etmek zordu (olurdu)
- Week 2'de MLP'yi aynÄ± disiplinle yapacaÄŸÄ±m

**Val MSE: 0.18 < 0.5 âœ“**
**Week 1 BAÅžARILI! ðŸŽ‰**
```

2. **Gradient Check Testi Ekle**
```python
# tests/test_linreg.py'a ekle

def test_gradient_numerical():
    """Autograd vs numerical gradient check."""
    model = LinearRegression(input_dim=5)
    x = torch.randn(10, 5)
    y = torch.randn(10, 1)
    
    # Autograd gradient
    loss = F.mse_loss(model(x), y)
    loss.backward()
    auto_grad = model.linear.weight.grad.clone()
    
    # Numerical gradient
    epsilon = 1e-5
    numerical_grad = torch.zeros_like(auto_grad)
    
    for i in range(auto_grad.shape[0]):
        for j in range(auto_grad.shape[1]):
            # Forward +epsilon
            model.linear.weight.data[i, j] += epsilon
            loss_plus = F.mse_loss(model(x), y)
            
            # Forward -epsilon
            model.linear.weight.data[i, j] -= 2 * epsilon
            loss_minus = F.mse_loss(model(x), y)
            
            # Restore
            model.linear.weight.data[i, j] += epsilon
            
            # Numerical gradient
            numerical_grad[i, j] = (loss_plus - loss_minus) / (2 * epsilon)
    
    # Check
    assert torch.allclose(auto_grad, numerical_grad, atol=1e-4)
```

**Kabul Kriteri:**
- âœ… week1_report.md tamamlandÄ±
- âœ… Gradient check testi yeÅŸil
- âœ… exp_log.csv dolu
- âœ… TÃ¼m grafikler outputs/'ta

**Ã‡Ä±ktÄ±:**
```bash
git add .
git commit -m "day5: final report + gradient check, Week 1 complete!"
git tag week1-complete
```

---

## ðŸ“Œ GÃ¼nlÃ¼k Ã‡alÄ±ÅŸma FormatÄ± (Tekrar Eden Ritim)

Her gÃ¼n aynÄ± disiplin:

```
1. HEDEF (1 cÃ¼mle)
   "Val MSE < 0.4 yapmak"

2. PLAN (3 madde)
   - LR sweep (1e-3, 5e-3, 1e-2)
   - Early stopping on
   - L2=1e-3 sabit

3. KOÅž
   KomutlarÄ± Ã§alÄ±ÅŸtÄ±r, logla

4. KAYDET
   - exp_log.csv'ye yaz
   - GrafiÄŸi outputs/'a kaydet
   - KÄ±sa not (5 satÄ±r)

5. COMMIT
   git commit -m "dayX: [ne yaptÄ±n], [sonuÃ§]"
```

**Template (her gÃ¼n kullan):**

```bash
# Sabah (5 dk)
echo "## Day X - $(date)" >> daily_log.md
echo "Hedef: [...]" >> daily_log.md
echo "Plan:" >> daily_log.md
echo "  1. [...]" >> daily_log.md
echo "  2. [...]" >> daily_log.md

# AkÅŸam (5 dk)
echo "SonuÃ§: [...]" >> daily_log.md
echo "Ã–ÄŸrenim: [...]" >> daily_log.md
git commit -m "dayX: [Ã¶zet]"
```

---

## ðŸ§ª Mini Katalar (15-20 dk, Opsiyonel Ama KeskinleÅŸtirir)

### Kata 1: Gradient Check
**Hedef:** Autograd'Ä±n doÄŸruluÄŸunu numeric gradient ile doÄŸrula

```python
def gradient_check(model, x, y, epsilon=1e-5):
    """Compare autograd vs numerical gradient."""
    # Autograd
    loss = F.mse_loss(model(x), y)
    loss.backward()
    auto_grad = [p.grad.clone() for p in model.parameters()]
    
    # Numerical
    numerical_grads = []
    for param in model.parameters():
        num_grad = torch.zeros_like(param)
        for idx in np.ndindex(param.shape):
            param.data[idx] += epsilon
            loss_plus = F.mse_loss(model(x), y)
            
            param.data[idx] -= 2 * epsilon
            loss_minus = F.mse_loss(model(x), y)
            
            param.data[idx] += epsilon  # restore
            
            num_grad[idx] = (loss_plus - loss_minus) / (2 * epsilon)
        numerical_grads.append(num_grad)
    
    # Check
    for auto, num in zip(auto_grad, numerical_grads):
        rel_error = torch.abs(auto - num) / (torch.abs(auto) + torch.abs(num) + 1e-8)
        print(f"Max relative error: {rel_error.max().item():.2e}")
```

**Ã–ÄŸrenme:** Autograd'a gÃ¼ven (numerik doÄŸrulama)

### Kata 2: Shape Invariants
**Hedef:** TÃ¼m katmanlarda shape assert'leri ekle

```python
class LinearRegressionAsserted(nn.Module):
    def __init__(self, input_dim):
        super().__init__()
        self.input_dim = input_dim
        self.linear = nn.Linear(input_dim, 1)
    
    def forward(self, x):
        assert x.dim() == 2, f"Expected 2D, got {x.dim()}D"
        assert x.shape[1] == self.input_dim, \
            f"Expected {self.input_dim} features, got {x.shape[1]}"
        
        out = self.linear(x)
        
        assert out.shape[1] == 1, f"Output should be (N, 1), got {out.shape}"
        return out
```

**Ã–ÄŸrenme:** Shape hatalarÄ± erken yakala

### Kata 3: Seed Tekrar
**Hedef:** AynÄ± seed ile 3 kez koÅŸ, varyansÄ± Ã¶lÃ§

```python
def reproducibility_test(seed=42, n_runs=3):
    """Same config, multiple runs, measure variance."""
    results = []
    
    for run in range(n_runs):
        torch.manual_seed(seed)
        np.random.seed(seed)
        
        # Train
        val_mse = train_model(...)
        results.append(val_mse)
    
    mean = np.mean(results)
    std = np.std(results)
    
    print(f"Val MSE: {mean:.4f} Â± {std:.4f}")
    assert std < 0.01, "Too much variance! Check seeding."
```

**Ã–ÄŸrenme:** Reproducibility disiplini

---

## ðŸŽ¯ Net Cevap: "NasÄ±l, Ne Zaman Pratik?"

### BugÃ¼n (45 dk)
```
âœ… ÅžÄ°MDÄ° hÄ±zlÄ± seti yap â†’ BUGÃœN BÄ°TÄ°R
   1. Setup (5 dk)
   2. Data + Manuel GD (10 dk)
   3. nn.Module + Training (15 dk)
   4. Test + Lint (10 dk)
   5. Mini rapor (5 dk)
```

### YarÄ±n (GÃ¼n 1)
```
âœ… Sprint planÄ±na gÃ¶re GÃ¼n 1'i koÅŸ
   - E2E pipeline
   - Logging sistemi kur
   - Ä°lk experiment'i kaydet
```

### Her GÃ¼n
```
âœ… GÃ¼nlÃ¼k Ã§alÄ±ÅŸma formatÄ±nÄ± kullan:
   1. Hedef (1 cÃ¼mle)
   2. Plan (3 madde)
   3. KoÅŸ
   4. Kaydet (exp_log.csv + grafik)
   5. Commit + Ã¶zet
```

### Teori Ne Zaman?
```
âœ… Sadece "neden bu sonucu aldÄ±n?" kÄ±smÄ±nda
   - 3-5 maddelik aÃ§Ä±klama
   - Week 0'a referans ver
   - Geri kalanÄ± KOD!
```

---

## ðŸ“‚ Dosya YapÄ±sÄ± (Week 1 Sonu)

```
week1_tensors/
â”œâ”€â”€ README.md (bu dosya)
â”œâ”€â”€ data_synth.py
â”œâ”€â”€ linreg_manual.py
â”œâ”€â”€ linreg_module.py
â”œâ”€â”€ train.py
â”œâ”€â”€ visualize_losses.py
â”œâ”€â”€ week1_summary.md (hÄ±zlÄ± sprint)
â”œâ”€â”€ week1_report.md (detaylÄ± rapor)
â”œâ”€â”€ daily_log.md (gÃ¼nlÃ¼k notlar)
â”‚
â”œâ”€â”€ data/
â”‚   â””â”€â”€ week1_linreg.pt
â”‚
â”œâ”€â”€ outputs/
â”‚   â”œâ”€â”€ exp_log.csv
â”‚   â”œâ”€â”€ model.pt
â”‚   â”œâ”€â”€ metrics.json
â”‚   â”œâ”€â”€ loss_curve_normal.png
â”‚   â””â”€â”€ loss_curve_overfit.png
â”‚
â””â”€â”€ tests/
    â””â”€â”€ test_linreg.py
```

---

## ðŸ“‹ Åžablonlar

### exp_log.csv Åžablonu

```csv
exp_id,lr,l2,batch,patience,scaling,train_mse,val_mse,epochs,stopped_early,time_sec,notes
exp001,5e-3,1e-3,32,5,True,0.12,0.18,25,True,8.5,baseline
exp002,1e-2,1e-3,32,5,True,0.15,0.22,35,True,10.2,lr too high
exp003,1e-3,1e-3,32,5,True,0.18,0.25,50,True,15.1,lr too low
exp_overfit,5e-3,0.0,32,999,True,0.05,0.45,200,False,45.0,L2=0 overfit
exp_no_scale,5e-3,1e-3,32,5,False,0.25,0.35,45,True,12.3,no scaling slow
```

### week1_summary.md Åžablonu

```markdown
# Week 1 - Sprint Ã–zeti

## KonfigÃ¼rasyon
- LR: [...]
- L2: [...]
- Batch: [...]
- Patience: [...]
- Scaling: [Yes/No]

## SonuÃ§lar
- Train MSE: [...]
- Val MSE: [...] (< 0.5 âœ“)
- Epochs: [...] (early stopped)
- Time: [...] sec

## GÃ¶zlem (3-5 madde)
- [...]
- [...]
- [...]

## Week 0 BaÄŸlantÄ±sÄ±
- [Hangi teori burada gÃ¶rÃ¼ndÃ¼?]
```

### daily_log.md Åžablonu

```markdown
# Week 1 - Daily Log

## Day 1 - 2025-10-06
### Hedef
Val MSE < 0.5 yapmak

### Plan
1. Pipeline'Ä± koÅŸ
2. Logging sistemi kur
3. Ä°lk experiment'i kaydet

### SonuÃ§
- Val MSE: 0.18 âœ“
- Pipeline Ã§alÄ±ÅŸÄ±yor
- exp_log.csv kuruldu

### Ã–ÄŸrenim
- Scaling eklemek Ã§ok fark etti
- Early stopping epoch 25'te durdu

---

## Day 2 - 2025-10-07
### Hedef
[...]
```

---

## ðŸŽ“ BaÅŸarÄ± Kriteri

### HÄ±zlÄ± Sprint (BugÃ¼n)
```
âœ… Val MSE < 0.5
âœ… Pytest yeÅŸil
âœ… week1_summary.md var
âœ… Git commit yapÄ±ldÄ±
```

### 5 GÃ¼nlÃ¼k Sprint (Week 1 Sonu)
```
âœ… Val MSE < 0.5
âœ… TÃ¼m testler yeÅŸil
âœ… week1_report.md tamamlandÄ±
âœ… exp_log.csv dolu (5+ experiment)
âœ… Loss curve grafikleri var
âœ… Overfit Ã¶rneÄŸi gÃ¶sterildi
âœ… Week 0 teori baÄŸlantÄ±sÄ± aÃ§Ä±klandÄ±
âœ… Git tag: week1-complete
```

---

## ðŸš€ Hadi BaÅŸlayalÄ±m!

```bash
# ÅžÄ°MDÄ° baÅŸla (45 dk)
cd /Users/onur/code/novadev-protocol
source .venv/bin/activate
cd week1_tensors

# Ä°lk komutu Ã§alÄ±ÅŸtÄ±r
python data_synth.py

# Week 0'da Ã¶ÄŸrendiklerini BU KODDA GÃ–R! ðŸ’ª
```

**Sonraki Hafta:** MLP & MNIST sÄ±nÄ±flandÄ±rma (accuracy â‰¥ %90)