# Hafta 1: TensÃ¶r, Autograd, Linear Regression

**Hedef:** PyTorch'un temellerini anla, sÄ±fÄ±rdan linear regression yaz, train/val split + regularization ekle.

---

## ğŸ“š Teori (45 dakika)

### Ã–ÄŸrenilecekler:
1. **Tensor nedir?** Ã‡ok boyutlu array (numpy benzeri ama GPU desteÄŸi).
2. **Autograd:** Otomatik tÃ¼rev hesaplama (backpropagation iÃ§in).
3. **Optimizer:** SGD, Adam (gradient descent varyasyonlarÄ±).
4. **Loss fonksiyonu:** MSE (Mean Squared Error).
5. **Regularization:** L2 (weight decay) â†’ overfitting'i Ã¶nle.

### Kaynaklar:
- [PyTorch Autograd Tutorial](https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html)
- [Linear Regression Explained](https://www.youtube.com/watch?v=nk2CQITm_eo) (3Blue1Brown style)

---

## ğŸ’» Pratik (90 dakika)

### AdÄ±m 1: Manuel Linear Regression (ilkel API)

**GÃ¶rev:** `y = wx + b` formÃ¼lÃ¼nÃ¼ PyTorch tensÃ¶rleri ile yaz.

Dosya: `linreg_manual.py`

```python
# Pseudo-code:
# 1. Rastgele data Ã¼ret: X, y
# 2. Parametreleri baÅŸlat: w, b (requires_grad=True)
# 3. Loop:
#    a. Forward: y_pred = X @ w + b
#    b. Loss: mse = ((y_pred - y) ** 2).mean()
#    c. Backward: loss.backward()
#    d. Update: w -= lr * w.grad (manual SGD)
#    e. Zero grad: w.grad.zero_()
```

### AdÄ±m 2: PyTorch nn.Module ile (daha temiz)

Dosya: `linreg_module.py`

```python
class LinearRegression(nn.Module):
    def __init__(self, input_dim):
        super().__init__()
        self.linear = nn.Linear(input_dim, 1)
    
    def forward(self, x):
        return self.linear(x)

# optimizer = torch.optim.Adam(model.parameters(), lr=0.01)
```

### AdÄ±m 3: Train/Val Split + L2 Regularization

Dosya: `train.py`

- %80 train, %20 val split
- Her epoch sonunda val loss logla
- Early stopping (5 epoch boyunca iyileÅŸme yoksa dur)
- L2 reg: `optimizer = Adam(..., weight_decay=0.01)`

---

## ğŸ“Š Teslim (Hafta 1 Sonu)

### 1. Jupyter Notebook: `linreg_from_scratch.ipynb`

Ä°Ã§erik:
- Data oluÅŸturma (sentetik linear data)
- Manuel regresyon (adÄ±m adÄ±m)
- nn.Module regresyon
- Train/val curves (matplotlib)
- Final sonuÃ§: `MSE < 0.5` (sentetik data iÃ§in)

### 2. Test DosyasÄ±: `tests/test_linreg.py`

```python
def test_model_convergence():
    """Model train edildiÄŸinde MSE dÃ¼ÅŸmeli."""
    # ... train loop
    assert final_mse < 0.5

def test_l2_regularization():
    """L2 reg ile weight'ler kÃ¼Ã§Ã¼lmeli."""
    # ... compare with/without weight_decay
    assert norm_with_l2 < norm_without_l2
```

Ã‡alÄ±ÅŸtÄ±r:
```bash
pytest tests/test_linreg.py -v
```

### 3. KapanÄ±ÅŸ Raporu: `week1_summary.md`

```markdown
# Hafta 1: TensÃ¶r & Linear Regression Ã–zeti

## ğŸ¯ Tamamlananlar
- [ ] Manuel gradient descent
- [ ] nn.Module ile model
- [ ] Train/val split
- [ ] L2 regularization
- [ ] Testler geÃ§ti (MSE < 0.5)

## ğŸ“ˆ SonuÃ§lar
- Final Train MSE: X.XX
- Final Val MSE: X.XX
- Epoch sayÄ±sÄ±: XX

## ğŸ’­ Ã–ÄŸrendiklerim
- [3 madde: en Ã¶nemli insight'lar]

## ğŸ› KarÅŸÄ±laÅŸÄ±lan Sorunlar
- [Varsa sorunlar ve Ã§Ã¶zÃ¼mler]

## â¡ï¸ Hafta 2 HazÄ±rlÄ±ÄŸÄ±
- MNIST dataset indir
- MLP (multi-layer perceptron) teorisi oku
```

---

## ğŸ¯ BaÅŸarÄ± Kriteri

âœ… **MSE < 0.5** (sentetik linear dataset iÃ§in)
âœ… **Testler geÃ§meli:** `pytest tests/test_linreg.py`
âœ… **Loss curve grafiÄŸi:** Train ve val loss beraber plot edilmeli

---

## ğŸ”§ Starter Kod

`linreg_manual.py` iÃ§inde iskelet hazÄ±r. Eksik kÄ±sÄ±mlarÄ± sen dolduracaksÄ±n (ğŸš§ TODO iÅŸaretleri var).

---

**Sonraki Hafta:** MLP & MNIST sÄ±nÄ±flandÄ±rma (accuracy â‰¥ %90)
