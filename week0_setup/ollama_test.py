"""
Hafta 0: Ollama Lokal LLM Test
Basit prompt gÃ¶nderme denemesi (HTTP API Ã¼zerinden).
"""


try:
    import requests

    REQUESTS_AVAILABLE = True
except ImportError:
    REQUESTS_AVAILABLE = False
    print("âš ï¸  'requests' paketi bulunamadÄ±. Kurulum iÃ§in:")
    print("   pip install requests")


def test_ollama(model: str = "qwen2.5:7b", prompt: str = "Explain PyTorch tensors in 2 sentences."):
    """Test Ollama API with a simple prompt."""
    if not REQUESTS_AVAILABLE:
        return

    url = "http://localhost:11434/api/generate"
    payload = {
        "model": model,
        "prompt": prompt,
        "stream": False,
    }

    print(f"ğŸ§  Testing Ollama with model: {model}")
    print(f"ğŸ“ Prompt: {prompt}\n")

    try:
        response = requests.post(url, json=payload, timeout=60)
        response.raise_for_status()

        result = response.json()
        answer = result.get("response", "")

        print(f"âœ… Response:\n{answer}\n")
        print(f"â±ï¸  Total duration: {result.get('total_duration', 0) / 1e9:.2f}s")
        print(f"ğŸ“Š Tokens generated: {result.get('eval_count', 0)}")

    except requests.exceptions.ConnectionError:
        print("âŒ Ollama servisine baÄŸlanÄ±lamadÄ±!")
        print("   Ã‡Ã¶zÃ¼m: Terminal'de 'ollama serve' komutunu Ã§alÄ±ÅŸtÄ±r.")
    except requests.exceptions.Timeout:
        print("âŒ Ä°stek zaman aÅŸÄ±mÄ±na uÄŸradÄ± (60s). Model Ã§ok bÃ¼yÃ¼k olabilir.")
    except requests.exceptions.HTTPError as e:
        print(f"âŒ HTTP hatasÄ±: {e}")
        print("   Model indirilmiÅŸ mi? Kontrol: ollama list")
    except Exception as e:
        print(f"âŒ Beklenmeyen hata: {e}")


def list_ollama_models():
    """List available Ollama models."""
    if not REQUESTS_AVAILABLE:
        return

    url = "http://localhost:11434/api/tags"

    try:
        response = requests.get(url, timeout=5)
        response.raise_for_status()

        models = response.json().get("models", [])
        if models:
            print("ğŸ“š Ä°ndirilen Ollama modelleri:")
            for model in models:
                name = model.get("name", "unknown")
                size_gb = model.get("size", 0) / (1024**3)
                print(f"   - {name} ({size_gb:.2f} GB)")
            print()
        else:
            print("âš ï¸  HenÃ¼z model indirilmemiÅŸ. Ã–rnek:")
            print("   ollama pull qwen2.5:7b")
            print()

    except requests.exceptions.ConnectionError:
        print("âŒ Ollama servisine baÄŸlanÄ±lamadÄ±.")
        print("   Ã‡Ã¶zÃ¼m: ollama serve\n")


if __name__ == "__main__":
    print("=" * 60)
    print("  Ollama Lokal LLM Test")
    print("=" * 60 + "\n")

    list_ollama_models()
    test_ollama()

    print("\nğŸ’¡ FarklÄ± model denemek iÃ§in:")
    print("   python ollama_test.py")
    print("   # Kod iÃ§inde model='llama3.2:7b' olarak deÄŸiÅŸtir\n")
