# Week 0: Matematiksel Temeller Part 2 - Ä°leri Konular

**NovaDev v1.0 - Probabilistik & Teorik Derinlik**

> "Neden bu loss? Neden bu regularization? â†’ Probabilistik kÃ¶kenler"

---

## 5ï¸âƒ£ SayÄ±sal KoÅŸullar & Curvature

### ğŸ¯ Condition Number (KoÅŸul SayÄ±sÄ±)

**TanÄ±m:** Loss yÃ¼zeyinin "ne kadar Ã§arpÄ±k" olduÄŸunu Ã¶lÃ§er.

```
Îº(H) = Î»_max / Î»_min
       â†‘         â†‘
   En bÃ¼yÃ¼k  En kÃ¼Ã§Ã¼k
   eigenvalue
   
H = Hessian matrix (2. tÃ¼revler)
```

#### GÃ¶rsel AÃ§Ä±klama

**KÃ¼Ã§Ã¼k Îº (iyi koÅŸullanmÄ±ÅŸ):**
```
     â—         Dairevi yÃ¼zey
   â•±   â•²       
  â”‚  â—  â”‚      â† Minimum
   â•²   â•±
     â—
     
GD doÄŸrudan iner âœ“
```

**BÃ¼yÃ¼k Îº (kÃ¶tÃ¼ koÅŸullanmÄ±ÅŸ):**
```
     â•±â•²        Eliptik yÃ¼zey
    â•±  â•²
   â•±    â•²      â† Ã‡ok uzun
  â”‚  â—   â”‚     â† Ã‡ok dar
  â”‚      â”‚
   â•²    â•±
    â•²  â•±
     â•²â•±
     
GD zikzak yapar âœ—
```

### ğŸ“ Neden Ã–lÃ§ekleme Ä°ÅŸe Yarar?

#### Matematiksel KanÄ±t (BasitleÅŸtirilmiÅŸ)

**Ã–lÃ§eksiz:**
```
L(w) = (1/2) Î£ (y - w_1Ã—1 - w_2Ã—1000)Â²

Hessian'Ä±n eigenvalue'larÄ±:
Î»_1 â‰ˆ 1
Î»_2 â‰ˆ 1,000,000

Îº = 1,000,000 / 1 = 10â¶  â† Ã‡ARPIK!
```

**Ã–lÃ§eklenmiÅŸ:**
```
x_1' = x_1 / std(x_1)
x_2' = x_2 / std(x_2)

Î»_1 â‰ˆ 1
Î»_2 â‰ˆ 1

Îº = 1 / 1 = 1  â† YUVARLAK!
```

### ğŸ§® Ä°kinci TÃ¼rev Sezgisi

**Birinci tÃ¼rev (gradient):** YÃ¶n
**Ä°kinci tÃ¼rev (Hessian):** EÄŸrilik

```
f''(x) < 0: âˆ©  (konkav, maksimum civarÄ±)
f''(x) = 0: â”€  (inflection point)
f''(x) > 0: âˆª  (konveks, minimum civarÄ±)
```

**Newton's Method:**
```
Î¸_new = Î¸_old - H^(-1) Ã— âˆ‡L
                  â†‘
             EÄŸriliÄŸi kompanse et
```

**Problem:** Hessian hesabÄ± **O(dÂ²)** â†’ Ã‡ok pahalÄ±!

**Ã‡Ã¶zÃ¼m:** Adam/Momentum Hessian'Ä± **yaklaÅŸÄ±k** kullanÄ±r
- Ucuz
- Pratikte yeterli

---

## 6ï¸âƒ£ Probabilistik BakÄ±ÅŸ: Neden MSE? Neden CE?

### ğŸ² Maximum Likelihood Estimation (MLE)

**Temel Fikir:**
> "GÃ¶zlediÄŸim verinin olasÄ±lÄ±ÄŸÄ±nÄ± maksimize et"

```
Î¸* = argmax_Î¸ p(Data | Î¸)
            â†‘
      Likelihood (olabilirlik)
```

**Log trick (rahat Ã§alÄ±ÅŸmak iÃ§in):**
```
log p(D|Î¸) = Î£ log p(y_i | x_i, Î¸)
            â†‘
       Log-likelihood
```

### ğŸ“ MSE'nin Probabilistik KÃ¶keni

#### VarsayÄ±m: Gaussian Noise

```
y = f_Î¸(x) + Îµ
Îµ ~ N(0, ÏƒÂ²)  â† Normal daÄŸÄ±lÄ±m

â†’ p(y|x,Î¸) = N(f_Î¸(x), ÏƒÂ²)
```

#### Log-Likelihood TÃ¼retimi

```
log p(y|x,Î¸) = log N(y; f_Î¸(x), ÏƒÂ²)
             = -(y - f_Î¸(x))Â² / (2ÏƒÂ²) + const
             
Maksimize et:
argmax Î£ log p(y_i|x_i,Î¸)
= argmax Î£ [-(y_i - f_Î¸(x_i))Â²]
= argmin Î£ (y_i - f_Î¸(x_i))Â²
           â†‘
          MSE!
```

**SonuÃ§:**
```
MSE minimize etmek
    â‰¡
Gaussian noise varsayÄ±mÄ± altÄ±nda
likelihood maksimize etmek
```

### ğŸ¯ Cross-Entropy'nin Probabilistik KÃ¶keni

#### VarsayÄ±m: Bernoulli Distribution (Binary)

```
y âˆˆ {0, 1}
p(y=1|x,Î¸) = Ïƒ(f_Î¸(x))  â† Sigmoid
```

#### Log-Likelihood TÃ¼retimi

```
p(y|x,Î¸) = p^y Ã— (1-p)^(1-y)

log p(y|x,Î¸) = y log p + (1-y) log(1-p)

Maksimize et:
argmax Î£ [y_i log p_i + (1-y_i) log(1-p_i)]
= argmin Î£ [-y_i log p_i - (1-y_i) log(1-p_i)]
            â†‘
     Binary Cross-Entropy!
```

**SonuÃ§:**
```
BCE minimize etmek
    â‰¡
Bernoulli varsayÄ±mÄ± altÄ±nda
likelihood maksimize etmek
```

### ğŸ“ MAP: Bayesian Twist

#### Maximum A Posteriori

**MLE:** p(Î¸ | Data)
**MAP:** p(Î¸ | Data) Ã— p(Î¸)
                        â†‘
                    Prior belief
                    (Ã¶n bilgi)

**Bayes KuralÄ±:**
```
p(Î¸|D) âˆ p(D|Î¸) Ã— p(Î¸)
  â†‘        â†‘        â†‘
Posterior  Like   Prior
```

**Log formunda:**
```
log p(Î¸|D) = log p(D|Î¸) + log p(Î¸) + const

argmax = argmin [-log p(D|Î¸) - log p(Î¸)]
                    â†‘            â†‘
                  Loss      Regularization!
```

### ğŸ“Š Regularization'Ä±n Probabilistik AnlamÄ±

#### L2 Regularization = Gaussian Prior

```
Prior: Î¸ ~ N(0, 1/Î»)

log p(Î¸) = -Î»/2 Ã— Î£ Î¸_jÂ²

MAP:
argmin [Loss + Î»/2 Ã— Î£ Î¸_jÂ²]
                      â†‘
                  Ridge (L2)
```

**Yorum:** "Parametreler sÄ±fÄ±ra yakÄ±n olsun tercihim"

#### L1 Regularization = Laplace Prior

```
Prior: Î¸ ~ Laplace(0, 1/Î»)

log p(Î¸) = -Î» Ã— Î£ |Î¸_j|

MAP:
argmin [Loss + Î» Ã— Î£ |Î¸_j|]
                    â†‘
                Lasso (L1)
```

**Yorum:** "Parametreler TAM SIFIR olsun tercihim" (sparsity)

### ğŸ’¡ Ã–zet Tablo

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  VarsayÄ±m   â”‚     Loss     â”‚   Regularizationâ”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Gauss Noise â”‚     MSE      â”‚    L2 (Gauss)   â”‚
â”‚ Bernoulli   â”‚     BCE      â”‚    L1 (Laplace) â”‚
â”‚ Categorical â”‚     CE       â”‚    -            â”‚
â”‚ Poisson     â”‚  Poisson Lossâ”‚    -            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Mesaj:**
> Loss ve regularization **rastgele** deÄŸil;
> probabilistik varsayÄ±mlarÄ±n matematiksel sonuÃ§larÄ±.

---

## 7ï¸âƒ£ Bias-Variance Trade-off: Matematiksel AyrÄ±ÅŸtÄ±rma

### ğŸ“ Formal TanÄ±m

**Beklenen test hatasÄ±:**
```
E[(y - Å·)Â²] = BiasÂ² + Variance + Irreducible Error
               â†‘        â†‘              â†‘
           Systematic Sensitivity  GÃ¼rÃ¼ltÃ¼
           hata      to data      (ÎµÂ²)
```

### ğŸ§® TÃ¼retim (BasitleÅŸtirilmiÅŸ)

```
GerÃ§ek: y = f(x) + Îµ,  Îµ ~ N(0, ÏƒÂ²)
Tahmin: Å· = fÌ‚(x)

E[(y - Å·)Â²] 
= E[(y - f + f - Å·)Â²]
= E[(f - Å·)Â²] + E[ÎµÂ²]
  â†‘             â†‘
  Model err   Irreducible

E[(f - Å·)Â²]
= E[(f - E[Å·] + E[Å·] - Å·)Â²]
= (f - E[Å·])Â² + E[(E[Å·] - Å·)Â²]
    â†‘              â†‘
   BiasÂ²        Variance
```

### ğŸ“Š GÃ¶rsel AÃ§Ä±klama

```
        Bias
         â†“
    â”Œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”  â† Model ortalamasÄ±
    â”‚    â—    â”‚
    â”‚  â— â— â—  â”‚  â† FarklÄ± train set'lerde
    â”‚    â—    â”‚     model tahminleri
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â†•
      Variance
      
      âŠ• = GerÃ§ek hedef
```

**DÃ¼ÅŸÃ¼k Bias + DÃ¼ÅŸÃ¼k Variance:**
```
    âŠ•
  â— â— â—  â† Model tutarlÄ± ve doÄŸru
```

**YÃ¼ksek Bias + DÃ¼ÅŸÃ¼k Variance:**
```
    âŠ•
    
  â— â— â—  â† Model tutarlÄ± ama yanlÄ±ÅŸ
```

**DÃ¼ÅŸÃ¼k Bias + YÃ¼ksek Variance:**
```
â—   âŠ•   â—
    â—     â† Model bazen doÄŸru ama
  â—         tutarsÄ±z (overfit)
```

### ğŸ¯ Kapasite-Hata Ä°liÅŸkisi

```
Error
  â”‚
  â”‚â•²              â† Total Error
  â”‚ â•²
  â”‚  â•²  â•±        
  â”‚   â•²â•±         â† Bias
  â”‚    â•²         â† Variance
  â”‚     â•²___
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Model Capacity
  Basit      KarmaÅŸÄ±k
  
  Underfit  Sweet   Overfit
            Spot
```

### ğŸ’Š Regularization = Variance KontrolÃ¼

```
Regularization â†‘
  â†’ Kapasite â†“
  â†’ Variance â†“
  â†’ Bias â†‘ (biraz)
  
AmaÃ§: Toplam hatayÄ± minimize et
```

---

## 8ï¸âƒ£ Regularization DerinliÄŸi

### ğŸšï¸ L2 (Ridge) Matematiksel Analiz

```
L_ridge = MSE + Î»/2 Ã— ||w||Â²

Gradient:
âˆ‡L_ridge = âˆ‡MSE + Î» Ã— w

Update:
w â† w - Î· Ã— (âˆ‡MSE + Î»w)
  = w(1 - Î·Î») - Î·âˆ‡MSE
      â†‘
   Weight decay
   (her step'te biraz kÃ¼Ã§Ã¼lt)
```

**Etki:** AÄŸÄ±rlÄ±klarÄ± **kÃ¼Ã§Ã¼k** tutar (sÄ±fÄ±ra itmez)

### ğŸšï¸ L1 (Lasso) Matematiksel Analiz

```
L_lasso = MSE + Î» Ã— ||w||â‚

Gradient (subdifferential):
âˆ‚L_lasso = âˆ‡MSE + Î» Ã— sign(w)

Update:
w â† w - Î· Ã— (âˆ‡MSE + Î» sign(w))
                      â†‘
                 Sabit bÃ¼yÃ¼klÃ¼kte
                 itme (sÄ±fÄ±ra)
```

**Etki:** AÄŸÄ±rlÄ±klarÄ± **TAM SIFIR** yapar (feature selection)

### ğŸ“Š L1 vs L2 Geometri

```
L2 (Ridge):
    w_2
     â†‘
   â•±â”€â”€â”€â•²   â† Daire (wâ‚Â²+wâ‚‚Â²=c)
  â”‚  â—  â”‚
   â•²â”€â”€â”€â•±
 â”€â”€â”€â”€â”€â—â”€â”€â”€â”€â”€â†’ w_1
      â”‚
   KesiÅŸme genelde
   eksen dÄ±ÅŸÄ±nda

L1 (Lasso):
    w_2
     â†‘
    â•±â”‚â•²    â† Elmas (|wâ‚|+|wâ‚‚|=c)
   â•± â”‚ â•²
  â•±  â—  â•²  â† KesiÅŸme genelde
 â”€â”€â”€â”€â—â”€â”€â”€â”€â†’ w_1  eksen Ã¼zerinde
     â”‚            (sparse!)
```

### ğŸ”„ Early Stopping = Implicit Regularization

**GÃ¶zlem:**
```
EÄŸitim devam ettikÃ§e:
  Train loss â†“â†“â†“
  Val loss â†“ â†’ â†‘
           â†‘
    Bu noktada dur!
```

**Matematiksel AÃ§Ä±klama:**

```
Gradient descent her step'te:
w_t = w_0 - Î·t Ã— âˆ‡Ì„L
         â†‘
    Toplam gradient etkisi

t kÃ¼Ã§Ã¼k â†’ w kÃ¼Ã§Ã¼k â†’ Implicit L2
t bÃ¼yÃ¼k â†’ w bÃ¼yÃ¼k â†’ Overfit risk
```

**KanÄ±t (sketch):**
- Early stop â‰ˆ aÄŸÄ±rlÄ±klarÄ± bÃ¼yÃ¼meden durdur
- Bu â‰ˆ L2'nin etkisi (kÃ¼Ã§Ã¼k aÄŸÄ±rlÄ±k tercihi)

### ğŸ² Dropout (Derin AÄŸlarda)

```
Training:
  Her neuron p olasÄ±lÄ±kla "Ã¶l"
  â†’ Ensemble effect
  
Test:
  TÃ¼m neuronlar aktif ama
  Ã§Ä±ktÄ±larÄ± p ile scale et
```

**Neden Ä°ÅŸe Yarar:**
- Co-adaptation'Ä± kÄ±rar
- Ensemble benzeri
- Implicit regularization

---

## 9ï¸âƒ£ DeÄŸerlendirme Metrikleri: Derin BakÄ±ÅŸ

### ğŸ“Š Regression Metrikleri

#### RÂ² (Coefficient of Determination)
```
RÂ² = 1 - (SS_res / SS_tot)

SS_res = Î£(y - Å·)Â²     â† Residual sum of squares
SS_tot = Î£(y - È³)Â²     â† Total sum of squares

Yorum:
RÂ² = 0.85 â†’ Model varyansÄ±n %85'ini aÃ§Ä±klÄ±yor
RÂ² < 0   â†’ Model mean'den kÃ¶tÃ¼!
```

#### RMSE vs MAE
```
RMSE = âˆš(MSE)
       â†‘
   Scale geri kazanÄ±ldÄ±
   (yorumlanabilir)

MAE  = Mean absolute error
       Robust to outliers
```

### ğŸ¯ Classification Metrikleri

#### Confusion Matrix AyrÄ±ÅŸtÄ±rma

```
             Predicted
           Pos      Neg
Actual Pos  TP  |   FN    â† Recall = TP/(TP+FN)
       Neg  FP  |   TN
            â†“
    Precision = TP/(TP+FP)
```

**Precision:** "Pozitif dediÄŸim ne kadar doÄŸru?"
**Recall:** "GerÃ§ek pozitiflerin ne kadarÄ±nÄ± yakaladÄ±m?"

#### F1 Score (Harmonic Mean)
```
F1 = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)

Neden harmonic mean?
  â†’ KÃ¼Ã§Ã¼k deÄŸerlere aÄŸÄ±rlÄ±k verir
  â†’ Precision=0.9, Recall=0.1
    Arithmetic: 0.5  (yanÄ±ltÄ±cÄ±!)
    Harmonic: 0.18   (gerÃ§ekÃ§i)
```

#### ROC-AUC vs PR-AUC

**ROC Curve:**
```
TPR vs FPR (eÅŸikler deÄŸiÅŸirken)

TPR = TP / (TP + FN)  â† True Positive Rate
FPR = FP / (FP + TN)  â† False Positive Rate
```

**PR Curve:**
```
Precision vs Recall

Ä°mbalanced data'da daha bilgilendirici!
```

**Ã–rnek (Imbalanced):**
```
Pozitif: %1
Negatif: %99

Dummy model: "Hep negatif de"
  Accuracy: %99  â† YanÄ±ltÄ±cÄ±!
  Recall: 0      â† GerÃ§ek
```

#### Calibration (Kalibrasyon)

```
Model: "Bu Ã¶rnek %80 olasÄ±lÄ±kla pozitif"
GerÃ§ek: 100 Ã¶rneÄŸin 80'i pozitif mi?

Calibration curve:
  y = x doÄŸrusu â†’ MÃ¼kemmel
  Sapma â†’ Kalibrasyon gerekli
```

**Neden Ã–nemli:**
- TÄ±p, finans gibi alanlarda
- OlasÄ±lÄ±k tahminleri kritik

---

## ğŸ”Ÿ Deney Disiplini: Bilimsel YÃ¶ntem

### ğŸ”¬ Hipotez OdaklÄ± Deney

```
1. GÃ¶zlem:    Val loss platoya ulaÅŸtÄ±
2. Hipotez:   LR Ã§ok bÃ¼yÃ¼k / Overfit
3. Deney:     LR'Ä± yarÄ±ya indir
4. Ã–lÃ§Ã¼m:     Val loss deÄŸiÅŸimi
5. SonuÃ§:     Kabul/Ret
6. Ã–ÄŸrenme:   Sonraki hipotez
```

### ğŸ“Š Baseline Stratejisi

```
Level 0: Dummy
  - Mean predictor (regression)
  - Most frequent class (classification)
  
Level 1: Simple
  - Linear regression
  - Logistic regression
  
Level 2: Standard
  - RF/XGBoost (tabular)
  - ResNet (vision)
  - BERT (NLP)
  
Level 3: Custom
  - Domain-specific architecture
```

**Kural:** Her level'Ä± beat et, sonra geÃ§!

### ğŸ” Ablation Studies

```
Full model: A + B + C + D â†’ 0.85

Ablation:
  A + B + C     â†’ 0.82  (D katkÄ±sÄ±: +0.03)
  A + B     + D â†’ 0.80  (C katkÄ±sÄ±: +0.05)
  A     + C + D â†’ 0.78  (B katkÄ±sÄ±: +0.07)
      B + C + D â†’ 0.60  (A katkÄ±sÄ±: +0.25)
```

**SonuÃ§:** A en kritik, B ve C Ã¶nemli, D kÃ¼Ã§Ã¼k etki

### ğŸ² Hyperparameter Search

#### Grid Search
```
LR: [0.001, 0.01, 0.1]
L2: [0.0001, 0.001, 0.01]

â†’ 3Ã—3 = 9 deney
```

**Dezavantaj:** Kombinatoryal patlama

#### Random Search
```
LR: Uniform(0.0001, 0.1)
L2: Log-uniform(1e-5, 1e-2)

â†’ N rastgele nokta
```

**Avantaj:** Daha verimli (kanÄ±tlanmÄ±ÅŸ!)

#### Bayesian Optimization
```
1. Ä°lk denemeler yap
2. Gaussian Process fit et
3. Acquisition function ile sonraki noktayÄ± seÃ§
4. Tekrarla
```

**Avantaj:** Az deneyle iyi sonuÃ§

### ğŸ“ Experiment Logging Template

```markdown
## Experiment 2025-10-06-003

### Hypothesis
LR=0.01'de overfit var, 0.001'e dÃ¼ÅŸÃ¼rÃ¼nce dÃ¼zelir mi?

### Setup
- Model: LinearRegression(input=10)
- Optimizer: Adam(lr=0.001, weight_decay=1e-4)
- Batch: 32
- Epochs: 100
- Seed: 42
- Device: MPS

### Baseline
Exp-002: lr=0.01 â†’ Train=0.05, Val=0.15 (overfit!)

### Results
Train Loss: 0.12 (+0.07 vs baseline)
Val Loss:   0.10 (-0.05 vs baseline) âœ“
Time: 45s

### Analysis
- Overfit azaldÄ± âœ“
- Training biraz yavaÅŸ
- Val iyileÅŸti â†’ Hipotez doÄŸru

### Next Steps
- lr=0.001 ile L2'yi arttÄ±r?
- Early stopping ekle?
```

---

## 1ï¸âƒ£1ï¸âƒ£ Tensors & Autograd: Derin Sezgi

### ğŸ§Š Tensor = Data + Device + Dtype

```python
x = torch.randn(10, 5, device='mps', dtype=torch.float32)
         â†‘       â†‘        â†‘            â†‘
       YaratÄ±cÄ± Shape  Nerede?    Ne tÃ¼r sayÄ±?
```

**Neden Ã–nemli:**
- Device: Performans (CPU/GPU/MPS)
- Dtype: Bellek + sayÄ±sal stabilite
- Shape: Her iÅŸlemde kontrol ÅŸart

### ğŸ“¡ Broadcasting: Otomatik GeniÅŸleme

```
(10, 1) + (1, 5) â†’ (10, 5)

Kural:
1. SaÄŸdan hizala
2. Boyutlar eÅŸit VEYA biri 1 olmalÄ±
3. Eksik boyut 1 kabul edilir

Ã–rnekler:
(3, 1, 5) + (4, 5) â†’ (3, 4, 5) âœ“
(3, 2) + (3, 1) â†’ (3, 2) âœ“
(3, 2) + (2, 3) â†’ ERROR âœ—
```

### ğŸ”„ Computational Graph

```
     x              y
      â†“             â†“
    [Linear]      [Square]
       â†“             â†“
       a    â†’   [Multiply] â†’ b
                     â†“
                  [Sum] â†’ L
```

**Forward:**
```
x, y â†’ a, b â†’ L  (deÄŸerleri hesapla)
```

**Backward:**
```
âˆ‚L/âˆ‚L = 1
âˆ‚L/âˆ‚b = âˆ‚L/âˆ‚L Ã— âˆ‚L/âˆ‚b  (chain rule)
âˆ‚L/âˆ‚a = âˆ‚L/âˆ‚b Ã— âˆ‚b/âˆ‚a
âˆ‚L/âˆ‚x = âˆ‚L/âˆ‚a Ã— âˆ‚a/âˆ‚x
```

**Autograd = Chain Rule Otomasyonu**

### âš ï¸ Gradient Accumulation

```python
# âŒ YANLIÅ
for epoch in range(epochs):
    loss = compute_loss()
    loss.backward()  # Gradientler BÄ°RÄ°KÄ°YOR!
    optimizer.step()

# âœ… DOÄRU
for epoch in range(epochs):
    optimizer.zero_grad()  # Ã–nce temizle
    loss = compute_loss()
    loss.backward()
    optimizer.step()
```

**Neden:** `backward()` **add** yapar, **set** deÄŸil!

---

## 1ï¸âƒ£2ï¸âƒ£ Lineer Regresyon: Neden Ä°lk AdÄ±m?

### ğŸ¯ Pedagojik Nedenler

#### 1. KapalÄ± Form Ã‡Ã¶zÃ¼mÃ¼ Var
```
Normal Equations:
w* = (X^T X)^(-1) X^T y

â†’ Analitik Ã§Ã¶zÃ¼m biliyoruz
â†’ GD'nin doÄŸruluÄŸunu kontrol edebiliriz
```

#### 2. Konveks Problem
```
L(w) = ||y - Xw||Â²

â†’ Tek global minimum
â†’ Local minimum yok
â†’ Optimizasyon davranÄ±ÅŸÄ± temiz
```

#### 3. GÃ¶rsel Anlama Kolay
```
2D plot:
  y
  â”‚  â—
  â”‚    â—  â—
  â”‚  â—    â”€â”€â”€ Fit line
  â”‚ â—  â—
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ x
```

#### 4. TÃ¼m Kavramlar Var
- Loss (MSE)
- Gradient
- Optimization (GD)
- Regularization (Ridge/Lasso)
- Overfitting
- Val/Test split
- Metrics (RÂ², RMSE)

### ğŸ“ Geometrik Sezgi

**Linear regression = Projeksiyon**

```
y âˆˆ R^n  (veri uzayÄ±)
Å· âˆˆ span(X)  (Ã¶zellik uzayÄ±)

Å· = argmin ||y - v||Â²
    v âˆˆ span(X)
    
â†’ y'nin span(X)'e ortogonal projeksiyonu!
```

---

## 1ï¸âƒ£3ï¸âƒ£ Saha HatalarÄ±: GerÃ§ek Hayat Deneyimi

### ğŸ› En SÄ±k 10 Hata

#### 1. Data Leakage
```
âŒ Normalizasyon tÃ¼m veride
âœ… Train'den Ã¶ÄŸren, test'e uygula
```

#### 2. YanlÄ±ÅŸ Loss/Metric
```
âŒ Regression'a CrossEntropy
âŒ Binary classification'a MSE
```

#### 3. Val = Test KarÄ±ÅŸtÄ±rma
```
âŒ Hiperparametre seÃ§imi test'te
âœ… Val'de seÃ§, test'e bir kez bak
```

#### 4. LR Felaketi
```
Belirti: Loss â†’ NaN
Ã‡Ã¶zÃ¼m: LR'Ä± /10 yap
```

#### 5. Shape UyumsuzluÄŸu
```
Belirti: "RuntimeError: size mismatch"
Ã‡Ã¶zÃ¼m: Her adÄ±mda print(x.shape)
```

#### 6. Device Mismatch
```
Belirti: "Expected CPU tensor but got CUDA"
Ã‡Ã¶zÃ¼m: TÃ¼m tensor'leri aynÄ± device'a koy
```

#### 7. Gradient SÄ±fÄ±rlama Unutma
```
Belirti: Loss patlÄ±yor, training unstable
Ã‡Ã¶zÃ¼m: optimizer.zero_grad() her iterasyon
```

#### 8. Seed Yok
```
Belirti: SonuÃ§lar tekrarlanamÄ±yor
Ã‡Ã¶zÃ¼m: set_seed(42) baÅŸta
```

#### 9. Val Loss Ä°zlememe
```
Belirti: Train iyi, test kÃ¶tÃ¼ (overfit)
Ã‡Ã¶zÃ¼m: Early stopping kur
```

#### 10. Feature Scale Unutma
```
Belirti: LR hassas, yakÄ±nsama yavaÅŸ
Ã‡Ã¶zÃ¼m: StandardScaler kullan
```

### ğŸ” Debug ProtokolÃ¼ (6 AdÄ±m)

```
1. LR KontrolÃ¼
   â–¡ Ã‡ok bÃ¼yÃ¼k mÃ¼? (loss zÄ±plar)
   â–¡ Ã‡ok kÃ¼Ã§Ã¼k mÃ¼? (loss hareket etmez)

2. Data SÄ±zÄ±ntÄ±sÄ±
   â–¡ Train/val/test doÄŸru ayrÄ±ldÄ± mÄ±?
   â–¡ Normalizasyon train'den Ã¶ÄŸrenildi mi?

3. Shape/Device
   â–¡ TÃ¼m tensor'ler aynÄ± device'da mÄ±?
   â–¡ Shape'ler beklediÄŸin gibi mi?

4. Gradient AkÄ±ÅŸÄ±
   â–¡ zero_grad() her iterasyonda mÄ±?
   â–¡ backward() Ã§aÄŸrÄ±lÄ±yor mu?
   â–¡ requires_grad=True mÄ±?

5. Loss/Metric DoÄŸru
   â–¡ Problem tipi ile uyumlu mu?
   â–¡ Reduction (mean/sum) doÄŸru mu?

6. Reproduksiyon
   â–¡ Seed sabitlendi mi?
   â–¡ Versiyonlar loglandÄ± mÄ±?
```

---

## 1ï¸âƒ£4ï¸âƒ£ Week 0 â†’ Week 1 KÃ¶prÃ¼

### ğŸ“ Week 1'de YapacaklarÄ±nÄ±n Teorisi

#### 1. Sentetik Veri
```
y = 3x + 2 + Îµ,  Îµ ~ N(0, 0.1)

Neden sentetik?
  â†’ GerÃ§eÄŸi biliyoruz
  â†’ Kontrol bizde
  â†’ Debug kolay
```

#### 2. Manuel Gradient Descent
```python
# Autograd YOK, sen hesapla:
âˆ‚L/âˆ‚w = ?
âˆ‚L/âˆ‚b = ?

â†’ Zincir kuralÄ±nÄ± Ã‡IPLAK gÃ¶rÃ¼rsÃ¼n
```

#### 3. nn.Module ile EÄŸitim
```python
# Autograd VAR, PyTorch halleder:
loss.backward()

â†’ Pratik workflow Ã¶ÄŸrenirsin
```

#### 4. Train/Val Split
```python
train_data, val_data = split(data, 0.8)

â†’ Overfitting'i CANLI izlersin
```

#### 5. Erken Durdurma
```python
if val_loss > best_val_loss for N epochs:
    break

â†’ Implicit regularization gÃ¶rÃ¼rsÃ¼n
```

### ğŸ¯ AmaÃ§

> "Neden bÃ¶yle?" sorusuna **akÄ±cÄ±** cevap verebilir hale gel.
> Kod sadece resmileÅŸtirme.

---

## ğŸ“š KapanÄ±ÅŸ: Edebiyat Kristalize

**Ã–ÄŸrenme = Hata yÃ¼zeyinde yÃ¼rÃ¼yen gezgin**

**Elinde:**
- EÄŸim pusulasÄ± (gradient)
- AdÄ±m Ã¶lÃ§er (LR)
- Harita yok! (keÅŸfediyorsun)

**VicdanÄ±n:**
- Validation seti
- "Ezberleme" sinyali

**Stratejin:**
- AdamW ile momentum al
- SGD ile sakin sulara Ã§ekil
- Regularization ile dizginle

**KazandÄ±ÄŸÄ±nda:**
- Neden kazandÄ±ÄŸÄ±nÄ± anlat
- KaybettiÄŸinde sebebini bil

**BugÃ¼n yerleÅŸtirdik:** Bu "neden"leri

---

## âœ… Final Self-Assessment

### Matematiksel Derinlik
- [ ] MLE â†’ MSE tÃ¼retimini anlÄ±yorum
- [ ] MAP â†’ Regularization baÄŸlantÄ±sÄ±nÄ± gÃ¶rÃ¼yorum
- [ ] Bias-variance ayrÄ±ÅŸtÄ±rmasÄ±nÄ± formÃ¼le edebiliyorum
- [ ] Condition number'Ä±n etkisini biliyorum

### Pratik Beceri
- [ ] Loss seÃ§imini probabilistik temelle savunabiliyorum
- [ ] Regularization'Ä± prior olarak yorumlayabiliyorum
- [ ] Metrik seÃ§imini probleme gÃ¶re yapabiliyorum
- [ ] Debug protokolÃ¼nÃ¼ ezberim

### BÃ¼tÃ¼nsel AnlayÄ±ÅŸ
- [ ] "Neden bu loss?" â†’ Anlatabiliyorum
- [ ] "Neden bu optimizer?" â†’ Sebepliyorum
- [ ] "Neden overfit oldu?" â†’ TeÅŸhis ediyorum
- [ ] LiteratÃ¼r okuyabilecek temele sahibim

**Hepsi âœ… ise:** Week 1 linear regression'a **hazÄ±rsÄ±n**! ğŸš€

---

## ğŸš€ SÄ±radaki AdÄ±m

```bash
cd /Users/onur/code/novadev-protocol
source .venv/bin/activate

# Teori oturdu mu? Test et:
python week1_tensors/linreg_manual.py

# Her satÄ±rda "neden?" diye sor kendine
# CevabÄ± theory_mathematical.md'de var!
```

**BaÅŸarÄ± Ã–lÃ§Ã¼tÃ¼:**
> Week 1'de kod yazarken:
> "Aha! Ä°ÅŸte bu yÃ¼zden MSE!"
> "Ä°ÅŸte bu yÃ¼zden L2!"
> diyebilmen.

---

**HazÄ±r mÄ±sÄ±n? Matematiksel temeller tamam. ArtÄ±k pratik zamanÄ±!** ğŸ’ª
