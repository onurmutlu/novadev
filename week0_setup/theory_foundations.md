# Week 0: Makine Ã–ÄŸrenmesi Temelleri - Ders NotlarÄ±

**NovaDev v1.0 - Foundation Week**

> "Teori olmadan pratik kÃ¶rdÃ¼r; pratik olmadan teori anlamsÄ±zdÄ±r."

---

## ğŸ“– Bu DÃ¶kÃ¼man HakkÄ±nda

Bu notlar, **makine Ã¶ÄŸrenmesinin temel kavramlarÄ±nÄ±** sÄ±fÄ±rdan, sezgisel bir ÅŸekilde aÃ§Ä±klar. AmaÃ§: kod yazmadan Ã¶nce **zihinsel modeli** doÄŸru kurmak. Kahvenizi alÄ±n, rahat bir yere oturun. â˜•

**Hedef:** Week 1'e baÅŸlamadan Ã¶nce bu sayfadaki tÃ¼m kavramlarÄ± **kendi kelimelerinizle** anlatabilmek.

---

## 0ï¸âƒ£ Neyi Ã–ÄŸreniyoruz? (Bir CÃ¼mle)

**Makine Ã–ÄŸrenmesi = Veri â†’ Fonksiyon Ã¶ÄŸrenmek**

Veri bize hikÃ¢yeyi fÄ±sÄ±ldar, model o fÄ±sÄ±ltÄ±yÄ± **parametreleri** ayarlayarak fonksiyona Ã§evirir.

**Dikkat:** "DoÄŸru" yok; **en iÅŸe yarayan** var.

### ğŸ¯ Zihinsel Ã‡erÃ§eve

```
Veri (Ham Bilgi)  â†’  Model (Ã–ÄŸrenen)  â†’  Tahmin (Ã‡Ä±ktÄ±)
     â†“                      â†“                  â†“
  Ã–rnek:              Parametreler:        SonuÃ§:
  Ev metrajÄ±          w, b              Fiyat tahmini
```

**Analoji:** Bir Ã§ocuÄŸa bisiklet sÃ¼rmeyi Ã¶ÄŸretiyorsun. DÃ¼ÅŸer, kalkar, hata yapar. Sonunda denge noktasÄ±nÄ± **kendisi** bulur. Sen sadece ortamÄ± ve dÃ¶nÃ¼tÃ¼ saÄŸlarsÄ±n.

---

## 1ï¸âƒ£ Model Nedir? (Sihir DeÄŸil, Fonksiyon)

### Matematiksel TanÄ±m

Model = f_Î¸(x)

Burada:
- **x**: Girdi (Ã¶zellikler / features)
- **Î¸** (theta): Parametreler (aÄŸÄ±rlÄ±klar / weights)
- **f**: Fonksiyon (genelde karmaÅŸÄ±k, ama matematiksel)
- **Ã‡Ä±ktÄ±**: Tahmin (Å·)

### ğŸ” Daha Derin BakÄ±ÅŸ

**Bizim Ä°ÅŸimiz:** Ä°yi bir Î¸ bulmak.

"Ä°yi" ne demek?
- EÄŸitim verisinde **dÃ¼ÅŸÃ¼k hata**
- Test verisinde de **genelleme** yapabiliyor
- **Stabil** (kÃ¼Ã§Ã¼k veri deÄŸiÅŸikliÄŸinde Ã§Ã¶kmÃ¼yor)

### ğŸ’¡ Zihinsel Model

**Radyo Metaforu:**
```
Model = Eski radyo
Parametreler = Frekans dÃ¼ÄŸmeleri
EÄŸitim = DÃ¼ÄŸmeleri Ã§evirerek yayÄ±nÄ± netleÅŸtirme
Loss = Parazit miktarÄ±
```

BaÅŸta sadece gÃ¼rÃ¼ltÃ¼ duyarsÄ±n. DÃ¼ÄŸmeleri (parametreleri) Ã§evirdikÃ§e ses (tahmin) netleÅŸir.

### âš ï¸ YaygÄ±n YanÄ±lgÄ±lar

1. **"Model dÃ¼ÅŸÃ¼nÃ¼r"** âŒ
   â†’ Model matematiksel bir fonksiyondur, dÃ¼ÅŸÃ¼nmez. Sadece verdiÄŸin veriye **uyar**.

2. **"Daha bÃ¼yÃ¼k model = daha iyi"** âŒ
   â†’ Bazen evet, bazen aÅŸÄ±rÄ± Ã¶ÄŸrenme (overfit). Denge lazÄ±m.

3. **"Model her ÅŸeyi Ã§Ã¶zer"** âŒ
   â†’ Model sadece **verdiÄŸin verinin deseni**ni Ã¶ÄŸrenir. Veri kÃ¶tÃ¼yse, model kÃ¶tÃ¼.

---

## 2ï¸âƒ£ Veri Nedir? (Ve Neden Kutsal)

### Veri Anatomisi

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Ã–zellik 1  â”‚  Ã–zellik 2  â”‚  Hedef  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚     120     â”‚      3      â”‚   450K  â”‚  â† Bir Ã¶rnek (sample)
â”‚     85      â”‚      2      â”‚   280K  â”‚
â”‚     ...     â”‚     ...     â”‚   ...   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†“              â†“             â†“
  Alan (mÂ²)     Oda sayÄ±sÄ±    Fiyat
```

- **SatÄ±r**: Bir Ã¶rnek / sample / data point
- **SÃ¼tun**: Bir Ã¶zellik / feature / attribute
- **Hedef (Label)**: "DoÄŸru cevap" (supervised learning'de)

### ğŸ¯ Ä°ddia

**Ã–ÄŸrenme = Verideki istikrarlÄ± paternleri bulmak**

"Ä°stikrarlÄ±" derken:
- TesadÃ¼f deÄŸil, **tekrarlanan** iliÅŸkiler
- GÃ¼rÃ¼ltÃ¼den ayÄ±rt edilebilen **sinyal**

### âš–ï¸ Kural 1: i.i.d. VarsayÄ±mÄ±

**i.i.d.** = independent and identically distributed (baÄŸÄ±msÄ±z ve Ã¶zdeÅŸ daÄŸÄ±lÄ±mlÄ±)

**AnlamÄ±:** EÄŸitim ve test verileri **aynÄ± dÃ¼nyadan** gelmeli.

**Ã–rnek - Ä°hlal:**
```
EÄŸitim: 2020 verileri (pandemi Ã¶ncesi)
Test:   2022 verileri (pandemi sonrasÄ±)
â†’ DaÄŸÄ±lÄ±m kaymasÄ±! Model ÅŸaÅŸÄ±rÄ±r.
```

**Ã‡Ã¶zÃ¼m:**
- Train/Val/Test ayÄ±rÄ±mÄ±nÄ± **rastgele** yap
- Zamansal baÄŸÄ±mlÄ±lÄ±k varsa (time series), kronolojik ayÄ±r

### ğŸ“Š Train/Val/Test AyrÄ±mÄ±

```
[â– â– â– â– â– â– â– â– â–¡â–¡â–¡]  = TÃ¼m Veri (100%)
 â†“
[â– â– â– â– â– â– â– â– ] [â–¡] [â–¡]
   Train   Val Test
   70-80%  10-15% 10-15%
```

**Roller:**
- **Train:** Modeli eÄŸit (parametreleri Ã¶ÄŸren)
- **Val:** Hiperparametre ayarla (LR, regularization, vb.)
- **Test:** Final deÄŸerlendirme (bir kez, sonuÃ§larÄ± raporla)

### âš ï¸ Veri SÄ±zÄ±ntÄ±sÄ± (Data Leakage)

**En Tehlikeli Hata!**

```
âŒ YANLIÅ:
1. TÃ¼m veriyi normalize et
2. Sonra train/test ayÄ±r
â†’ Test bilgisi train'e sÄ±zdÄ±!

âœ… DOÄRU:
1. Train/test ayÄ±r
2. Train'den Ã¶ÄŸrendiÄŸin parametrelerle normalize et
3. AynÄ± parametrelerle test'i normalize et
```

---

## 3ï¸âƒ£ KayÄ±p (Loss) = Pusulan

### TanÄ±m

**Loss Fonksiyonu:** Modelin hatasÄ±nÄ± **tek bir sayÄ±**ya indirger.

L(y_gerÃ§ek, y_tahmin)

**AmaÃ§:** Bu sayÄ±yÄ± **minimize** et.

### ğŸ¯ Neden Tek SayÄ±?

Ã‡Ã¼nkÃ¼ optimizasyon algoritmalarÄ± **vektÃ¶r alanÄ±nda tek yÃ¶n** arar. Ã‡ok kriterli hedef, algoritmayÄ± ÅŸaÅŸÄ±rtÄ±r.

### ğŸ“ YaygÄ±n Loss FonksiyonlarÄ±

#### Regresyon: MSE (Mean Squared Error)

MSE = (1/N) Ã— Î£(y_i - Å·_i)Â²

- **Avantaj:** TÃ¼revi kolay, bÃ¼yÃ¼k hatalarÄ± cezalandÄ±rÄ±r
- **Dezavantaj:** Outlier'lara hassas

**Alternatif:** MAE (L1 loss) - daha robust

#### SÄ±nÄ±flandÄ±rma: Cross-Entropy

CE = -Î£ y_i Ã— log(Å·_i)

- **Avantaj:** OlasÄ±lÄ±k daÄŸÄ±lÄ±mlarÄ± iÃ§in ideal
- **KullanÄ±m:** Softmax Ã§Ä±ktÄ±sÄ±yla beraber

### ğŸ’¡ Kural 2: "Ne Ã–lÃ§ersen Ona DÃ¶nÃ¼ÅŸÃ¼rsÃ¼n"

```
Accuracy optimize et â†’ SÄ±nÄ±f dengesizliÄŸinde yanÄ±lÄ±r
F1 optimize et â†’ Precision/Recall dengesini tutar
MSE optimize et â†’ Outlier'lara takÄ±lÄ±r
```

**SeÃ§im Stratejisi:**
1. Ä°ÅŸ problemini anla
2. Hangi hata tipi daha maliyetli?
3. O metriÄŸi seÃ§

### ğŸ§ª Loss EÄŸrisi Okuma

```
Loss
  â”‚
  â”‚â•²
  â”‚ â•²
  â”‚  â•²___
  â”‚      â•²____
  â”‚           â”€â”€â”€â”€â”€â”€â”€  â† Platoya girdi
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Epoch
     Ã–ÄŸrenme       Ä°yileÅŸme       Doyma
     baÅŸlÄ±yor      hÄ±zlÄ±         yavaÅŸladÄ±
```

**SaÄŸlÄ±klÄ± EÄŸitim:**
- BaÅŸta hÄ±zlÄ± dÃ¼ÅŸÃ¼ÅŸ
- Sonra yavaÅŸlama
- Platoya yaklaÅŸma

**Problem Ä°ÅŸaretleri:**
- HiÃ§ dÃ¼ÅŸmÃ¼yor â†’ LR Ã§ok kÃ¼Ã§Ã¼k / model Ã§ok basit
- ZÄ±plÄ±yor â†’ LR Ã§ok bÃ¼yÃ¼k
- Ani dÃ¼ÅŸÃ¼ÅŸ sonra patlama â†’ SayÄ±sal instabilite

---

## 4ï¸âƒ£ Optimizasyon: Gradient Descent

### ğŸ¯ AmaÃ§

L(Î¸) kaybÄ±nÄ± **azaltmak**

### ğŸ§­ Gradient = PusÄ±la

**Gradient (âˆ‡_Î¸ L):** Loss'un parametrelere gÃ¶re **tÃ¼revi**

```
         L(Î¸)
          â”‚
        â•±   â•²
      â•±       â•²
    â•±           â•²
  â•±               â•²
 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  Î¸
         â†‘
    Bu noktadaysan,
    gradient sola iÅŸaret eder
    (eÄŸim negatif)
```

**Sezgi:** DaÄŸÄ±n zirvesinde sisli havada kaybolmuÅŸsun. Elinde sadece **eÄŸim Ã¶lÃ§er** var. En dik iniÅŸ yÃ¶nÃ¼nde kÃ¼Ã§Ã¼k adÄ±m atarsÄ±n.

### ğŸ“ GÃ¼ncelleme KuralÄ±

Î¸_yeni = Î¸_eski - Î· Ã— âˆ‡_Î¸ L

Burada:
- **Î·** (eta): Ã–ÄŸrenme oranÄ± (Learning Rate / LR)
- **âˆ‡_Î¸ L**: Gradient (tÃ¼rev)

### ğŸšï¸ Kural 3: LR Piramidi

```
LR Ã§ok bÃ¼yÃ¼k:
  Loss â†‘â†“â†‘â†“  (zÄ±plÄ±yor, NaN olabilir)
  
LR uygun:
  Loss â•²â•²â•²___ (smooth dÃ¼ÅŸÃ¼ÅŸ)
  
LR Ã§ok kÃ¼Ã§Ã¼k:
  Loss â•²_ (yavaÅŸ Ã¶ÄŸrenme, zaman kaybÄ±)
```

**Pratik ReÃ§ete:**
1. BaÅŸta biraz cesur (1e-3, 1e-2)
2. Platoya gelince dÃ¼ÅŸÃ¼r (schedule)
3. Stabilite yoksa Ã¶nce **LR'Ä±** dÃ¼ÅŸÃ¼r

### ğŸ”„ Gradient Descent TÃ¼rleri

#### Batch GD
- **Her** veriyi gÃ¶r, sonra gÃ¼ncelle
- **Avantaj:** Stabil
- **Dezavantaj:** YavaÅŸ (bÃ¼yÃ¼k veri setlerinde)

#### Stochastic GD (SGD)
- **Her Ã¶rnekten** sonra gÃ¼ncelle
- **Avantaj:** HÄ±zlÄ±, lokal minimum'lardan kaÃ§abilir
- **Dezavantaj:** GÃ¼rÃ¼ltÃ¼lÃ¼

#### Mini-batch GD
- **KÃ¼Ã§Ã¼k gruplar** (32, 64, 128 Ã¶rnek) ile gÃ¼ncelle
- **Pratik standart:** Ä°kisinin dengesi

---

## 5ï¸âƒ£ TensÃ¶rler: Numpy Ama Steroidli

### ğŸ§Š TensÃ¶r Anatomisi

```
Scalar (0D):     5
Vector (1D):     [1, 2, 3]
Matrix (2D):     [[1, 2],
                  [3, 4]]
Tensor (3D+):    [[[...]]]
```

**PyTorch TensÃ¶rÃ¼ = Numpy Array + GPU DesteÄŸi + Autograd**

### ğŸ·ï¸ Ã–nemli Ã–zellikler

#### Shape (Åekil)
```python
x = torch.randn(32, 3, 224, 224)
# (batch_size, channels, height, width)
#       32        3      224     224
```

#### Dtype (Veri Tipi)
- `torch.float32` (default, Ã§oÄŸunlukla yeterli)
- `torch.float16` (hÄ±z iÃ§in, mixed precision)
- `torch.int64` (index'ler iÃ§in)

#### Device (Cihaz)
- `cpu`: Herkes iÃ§in
- `cuda`: NVIDIA GPU
- `mps`: Apple Silicon (M1/M2/M3)

### ğŸ¯ Kural 4: Shape Bilinci

**En Ã§ok zaman kaybÄ±:** Shape uyumsuzluÄŸu

```python
# âŒ Hata
x = torch.randn(10, 5)  # (10, 5)
y = torch.randn(3, 5)   # (3, 5)
z = x + y  # Error! Shape mismatch

# âœ… DoÄŸru
x = torch.randn(10, 5)  # (10, 5)
y = torch.randn(1, 5)   # (1, 5) - broadcast olacak
z = x + y  # OK! â†’ (10, 5)
```

**AlÄ±ÅŸkanlÄ±k:** Her iÅŸlemde `print(x.shape)` yap.

### ğŸ“¡ Broadcasting

**KÃ¼Ã§Ã¼k tensÃ¶r** otomatik olarak **bÃ¼yÃ¼k tensÃ¶r**Ã¼n ÅŸekline uyar.

```
(10, 1) + (1, 5) â†’ (10, 5)
 â†“         â†“
SatÄ±rlara  SÃ¼tunlara
kopyala    kopyala
```

**Kural:**
1. SaÄŸdan baÅŸla
2. Boyutlar eÅŸit ya da biri 1 olmalÄ±
3. Eksik boyutlar 1 kabul edilir

### ğŸ”„ View vs Reshape vs Contiguous

```python
# view: hafÄ±zada yer deÄŸiÅŸtirme
x.view(2, 8)  # HÄ±zlÄ± ama contiguous olmalÄ±

# reshape: gerekirse kopyalar
x.reshape(2, 8)  # Her zaman Ã§alÄ±ÅŸÄ±r

# contiguous: hafÄ±zada dÃ¼zenle
x = x.transpose(0, 1).contiguous()
```

**Ä°pucu:** View hata verirse `contiguous()` ekle.

---

## 6ï¸âƒ£ Autograd: Zinciri Tersten Gez

### ğŸ¯ Hesap GrafiÄŸi (Computational Graph)

```
        x (input)
         â†“
    [Linear Layer]
         â†“
        a (activation)
         â†“
    [Another Layer]
         â†“
      output
         â†“
       Loss
```

**Ä°leri GeÃ§iÅŸ (Forward):** YukarÄ±dan aÅŸaÄŸÄ±ya hesapla

**Geri GeÃ§iÅŸ (Backward):** AÅŸaÄŸÄ±dan yukarÄ±ya tÃ¼revleri hesapla (chain rule)

### ğŸ”— Zincir KuralÄ± Sezgisi

**Analoji:** LEGO'lardan kule yaptÄ±n.

- **Forward:** ParÃ§alarÄ± Ã¼st Ã¼ste koy
- **Backward:** YÄ±karken her parÃ§anÄ±n Ã¼stÃ¼ne binen yÃ¼kÃ¼ hesapla

```
dL/dx = dL/dy Ã— dy/dx
        â†‘       â†‘
    Ãœstten   Lokal
    gelen    tÃ¼rev
```

### ğŸ“ Autograd KullanÄ±mÄ±

```python
# 1. Parametre tanÄ±mla (gradient takibi aÃ§Ä±k)
w = torch.randn(5, 3, requires_grad=True)

# 2. Forward pass
y = model(x)
loss = criterion(y, target)

# 3. Backward (gradientleri hesapla)
loss.backward()

# 4. Gradientleri oku
print(w.grad)  # dL/dw

# 5. GÃ¼ncelle (gradient birikmesin)
with torch.no_grad():
    w -= lr * w.grad
    
# 6. Gradientleri sÄ±fÄ±rla (sonraki iterasyon iÃ§in)
w.grad.zero_()
```

### âš ï¸ YaygÄ±n Hatalar

**1. Gradient birikmesi**
```python
# âŒ Her iterasyonda zero_grad() yok
for epoch in epochs:
    loss.backward()  # Gradientler birikir!
    
# âœ… DoÄŸru
for epoch in epochs:
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()
```

**2. no_grad() unutmak**
```python
# âŒ GÃ¼ncelleme sÄ±rasÄ±nda graf kirlenebilir
w = w - lr * w.grad

# âœ… DoÄŸru
with torch.no_grad():
    w = w - lr * w.grad
```

---

## 7ï¸âƒ£ OptimizatÃ¶rler: Adam vs SGD

### ğŸ”§ Optimizer SeÃ§imi

#### SGD (Stochastic Gradient Descent)
```
Î¸ = Î¸ - lr Ã— gradient
```

**Avantaj:**
- Basit, anlaÅŸÄ±lÄ±r
- Genelleme iyi olabilir
- Az bellek

**Dezavantaj:**
- YavaÅŸ yakÄ±nsama
- LR seÃ§imi kritik

#### Momentum
```
v = Î² Ã— v + gradient
Î¸ = Î¸ - lr Ã— v
```

**Analoji:** Topu daÄŸdan aÅŸaÄŸÄ± yuvarla. Momentum birikir, vadileri aÅŸar.

**Avantaj:**
- Lokal minimum'lardan kaÃ§abilir
- Daha hÄ±zlÄ± yakÄ±nsama

#### Adam (Adaptive Moment Estimation)
```
m = Î²1 Ã— m + (1-Î²1) Ã— gradient        (momentum)
v = Î²2 Ã— v + (1-Î²2) Ã— gradientÂ²       (variance)
Î¸ = Î¸ - lr Ã— m / (âˆšv + Îµ)
```

**Avantaj:**
- Her parametreye **adaptif LR**
- Pratikte Ã§ok iÅŸe yarar
- Hiperparametre hassasiyeti az

**Dezavantaj:**
- Bazen genelleme SGD'den dÃ¼ÅŸÃ¼k olabilir
- Daha fazla bellek

### ğŸ’¡ Kural 5: Pratik ReÃ§ete

```
BaÅŸlangÄ±Ã§:
  Adam + kÃ¼Ã§Ã¼k weight_decay (1e-4)
  LR: 1e-3 veya 1e-4
  
Stabil olunca:
  Ä°steÄŸe baÄŸlÄ± SGD+Momentum'a geÃ§
  (daha iyi genelleme iÃ§in)
```

### âš–ï¸ Weight Decay (L2 Regularization)

**AmaÃ§:** AÄŸÄ±rlÄ±klarÄ± **kÃ¼Ã§Ã¼k** tut â†’ Overfit'i azalt

```
Loss = Loss_data + Î» Ã— Î£(wÂ²)
                      â†‘
                 Regularization
                 terimi
```

**PyTorch'ta:**
```python
optimizer = torch.optim.Adam(
    model.parameters(),
    lr=1e-3,
    weight_decay=1e-4  # L2 reg
)
```

---

## 8ï¸âƒ£ Bias-Variance Trade-off

### ğŸ¯ Ä°kili Sorun

```
Toplam Hata = BiasÂ² + Variance + Noise
               â†“         â†“         â†“
           Underfitting Overfitting KaÃ§Ä±nÄ±lmaz
```

### ğŸ“Š Underfit vs Overfit

```
Performance
    â”‚         â•±â”€â”€â”€â”€â•²  â† Sweet Spot
    â”‚       â•±        â•²
    â”‚     â•±  Overfit  â•²
    â”‚   â•±              â•²
    â”‚ â•± Underfit         â•²
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  Model Complexity
      Basit              KarmaÅŸÄ±k
```

### ğŸ” TeÅŸhis

#### Underfit (Az Ã–ÄŸrenme)
```
Train Loss: YÃ¼ksek (â‰¥ 1.5)
Val Loss:   YÃ¼ksek (â‰¥ 1.5)
```

**Belirtiler:**
- Model yeterince Ã¶ÄŸrenemiyor
- Train loss bile yÃ¼ksek

**Ã‡Ã¶zÃ¼mler:**
- Daha karmaÅŸÄ±k model
- Daha uzun eÄŸitim
- Daha iyi Ã¶zellik mÃ¼hendisliÄŸi
- LR arttÄ±r

#### Overfit (AÅŸÄ±rÄ± Ã–ÄŸrenme)
```
Train Loss: DÃ¼ÅŸÃ¼k (< 0.1)
Val Loss:   YÃ¼ksek (> 1.0)
```

**Belirtiler:**
- Train mÃ¼kemmel, val/test kÃ¶tÃ¼
- Model "ezberleme" yapÄ±yor

**Ã‡Ã¶zÃ¼mler:**
- Daha fazla veri
- Regularization (L2, dropout)
- Early stopping
- Data augmentation
- Daha basit model

### ğŸ“ˆ Kural 6: Val EÄŸrisi Ä°zleme

```
Loss
  â”‚
  â”‚â•²         Train
  â”‚ â•²â•²â•²â•²___________
  â”‚     â•²
  â”‚       â•²â•±â•±â•±â•± Val
  â”‚           â•±
  â”‚         â•±  â† Bu noktada DUR!
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  Epoch
            â†‘
        Overfit
        baÅŸlÄ±yor
```

**Early Stopping:**
Val loss N epoch boyunca iyileÅŸmezse **dur**.

---

**(DevamÄ± theory_advanced.md'de...)**

---

## ğŸ“š Week 0 Ã–zet

### Temel Kavramlar Cheat Sheet

| Kavram | Ã–zet | Dikkat |
|--------|------|--------|
| **Model** | f_Î¸(x) fonksiyonu | DÃ¼ÅŸÃ¼nmez, uyar |
| **Loss** | Hata Ã¶lÃ§Ã¼tÃ¼ | Ne Ã¶lÃ§ersen ona dÃ¶nÃ¼ÅŸÃ¼rsÃ¼n |
| **Gradient** | TÃ¼rev / yÃ¶n | PusÄ±lan |
| **LR** | AdÄ±m bÃ¼yÃ¼klÃ¼ÄŸÃ¼ | En kritik hiperparametre |
| **Tensor** | Ã‡ok boyutlu array | Shape bilinci! |
| **Autograd** | Otomatik tÃ¼rev | Chain rule otomatik |
| **Optimizer** | GÃ¼ncelleme kuralÄ± | Adam pratik, SGD teorik |
| **Overfit** | Ezber yapmak | Val loss izle |

### ğŸ¯ Mastery Checklist

Kendinize sorun:

- [ ] TensÃ¶r shape'ini gÃ¶zÃ¼mde canlandÄ±rabiliyorum
- [ ] Forward-backward akÄ±ÅŸÄ±nÄ± anlatabiliyorum
- [ ] LR'Ä±n etkisini tahmin edebiliyorum
- [ ] Overfit/underfit'i teÅŸhis edebiliyorum
- [ ] Loss eÄŸrisinden sorun Ã§Ä±karabilirim
- [ ] Gradient nedir, neden Ã¶nemlidir biliyorum

**EÄŸer hepsi âœ… ise:** Week 1'e hazÄ±rsÄ±n! ğŸš€

---

## ğŸ§ª Mini AlÄ±ÅŸtÄ±rmalar (10-15 dk)

### AlÄ±ÅŸtÄ±rma 1: LR Deneyi

**Senaryo:** LR = 0.01'de loss dÃ¼zgÃ¼n dÃ¼ÅŸÃ¼yor.

**Sorular:**
1. LR'Ä± 0.1 yaparsan ne olur?
2. LR'Ä± 0.001 yaparsan ne olur?
3. Hangi belirtiden anlarsÄ±n?

**Cevaplar (aÅŸaÄŸÄ±da)**

### AlÄ±ÅŸtÄ±rma 2: Overfit Tespiti

**GÃ¶zlem:**
```
Epoch 50:  Train Loss = 0.05, Val Loss = 0.08
Epoch 100: Train Loss = 0.01, Val Loss = 0.15
```

**Sorular:**
1. Bu overfit mi?
2. Ne yapmalÄ±sÄ±n?
3. Hangi metriÄŸe bakarak karar verdin?

### AlÄ±ÅŸtÄ±rma 3: Broadcasting

**Kod:**
```python
x = torch.randn(10, 1)  # (10, 1)
y = torch.randn(1, 5)   # (1, 5)
z = x + y               # Shape?
```

**SonuÃ§ shape'i nedir? Neden?**

---

### ğŸ“ Cevaplar

**AlÄ±ÅŸtÄ±rma 1:**
1. LR = 0.1: Loss zÄ±plar, NaN olabilir (Ã§ok bÃ¼yÃ¼k adÄ±m)
2. LR = 0.001: YavaÅŸ Ã¶ÄŸrenir (Ã§ok kÃ¼Ã§Ã¼k adÄ±m)
3. Loss grafiÄŸinden: smooth ise iyi, zÄ±plÄ±yorsa bÃ¼yÃ¼k

**AlÄ±ÅŸtÄ±rma 2:**
1. Evet, overfit! Train dÃ¼ÅŸÃ¼yor, val artÄ±yor
2. Early stopping (Epoch 50'de dur), regularization ekle
3. Val loss'un artÄ±ÅŸÄ±ndan

**AlÄ±ÅŸtÄ±rma 3:**
```python
z.shape  # torch.Size([10, 5])
# (10, 1) â†’ (10, 5) sÃ¼tunlara kopyala
# (1, 5)  â†’ (10, 5) satÄ±rlara kopyala
```

---

## ğŸš€ SÄ±radaki AdÄ±m

Bu temeller Ã¼zerine **Week 1** inÅŸa edilecek:
- Bu teorileri **linear regression**'da pratiÄŸe dÃ¶keceÄŸiz
- Manuel gradient descent yazacaÄŸÄ±z
- PyTorch API'sini kullanacaÄŸÄ±z
- Train/val split'i uygulayacaÄŸÄ±z

**HazÄ±r mÄ±sÄ±n?** `python week1_tensors/linreg_manual.py` ğŸ¯
