# Week 0: Ä°leri Konular & Pratik Ä°puÃ§larÄ±

**NovaDev v1.0 - Ä°leri Seviye Notlar**

> "Teori bilirsen debug yaparsÄ±n, pratiÄŸi bilirsen hÄ±zlÄ± gidersin."

---

## 9ï¸âƒ£ Reproduksiyon: Bilimin OmurgasÄ±

### ğŸ¯ Neden Ã–nemli?

**Ä°ddia:** AynÄ± koÅŸullarda aynÄ± sonuÃ§ â†’ GÃ¼ven

Bilim **tekrarlanabilirlik** Ã¼zerine kuruludur. ML'de bu Ã¶zellikle zor ama kritik.

### ğŸ”’ Seed (Tohum) Sabitleme

```python
import random
import numpy as np
import torch

def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    
    # Deterministic mode (biraz yavaÅŸlatÄ±r)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

# KullanÄ±m
set_seed(42)
```

**Dikkat:** MPS'de (Apple Silicon) tam determinizm garanti edilemez.

### ğŸ“‹ Experiment Log Template

```markdown
## Deney 2025-10-06-A

**AmaÃ§:** LR'Ä±n etkisini test et

**Setup:**
- Model: LinearRegression (input=10, output=1)
- Optimizer: Adam
- LR: 0.01
- Batch size: 32
- Epochs: 100
- Seed: 42
- Device: MPS

**Versiyon:**
- Python: 3.13.7
- PyTorch: 2.8.0
- OS: macOS 14.6

**SonuÃ§:**
- Train Loss: 0.045
- Val Loss: 0.052
- Time: 15.3s

**GÃ¶zlem:**
Loss smooth dÃ¼ÅŸtÃ¼, overfit yok.

**Sonraki adÄ±m:**
LR'Ä± 0.001'e dÃ¼ÅŸÃ¼r, karÅŸÄ±laÅŸtÄ±r.
```

### ğŸ’¾ Checkpoint Kaydetme

```python
def save_checkpoint(model, optimizer, epoch, loss, path):
    torch.save({
        'epoch': epoch,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'loss': loss,
        'random_state': torch.get_rng_state(),
    }, path)
    
def load_checkpoint(path, model, optimizer):
    checkpoint = torch.load(path)
    model.load_state_dict(checkpoint['model_state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    torch.set_rng_state(checkpoint['random_state'])
    return checkpoint['epoch'], checkpoint['loss']
```

### ğŸ“Š Kural 7: GÃ¼nlÃ¼k Deney

> "BugÃ¼n 1 deney koÅŸmadÄ±ysan, Ã¶ÄŸrenmedin."

**Neden?**
- Teori okumak â‰  Anlamak
- Kod yazmak â‰  Ã–ÄŸrenmek
- **Deney yapmak = GerÃ§ek Ã¶ÄŸrenme**

**Pratik:**
- Her gÃ¼n en az 1 hiperparametre deÄŸiÅŸtir
- Sonucu not al
- Ã–nceki deneylerle karÅŸÄ±laÅŸtÄ±r

---

## ğŸ”Ÿ DonanÄ±m: MPS/CUDA/CPU GerÃ§ekleri

### ğŸ–¥ï¸ Cihaz KarÅŸÄ±laÅŸtÄ±rmasÄ±

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                â”‚   CPU    â”‚   MPS    â”‚  CUDA    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ HÄ±z (7B model) â”‚  YavaÅŸ   â”‚  Orta    â”‚  HÄ±zlÄ±   â”‚
â”‚ Uyumluluk      â”‚  %100    â”‚  %95     â”‚  %98     â”‚
â”‚ Kurulum        â”‚  Kolay   â”‚  Kolay   â”‚  Orta    â”‚
â”‚ Bellek         â”‚  DÃ¼ÅŸÃ¼k   â”‚  Orta    â”‚  YÃ¼ksek  â”‚
â”‚ Maliyet        â”‚  $0      â”‚  Mac     â”‚  GPU $   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸšï¸ Batch Size Etkisi

```python
# KÃ¼Ã§Ã¼k batch (8, 16)
âœ… Daha gÃ¼rÃ¼ltÃ¼lÃ¼ gradient
âœ… Bazen daha iyi genelleme
âœ… Daha az bellek
âŒ YavaÅŸ (GPU'yu tam kullanmÄ±yor)

# BÃ¼yÃ¼k batch (128, 256)
âœ… HÄ±zlÄ± (GPU parallelizmi)
âœ… Stabil gradient
âŒ Daha fazla bellek
âŒ "Keskin" minimumlara takÄ±labilir
```

**Optimal Strateji:**
1. GPU belleÄŸine sÄ±ÄŸacak en bÃ¼yÃ¼k batch
2. Ama 256'dan bÃ¼yÃ¼k nadiren gerekir
3. Ä°kinin katlarÄ± tercih et (32, 64, 128)

### âš¡ Mixed Precision Training

```python
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

for epoch in epochs:
    optimizer.zero_grad()
    
    # Forward pass float16'da
    with autocast():
        output = model(input)
        loss = criterion(output, target)
    
    # Backward pass scale edilmiÅŸ
    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()
```

**KazanÃ§:**
- ~2x hÄ±z
- Daha az bellek
- Minimal kayÄ±p (genelde fark edilmez)

**Dikkat:** MPS'de henÃ¼z tam destek yok (2024 itibarÄ±yla).

### ğŸ¯ Kural 8: Basit Ã‡alÄ±ÅŸ

```
Ä°lk iterasyon:
  âŒ Mixed precision + multi-GPU + compiled model
  âœ… Tek GPU + float32 + basit kod
  
Ä°kinci iterasyon:
  âœ… Åimdi optimize et
```

**Neden?**
- Erken optimizasyon = motivasyon katili
- Ã–nce **Ã§alÄ±ÅŸÄ±r** yap, sonra **hÄ±zlÄ±** yap

---

## 1ï¸âƒ£1ï¸âƒ£ SayÄ±sal SaÄŸlÄ±k (Numerical Stability)

### ğŸ”¥ YaygÄ±n Sorunlar

#### Problem 1: NaN (Not a Number)

**Belirtiler:**
```python
Loss: 1.234
Loss: 0.876
Loss: 0.543
Loss: nan  â† Patlama!
```

**Nedenler:**
1. LR Ã§ok bÃ¼yÃ¼k â†’ Gradient patlamasÄ±
2. BÃ¶lme sÄ±fÄ±ra â†’ Infinity
3. Log(0) veya log(negatif)
4. SayÄ± taÅŸmasÄ± (overflow)

**Ã‡Ã¶zÃ¼mler:**
```python
# 1. LR kÃ¼Ã§Ã¼lt
lr = lr / 10

# 2. Gradient clipping
torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

# 3. GÃ¼venli operasyonlar
log_prob = torch.log(prob + 1e-8)  # Epsilon ekle

# 4. Initialization
# KÃ¼Ã§Ã¼k aÄŸÄ±rlÄ±klar baÅŸlat
```

#### Problem 2: Gradient Explosion

**Belirtiler:**
```
Gradient norm: 10.5
Gradient norm: 23.8
Gradient norm: 156.9
Gradient norm: 8921.4  â† Patlama!
```

**Ã‡Ã¶zÃ¼m: Gradient Clipping**
```python
# Norm-based clipping
torch.nn.utils.clip_grad_norm_(
    model.parameters(),
    max_norm=1.0  # Gradient'leri 1.0'a sÄ±nÄ±rla
)

# Value-based clipping
torch.nn.utils.clip_grad_value_(
    model.parameters(),
    clip_value=0.5
)
```

#### Problem 3: Gradient Vanishing

**Belirtiler:**
```
Layer 1 gradient: 0.5
Layer 2 gradient: 0.05
Layer 3 gradient: 0.0005
Layer 4 gradient: 0.0000001  â† Kayboluyor!
```

**Neden:**
Derin aÄŸlarda tÃ¼revler Ã§arpÄ±lÄ±rken kÃ¼Ã§Ã¼lÃ¼r.

**Ã‡Ã¶zÃ¼mler:**
1. **ReLU activation** (sigmoid yerine)
2. **Batch Normalization**
3. **Residual connections** (ResNet'te)
4. Daha iyi **initialization** (Xavier, He)

### ğŸ¯ Kural 9: Ä°lk Panik AnÄ±nda

```
1. LR'Ä± yarÄ±ya indir
2. Tekrar dene
3. %80 oranla dÃ¼zelir!
```

**TeÅŸhis AkÄ±ÅŸÄ±:**
```
NaN gÃ¶rdÃ¼n mÃ¼?
  â†“
LR'Ä± /2 yap
  â†“
DÃ¼zeldi mi?
  â”œâ”€ Evet â†’ Devam et
  â””â”€ HayÄ±r â†’ Gradient clip ekle
              â†“
            DÃ¼zeldi mi?
              â”œâ”€ Evet â†’ Devam et
              â””â”€ HayÄ±r â†’ Model/veri kontrol et
```

---

## 1ï¸âƒ£2ï¸âƒ£ GerÃ§ek Hayata BaÄŸ

### ğŸ¯ Neden Bu Kadar Teori?

**Senaryo 1: Model Ã§alÄ±ÅŸmÄ±yor**
```
Acemi:
  "Kod Ã§alÄ±ÅŸmÄ±yor, Stack Overflow'a bakalÄ±m"
  â†’ 3 saat kopyala-yapÄ±ÅŸtÄ±r
  â†’ Hala Ã§alÄ±ÅŸmÄ±yor

Expert:
  "Loss eÄŸrisine bakalÄ±m..."
  â†’ LR Ã§ok bÃ¼yÃ¼k (5 dakika)
  â†’ loss.backward() sonrasÄ± zero_grad() yok (2 dakika)
  â†’ DÃ¼zeldi! (7 dakika)
```

**Fark:** Teori â†’ hÄ±zlÄ± teÅŸhis

### ğŸ” HÄ±zlÄ± TeÅŸhis Check-list

```markdown
## Model Ã‡alÄ±ÅŸmÄ±yor - 6 AdÄ±m Protokol

### 1. LR KontrolÃ¼
- [ ] LR Ã§ok mu yÃ¼ksek? (loss zÄ±plÄ±yor mu?)
- [ ] LR Ã§ok mu dÃ¼ÅŸÃ¼k? (loss hareket etmiyor mu?)

### 2. Veri SÄ±zÄ±ntÄ±sÄ±
- [ ] Train/val/test doÄŸru mu ayrÄ±ldÄ±?
- [ ] Normalizasyon train'den mi Ã¶ÄŸrenildi?
- [ ] Shuffle yapÄ±ldÄ± mÄ±?

### 3. Shape/Device KontrolÃ¼
- [ ] TÃ¼m tensor'ler aynÄ± device'da mÄ±?
- [ ] Shape'ler beklediÄŸin gibi mi?
- [ ] Dtype uyumlu mu? (float32 vs int64)

### 4. Gradient AkÄ±ÅŸÄ±
- [ ] zero_grad() her iterasyonda mÄ±?
- [ ] backward() Ã§aÄŸrÄ±lÄ±yor mu?
- [ ] requires_grad=True parametrelerde mi?

### 5. Loss Fonksiyonu
- [ ] DoÄŸru loss seÃ§ildi mi? (MSE â‰  CE)
- [ ] Loss reduction doÄŸru mu? (mean/sum)
- [ ] Target shape doÄŸru mu?

### 6. Seed & Randomness
- [ ] Seed sabitlendi mi?
- [ ] SonuÃ§lar reprodukÄ±labilir mi?
```

### ğŸ“ GerÃ§ek Hikayeler

#### Hikaye 1: "Ä°ki HaftayÄ± BoÅŸa HarcadÄ±m"
```
Ã–ÄŸrenci: Val loss hiÃ§ iyileÅŸmiyor!
         2 hafta denedim, her ÅŸeyi denedim.

Mentor:  Veri nasÄ±l ayrÄ±ldÄ±?
Ã–ÄŸrenci: Ã–nce normalize ettim, sonra ayÄ±rdÄ±m.
Mentor:  Ä°ÅŸte sorun! Test bilgisi train'e sÄ±zdÄ±.

Ã‡Ã¶zÃ¼m:   10 dakika (veri pipeline'Ä± dÃ¼zelt)
Ders:    Teori bilmek = Zaman kazanmak
```

#### Hikaye 2: "Model PatladÄ±"
```
MÃ¼hendis: Production'da model NaN dÃ¶ndÃ¼rÃ¼yor!
          EÄŸitimde sorun yoktu.

Debug:    - Prod'da veri daÄŸÄ±lÄ±mÄ± deÄŸiÅŸmiÅŸ
          - Outlier'lar var
          - Log(0) patlamasÄ±

Ã‡Ã¶zÃ¼m:    - Input clipping ekle
          - Robust loss kullan (Huber)
          - Monitoring kur

Ders:     Train â‰  Prod, savunmalÄ± kod yaz
```

### ğŸ’¡ Professional vs Amateur

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  â”‚   Amateur    â”‚  Professionalâ”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Problem          â”‚ Panik yapar  â”‚ Checklist    â”‚
â”‚ Debug            â”‚ Random try   â”‚ Systematic   â”‚
â”‚ Experiment       â”‚ KaydÄ± yok    â”‚ Log tutar    â”‚
â”‚ Code             â”‚ KarmaÅŸÄ±k     â”‚ Basit        â”‚
â”‚ Reproduksiyon    â”‚ Åans         â”‚ Garanti      â”‚
â”‚ Error            â”‚ "Ã‡alÄ±ÅŸmÄ±yor" â”‚ "LR bÃ¼yÃ¼k"   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“š Edebiyat Ã–zeti

**Makine Ã–ÄŸrenmesi = Fonksiyon tasarlama sanatÄ±**

SÃ¼reÃ§:
1. **Veri**ni ciddiye al (garbage in = garbage out)
2. **Hedef**'i net kur (loss fonksiyonu)
3. **Model**'i baÅŸlat (parametreler rastgele)
4. **Gradient** ile yÃ¶n bul (pusÄ±la)
5. **LR** ile adÄ±m at (hÄ±z kontrol)
6. **Val** ile kontrol et (vicdan)
7. **Regularize** et (ezber deÄŸil Ã¶ÄŸrenme)
8. **Tekrarla** (epoch dÃ¶ngÃ¼sÃ¼)

**SonuÃ§:** Sihir deÄŸil, **gÃ¼venilir iÅŸlev**
- Veri deÄŸiÅŸtikÃ§e uyarlanabilir
- HatasÄ± Ã¶lÃ§Ã¼lebilir
- Tekrarlanabilir

---

## ğŸ¨ Ek Analojiler

### 1. Gradient = Damla Analojisi

```
        DaÄŸ Zirvesi
           â•±â•²
         â•±    â•²
       â•±        â•²
     â•±     â—      â•²    â† Su damlasÄ±
   â•±       â†“        â•²
 â•±         â†“          â•²
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  â† Vadi (minimum)
```

Su damlasÄ±:
- En dik eÄŸimden akar (gradient)
- KÃ¼Ã§Ã¼k adÄ±mlar atar
- Yerel minimum'lara takÄ±labilir
- Momentum ile atlar

**Model eÄŸitimi:** Binlerce damla aynÄ± anda akÄ±yor (mini-batch)

### 2. Loss YÃ¼zeyi = TopoÄŸrafya

```
3D Loss Landscape:

        â•±â•²    â•±â•²
       â•±  â•²  â•±  â•²
      â•± â—  â•²â•±    â•²   â† Yerel minimum
     â•±            â•²
    â•±              â•²
   â•±      â—†         â•²  â† Global minimum
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

â— = KÃ¶tÃ¼ (yerel)
â—† = Ä°yi (global)
```

**AmaÃ§:** â—†'ye ulaÅŸmak
**Problem:** â—'de takÄ±labilirsin
**Ã‡Ã¶zÃ¼m:** Momentum, SGD'nin gÃ¼rÃ¼ltÃ¼sÃ¼, farklÄ± init

### 3. Model = Radyo (GeliÅŸtirilmiÅŸ)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  [âˆ¿] [â—‹] [â•] [â–«]   â”‚  â† DÃ¼ÄŸmeler (parametreler)
â”‚                      â”‚
â”‚   ğŸ”Š  â™« â™ª â™«          â”‚  â† Ã‡Ä±ktÄ± (tahmin)
â”‚                      â”‚
â”‚   S/N Ratio: 85%    â”‚  â† Loss (gÃ¼rÃ¼ltÃ¼/sinyal)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

BaÅŸlangÄ±Ã§: Her dÃ¼ÄŸme rastgele â†’ GÃ¼rÃ¼ltÃ¼
EÄŸitim:    DÃ¼ÄŸmeleri ayarla â†’ MÃ¼zik netleÅŸir
Gradient:  "Bu dÃ¼ÄŸmeyi sola Ã§evir" sinyali
LR:        DÃ¼ÄŸmeyi ne kadar Ã§evireceÄŸin
```

### 4. Overfit = Ezber Yapan Ã–ÄŸrenci

```
Ezberci Ã–ÄŸrenci:
  SÄ±nav sorularÄ±: %100 âœ“
  FarklÄ± sorular: %40 âœ—
  â†’ AnlamadÄ±, ezberledi

Ã–ÄŸrenen Ã–ÄŸrenci:
  SÄ±nav sorularÄ±: %85 âœ“
  FarklÄ± sorular: %80 âœ“
  â†’ KavradÄ±, genelledi
```

**Model:** Overfit = Ezber
**Ã‡Ã¶zÃ¼m:** Regularization = "Anlama" zorla

---

## ğŸ§ª Ä°leri AlÄ±ÅŸtÄ±rmalar

### AlÄ±ÅŸtÄ±rma 4: Loss Landscape Hayal Et

**Senaryo:** 2D loss yÃ¼zeyi hayal et.

```
Loss(w1, w2)
      â†‘
      â”‚     â•±â•²
      â”‚   â•±    â•²
      â”‚ â•±        â•²
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ w1
         w2
```

**Sorular:**
1. Gradient hangi yÃ¶nÃ¼ gÃ¶sterir?
2. LR bÃ¼yÃ¼kse ne olur?
3. Momentum ne iÅŸe yarar?

### AlÄ±ÅŸtÄ±rma 5: Batch Size Deneyi

**Kod:**
```python
model_small = train(model, batch_size=8)
model_large = train(model, batch_size=256)
```

**Sorular:**
1. Hangisi daha hÄ±zlÄ± epoch bitirir?
2. Hangisi daha az bellek kullanÄ±r?
3. Hangisi daha iyi genelleme yapabilir?

### AlÄ±ÅŸtÄ±rma 6: Debug Senaryosu

**Problem:**
```
Epoch 1: Loss = 2.5
Epoch 2: Loss = nan
```

**Sorular:**
1. Ä°lk ÅŸÃ¼phen ne?
2. Hangi satÄ±rÄ± kontrol edersin?
3. HÄ±zlÄ± fix'in ne?

---

### ğŸ“ Cevaplar

**AlÄ±ÅŸtÄ±rma 4:**
1. En dik iniÅŸ yÃ¶nÃ¼ (negatif gradient)
2. Minimum'u aÅŸar, zÄ±plar (diverge)
3. Momentum vadileri hÄ±zlÄ± aÅŸar, lokal min'lerden kaÃ§ar

**AlÄ±ÅŸtÄ±rma 5:**
1. batch_size=256 daha hÄ±zlÄ± (GPU parallelizmi)
2. batch_size=8 daha az bellek
3. batch_size=8 bazen daha iyi (gÃ¼rÃ¼ltÃ¼lÃ¼ gradient â†’ regularization)

**AlÄ±ÅŸtÄ±rma 6:**
1. Ä°lk ÅŸÃ¼phe: **LR Ã§ok bÃ¼yÃ¼k**
2. Kontrol: `print(loss)` ve `print(model.parameters())`
3. Fix: `lr = lr / 10`

---

## ğŸ“ KapanÄ±ÅŸ: Sen Neredesin?

### Self-Assessment

**BaÅŸlangÄ±Ã§ (Week 0 Ã¶ncesi):**
```
[ ] TensÃ¶r nedir bilmiyordum
[ ] Gradient manuel hesaplamÄ±ÅŸtÄ±m ama autograd yabancÄ±
[ ] Loss fonksiyonu seÃ§imi ÅŸans iÅŸiydi
[ ] Overfit/underfit farkÄ±nÄ± bilmiyordum
```

**Hedef (Week 0 sonrasÄ±):**
```
[âœ“] TensÃ¶r operasyonlarÄ±nÄ± anlÄ±yorum
[âœ“] Autograd akÄ±ÅŸÄ±nÄ± gÃ¶rselleÅŸtirebiliyorum
[âœ“] Loss/optimizer/LR iliÅŸkisini kavradÄ±m
[âœ“] Debug yapabilirim (check-list ile)
[âœ“] Deney tasarlayÄ±p kaydedebilirim
```

**Ä°leri (Week 4+):**
```
[ ] Kendi modelimi sÄ±fÄ±rdan yazabilirim
[ ] Production pipeline kurabilirim
[ ] LiteratÃ¼r okuyup uygulayabilirim
[ ] Yeni problemlere transfer edebilirim
```

---

## ğŸ“– Ã–nerilen Kaynaklar

### Temel Seviye
- **3Blue1Brown**: Neural Networks (YouTube series)
- **Fast.ai**: Practical Deep Learning for Coders
- **PyTorch Tutorials**: Official docs

### Orta Seviye
- **Deep Learning Book** (Goodfellow et al.) - BÃ¶lÃ¼m 1-5
- **Dive into Deep Learning** (d2l.ai)
- **CS231n** (Stanford) - Lecture notes

### Ä°leri Seviye
- **Papers with Code**: Son araÅŸtÄ±rmalar
- **Distill.pub**: GÃ¶rsel aÃ§Ä±klamalar
- **ArXiv Sanity**: Paper takibi

---

## ğŸš€ Week 1'e GeÃ§iÅŸ

**Åu ana kadar:**
- âœ… Teoriyi oturtttuk
- âœ… KavramlarÄ± sindirik
- âœ… Sezgi geliÅŸtirdik

**Åimdi:**
- ğŸ¯ **Linear Regression** ile pratiÄŸe geÃ§
- ğŸ’» Kodu elle yaz (manuel GD)
- ğŸ§ª Deney yap (LR, batch size, vb.)
- ğŸ“Š SonuÃ§larÄ± kaydet

**Motto:** "Theory without practice is lame, practice without theory is blind."

---

## âœ… Final Checklist

AÅŸaÄŸÄ±daki sorularÄ± **kendinize** sorun:

### Kavramsal Anla
- [ ] Model = f_Î¸(x) ne demek aÃ§Ä±klayabiliyorum
- [ ] Gradient nedir, niye Ã¶nemlidir biliyorum
- [ ] Overfit vs underfit farkÄ±nÄ± gÃ¶rselleÅŸtirebiliyorum
- [ ] LR'Ä±n etkisini tahmin edebiliyorum

### Pratik Beceri
- [ ] PyTorch tensor oluÅŸturabilirim
- [ ] Forward-backward-update dÃ¶ngÃ¼sÃ¼nÃ¼ yazabilirim
- [ ] Loss eÄŸrisini okuyabilirim
- [ ] Basit hatalarÄ± debug edebilirim

### Mental Model
- [ ] "Neden?" sorusunu cevaplayabiliyorum
- [ ] Hiperparametre seÃ§imlerini savunabiliyorum
- [ ] Sorun gÃ¶rdÃ¼ÄŸÃ¼mde check-list uygulayabiliyorum

**Hepsi âœ… ise:** Week 1 linear regression seni bekliyor! ğŸŠ

---

## ğŸ“Œ Son SÃ¶z

> "Bir Ã¶ÄŸretmen tahtada anlattÄ±, sen anlamadÄ±n.  
> Ä°ki YouTube video izledin, yine anlamadÄ±n.  
> ÃœÃ§ kez kendi ellerin ile kod yazdÄ±n, **anladÄ±n**."

**Week 0 bittiÄŸinde:**
- Kafanda net bir **zihinsel model** olmalÄ±
- **Neden?** sorularÄ±nÄ±n cevabÄ±nÄ± bilmelisin
- Ä°lk hatayÄ± **teÅŸhis edebilmelisin**

**BaÅŸarÄ± Ã¶lÃ§Ã¼tÃ¼:** Week 1'de kod yazarken "Aha! Ä°ÅŸte o yÃ¼zden!" diyebilmen.

---

**HazÄ±r mÄ±sÄ±n?**

```bash
cd /Users/onur/code/novadev-protocol
source .venv/bin/activate
python week1_tensors/linreg_manual.py
```

ğŸš€ **Let's build!**
