# Week 0: Temel Kavramlar - Ãœniversite Seviyesi

**NovaDev v1.0 - Akademik YaklaÅŸÄ±m**

> "AdÄ±m adÄ±m, kavramlarÄ± tanÄ±mlayarak, sezgi + matematik dengesiyle. Week 1'de ne yaptÄ±ÄŸÄ±nÄ± NEDEN yaptÄ±ÄŸÄ±nÄ± bileceksin."

---

## ğŸ¯ Bu DÃ¶kÃ¼man HakkÄ±nda

**Hedef Kitle:** 
- âœ… theory_intro.md'yi okudum, temel sezgi var
- âœ… Åimdi daha formal/akademik aÃ§Ä±klamalar istiyorum
- âœ… Hafif matematik korkutmuyor, formÃ¼lleri anlamak istiyorum
- âœ… "Neden bÃ¶yle?" sorularÄ±na daha derin cevaplar arÄ±yorum

**Seviye:** Ãœniversite GiriÅŸ (Orta)
**SÃ¼re:** 90-120 dakika
**Stil:** TanÄ±m â†’ Sezgi â†’ Hafif Matematik â†’ Pratik
**Hedef:** Week 1 kod yazarken "bu formÃ¼l nereden geldi?" bilmek

---

## 1ï¸âƒ£ Makine Ã–ÄŸrenmesi Nedir? (Formal TanÄ±m)

### ğŸ“ Matematiksel Ã‡erÃ§eve

**TanÄ±m:** Makine Ã¶ÄŸrenmesi, veriden **Ã¶rÃ¼ntÃ¼** Ã¶ÄŸrenip yeni veriler iÃ§in **tahmin** yapan yÃ¶ntemlerin genel adÄ±dÄ±r.

**BileÅŸenler:**
```
Girdi (x):        Ã–zellik vektÃ¶rÃ¼, x âˆˆ â„áµˆ
Ã‡Ä±ktÄ± (Å·):        Tahmin, Å· = f_Î¸(x)
Parametreler (Î¸): Ayarlanabilir aÄŸÄ±rlÄ±klar
Model (f_Î¸):      x â†¦ Å· fonksiyonu
```

### ğŸ¯ AmaÃ§ FormÃ¼lasyonu

**Hedef:** Parametreleri Ã¶yle ayarla ki tahminler gerÃ§eÄŸe yakÄ±n olsun.

**Matematiksel:**
```
Î¸* = argmin_Î¸ E[(y - f_Î¸(x))Â²]
              â†‘
         Beklenen hata (tÃ¼m olasÄ± veri Ã¼zerinde)
```

**Pratikte:**
```
Î¸* â‰ˆ argmin_Î¸ (1/N) Î£áµ¢ L(yáµ¢, f_Î¸(xáµ¢))
                      â†‘
              Ampirik kayÄ±p (elimizdeki veri)
```

### ğŸ’¡ Sezgisel AÃ§Ä±klama

**Ãœniversite dersi analojisi:**
```
Girdi (x): Ã–ÄŸrencinin Ã¶nceki notlarÄ±, devam, Ã¶dev skoru
Model (f_Î¸): "Final notu tahmin" formÃ¼lÃ¼
Parametreler (Î¸): FormÃ¼ldeki aÄŸÄ±rlÄ±klar (devam %30, Ã¶dev %40, vize %30)
EÄŸitim: GeÃ§miÅŸ Ã¶ÄŸrencilere bakarak aÄŸÄ±rlÄ±klarÄ± ayarla
Test: Yeni Ã¶ÄŸrenci gelince final notunu tahmin et
```

---

## 2ï¸âƒ£ Veri ve Veri KÃ¼mesi (Dataset)

### ğŸ“Š Terminoloji

#### Ã–rnek (Sample)
**TanÄ±m:** Tek bir veri satÄ±rÄ±

**Notasyon:** 
```
(xáµ¢, yáµ¢) â† i'nci Ã¶rnek
xáµ¢ âˆˆ â„áµˆ  â† d-boyutlu Ã¶zellik vektÃ¶rÃ¼
yáµ¢ âˆˆ â„   â† hedef deÄŸer (regresyon)
yáµ¢ âˆˆ {0,1,...,K-1} â† sÄ±nÄ±f etiketi (classification)
```

#### Ã–zellik (Feature)
**TanÄ±m:** Ã–rneÄŸi tanÄ±mlayan nitelikler

**Tipler:**
- **SayÄ±sal (Numerical):** SÃ¼rekli (fiyat, aÄŸÄ±rlÄ±k) veya kesikli (sayÄ±m)
- **Kategorik (Categorical):** Sonlu kÃ¼me (renk, ÅŸehir)
- **SÄ±ralÄ± (Ordinal):** SÄ±ra var (dÃ¼ÅŸÃ¼k/orta/yÃ¼ksek)

#### Etiket (Label)
**TanÄ±m:** DoÄŸru cevap (supervised learning'de)

**GÃ¶zetimli vs GÃ¶zetimsiz:**
```
GÃ¶zetimli:    D = {(xâ‚,yâ‚), (xâ‚‚,yâ‚‚), ..., (xâ‚™,yâ‚™)}
GÃ¶zetimsiz:   D = {xâ‚, xâ‚‚, ..., xâ‚™}  (etiket yok)
```

### ğŸ“ Ã–ÄŸrenme ParadigmalarÄ±

#### 1. Supervised Learning (GÃ¶zetimli)
```
Hedef: Etiketli Ã¶rneklerden f: X â†’ Y Ã¶ÄŸren

Alt tÃ¼rler:
  - Regression: Y = â„ (sayÄ±sal tahmin)
  - Classification: Y = {0,1,...,K-1} (sÄ±nÄ±f tahmini)
  
Ã–rnekler:
  - Ev fiyatÄ± tahmini (regression)
  - Spam sÄ±nÄ±flama (binary classification)
  - Rakam tanÄ±ma (multi-class classification)
```

#### 2. Unsupervised Learning (GÃ¶zetimsiz)
```
Hedef: Verinin yapÄ±sÄ±nÄ±/Ã¶rÃ¼ntÃ¼sÃ¼nÃ¼ keÅŸfet

Alt tÃ¼rler:
  - Clustering: Benzer gruplarÄ± bul
  - Dimensionality Reduction: Boyut azalt (PCA, t-SNE)
  - Density Estimation: DaÄŸÄ±lÄ±mÄ± Ã¶ÄŸren
  
Ã–rnekler:
  - MÃ¼ÅŸteri segmentasyonu (clustering)
  - Veri gÃ¶rselleÅŸtirme (t-SNE)
  - Anomaly detection
```

#### 3. Reinforcement Learning (PekiÅŸtirmeli)
```
Hedef: Ã–dÃ¼l maksimize eden politika Ã¶ÄŸren

BileÅŸenler:
  - Agent: Karar verici
  - Environment: Ã‡evre
  - State: Durum
  - Action: Aksiyon
  - Reward: Ã–dÃ¼l
  
Ã–rnekler:
  - Oyun oynayan AI (AlphaGo)
  - Robot kontrolÃ¼
  - Otonom araÃ§lar
```

---

## 3ï¸âƒ£ EÄŸitimâ€“DoÄŸrulamaâ€“Test AyrÄ±mÄ±

### ğŸ”„ Neden BÃ¶lÃ¼yoruz?

**Ana Sebep:** **Genelleme** kabiliyetini Ã¶lÃ§mek

**Tehlike:** EÄŸitim verisine "ezber" (overfit)

### ğŸ“¦ Veri BÃ¶lme Stratejisi

#### Standard Split
```
D (tÃ¼m veri, N Ã¶rnek)
  â†“
â”œâ”€ D_train (70%, 0.7N)  â†’ Parametreleri Ã¶ÄŸren
â”œâ”€ D_val   (15%, 0.15N) â†’ Hiperparametreleri seÃ§
â””â”€ D_test  (15%, 0.15N) â†’ Final performans
```

**Matematiksel:**
```
Î¸* = argmin_Î¸ L_train(Î¸)           â† EÄŸitim
Î»* = argmin_Î» L_val(Î¸*(Î»))         â† Validation (hiperparametre)
Performans = L_test(Î¸*(Î»*))        â† Test (bir kez!)
```

#### Stratified Split (SÄ±nÄ±flama)
```
SÄ±nÄ±f oranlarÄ±nÄ± koru:

Orijinal: %80 negatif, %20 pozitif
  â†“
Train: %80 negatif, %20 pozitif
Val:   %80 negatif, %20 pozitif
Test:  %80 negatif, %20 pozitif
```

**Neden?** Dengesiz sÄ±nÄ±flarda rastgele split tehlikeli!

#### Temporal Split (Zaman Serisi)
```
Zaman â†’
[â”€â”€â”€â”€â”€Trainâ”€â”€â”€â”€â”€][â”€Valâ”€][Test]
   GeÃ§miÅŸ      YakÄ±n   Gelecek
               Gelecek
```

**Kritik:** GeleceÄŸi geÃ§miÅŸe ASLA sÄ±zdÄ±rma!

### âš ï¸ AltÄ±n Kurallar

```
1. Test setine sadece BÄ°R KEZ bak (final rapor)
2. Validation ile hiperparametre seÃ§
3. Test'e bakÄ±p ayar yapma â†’ kendini kandÄ±rÄ±rsÄ±n
4. Cross-validation (k-fold) veri azsa kullan
```

### ğŸ“Š i.i.d. VarsayÄ±mÄ±

**TanÄ±m:** **i**ndependent and **i**dentically **d**istributed

**Matematiksel:**
```
(xáµ¢, yáµ¢) ~ p(x, y)  baÄŸÄ±msÄ±z ve Ã¶zdeÅŸ daÄŸÄ±lÄ±mlÄ±

Yani:
  p_train(x, y) = p_test(x, y)
```

**GerÃ§ek Hayatta Ä°hlaller:**

#### Covariate Shift
```
p_train(x) â‰  p_test(x)  ama  p(y|x) aynÄ±

Ã–rnek: Kamera modeli deÄŸiÅŸti (gÃ¶rÃ¼ntÃ¼ daÄŸÄ±lÄ±mÄ± kaydÄ±)
       ama "kedi" tanÄ±mÄ± aynÄ±
```

#### Concept Drift
```
p_train(y|x) â‰  p_test(y|x)  ama  p(x) aynÄ±

Ã–rnek: "Spam" tanÄ±mÄ± zamanla evrildi
       ama email formatÄ± aynÄ±
```

#### Prior Shift
```
p_train(y) â‰  p_test(y)  (sÄ±nÄ±f oranlarÄ±)

Ã–rnek: EÄŸitimde %50 pozitif
       GerÃ§ekte %5 pozitif
```

---

## 4ï¸âƒ£ AmaÃ§ Fonksiyonu: KayÄ±p (Loss) ve Risk

### ğŸ“ KayÄ±p Fonksiyonu (Loss Function)

**TanÄ±m:** Tek bir Ã¶rnek iÃ§in hata Ã¶lÃ§Ã¼sÃ¼

**Notasyon:** â„“(y, Å·) veya L(y, f(x))

### ğŸ¯ Regresyon Loss'larÄ±

#### Mean Squared Error (MSE)
```
L_MSE = (1/N) Î£áµ¢ (yáµ¢ - Å·áµ¢)Â²

TÃ¼rev:
âˆ‚L/âˆ‚Å·áµ¢ = -2(yáµ¢ - Å·áµ¢)

Ã–zellikler:
  âœ“ Konveks (tek minimum)
  âœ“ Smooth (her yerde tÃ¼revlenebilir)
  âœ“ BÃ¼yÃ¼k hatalarÄ± aÄŸÄ±r cezalar
  âœ— Outlier'a hassas
```

**Ne Zaman:**
- Normal daÄŸÄ±lÄ±mlÄ± hatalar
- BÃ¼yÃ¼k hatalar gerÃ§ekten kÃ¶tÃ¼
- Standard regression

#### Mean Absolute Error (MAE)
```
L_MAE = (1/N) Î£áµ¢ |yáµ¢ - Å·áµ¢|

TÃ¼rev:
âˆ‚L/âˆ‚Å·áµ¢ = -sign(yáµ¢ - Å·áµ¢)

Ã–zellikler:
  âœ“ Outlier'a robust
  âœ“ Median predict eder
  âœ— SÄ±fÄ±rda tÃ¼rev tanÄ±msÄ±z (kÃ¶ÅŸeli)
```

**Ne Zaman:**
- Outlier Ã§ok
- Robust tahmin gerekli

#### Huber Loss (Hibrit)
```
         { (1/2)(y-Å·)Â²      if |y-Å·| â‰¤ Î´
L_Huber = {
         { Î´|y-Å·| - Î´Â²/2    if |y-Å·| > Î´

Ã–zellikler:
  âœ“ KÃ¼Ã§Ã¼k hata: MSE (smooth)
  âœ“ BÃ¼yÃ¼k hata: MAE (robust)
  âœ“ En iyi denge
```

### ğŸ² SÄ±nÄ±flama Loss'larÄ±

#### Binary Cross-Entropy (BCE)
```
L_BCE = -(1/N) Î£áµ¢ [yáµ¢ log(páµ¢) + (1-yáµ¢) log(1-páµ¢)]

Burada páµ¢ = Ïƒ(f(xáµ¢)), Ïƒ = sigmoid

TÃ¼rev:
âˆ‚L/âˆ‚f = p - y  (sigmoid ile gÃ¼zel!)

Ã–zellikler:
  âœ“ OlasÄ±lÄ±k tahmini
  âœ“ YanlÄ±ÅŸ Ã¶zgÃ¼vene aÄŸÄ±r ceza
  âœ“ Konveks
```

#### Categorical Cross-Entropy
```
L_CE = -(1/N) Î£áµ¢ Î£â‚– yáµ¢â‚– log(páµ¢â‚–)

Burada páµ¢ = softmax(f(xáµ¢))

Multi-class iÃ§in standard loss
```

### ğŸ“Š Risk KavramÄ±

**Expected Risk (GerÃ§ek AmaÃ§):**
```
R(f) = E_(x,y)~p(x,y) [â„“(y, f(x))]
       â†‘
    TÃ¼m olasÄ± veriler Ã¼zerinde beklenti
```

**Problem:** p(x,y)'yi bilmiyoruz!

**Ã‡Ã¶zÃ¼m: Empirical Risk Minimization (ERM)**
```
RÌ‚(f) = (1/N) Î£áµ¢ â„“(yáµ¢, f(xáµ¢))
        â†‘
    Elimizdeki Ã¶rneklerle yaklaÅŸÄ±klÄ±k

Hedef: Î¸* = argmin_Î¸ RÌ‚(f_Î¸)
```

---

## 5ï¸âƒ£ Optimizasyon: Gradient Descent

### ğŸ—» Geometrik Sezgi

**KayÄ±p yÃ¼zeyi = DaÄŸ arazisi**

```
L(Î¸)
  â†‘
  â”‚     â•±â•²    â•±â•²
  â”‚   â•±    â•²â•±    â•²
  â”‚ â•±              â•²
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Î¸
  
Hedef: En dip noktaya in (global minimum)
AraÃ§: Gradient (eÄŸim vektÃ¶rÃ¼)
```

### ğŸ“ Gradient Descent FormÃ¼lÃ¼

**GÃ¼ncelleme KuralÄ±:**
```
Î¸_{t+1} = Î¸_t - Î· âˆ‡_Î¸ L(Î¸_t)
          â†‘     â†‘    â†‘
       Yeni   LR  Gradient
```

**Gradient:**
```
âˆ‡_Î¸ L = [âˆ‚L/âˆ‚Î¸â‚, âˆ‚L/âˆ‚Î¸â‚‚, ..., âˆ‚L/âˆ‚Î¸â‚]áµ€

En dik YÃœKSELIÅ yÃ¶nÃ¼ â†’ Negatifini al = Ä°NÄ°Å
```

### ğŸšï¸ Learning Rate (Î·)

**Kritik Hiperparametre!**

```
Î· Ã§ok bÃ¼yÃ¼k:
  Î¸ â”€â”€â”€â†’ â”¼ â†â”€â”€â”€ Î¸'
         â†‘
    Minimum'u aÅŸar, diverge!
    
Î· optimal:
  Î¸ â”€â”€â”€â†’ Â· â”€â”€â”€â†’ Â· â”€â”€â”€â†’ â—
         Smooth convergence
         
Î· Ã§ok kÃ¼Ã§Ã¼k:
  Î¸ â†’ Â· â†’ Â· â†’ Â· â†’ ...
      Ã‡ok yavaÅŸ, zaman kaybÄ±
```

**Teorik:** Î· < 2/L (L = Lipschitz sabiti) garanti eder, ama pratikte deneysel seÃ§eriz.

### ğŸ’ Varyantlar

#### Batch Gradient Descent
```
g_t = (1/N) Î£áµ¢ âˆ‡_Î¸ â„“(yáµ¢, f_Î¸(xáµ¢))

Î¸_{t+1} = Î¸_t - Î· g_t

ArtÄ±: Stabil, deterministik
Eksi: YavaÅŸ (N bÃ¼yÃ¼kse)
```

#### Stochastic Gradient Descent (SGD)
```
Rastgele bir Ã¶rnek i seÃ§:
g_t = âˆ‡_Î¸ â„“(yáµ¢, f_Î¸(xáµ¢))

Î¸_{t+1} = Î¸_t - Î· g_t

ArtÄ±: HÄ±zlÄ±, memory-efficient
Eksi: GÃ¼rÃ¼ltÃ¼lÃ¼, zikzak
```

#### Mini-Batch GD â­
```
Rastgele B Ã¶rnek seÃ§ (batch):
g_t = (1/B) Î£áµ¢âˆˆbatch âˆ‡_Î¸ â„“(yáµ¢, f_Î¸(xáµ¢))

Î¸_{t+1} = Î¸_t - Î· g_t

ArtÄ±: HÄ±z + stabilite dengesi, GPU parallel
Standart: B = 32, 64, 128, 256
```

### ğŸš€ GeliÅŸmiÅŸ Optimizerler

#### Momentum
```
v_t = Î² v_{t-1} + g_t
Î¸_t = Î¸_{t-1} - Î· v_t

Î² tipik: 0.9

Sezgi: Top yuvarlanÄ±yor, ivme birikir
      â†’ Vadide sallanma azalÄ±r
      â†’ KÃ¼Ã§Ã¼k tepeleri aÅŸabilir
```

#### Nesterov Momentum
```
"Ã–nce atla, sonra bak" prensibi

Î¸_lookahead = Î¸ - Î² v
g_lookahead = âˆ‡L(Î¸_lookahead)
v_t = Î² v_{t-1} + g_lookahead
Î¸_t = Î¸_{t-1} - Î· v_t

Daha proaktif, oscillation azalÄ±r
```

#### Adam (Adaptive Moment Estimation) â­
```
m_t = Î²â‚ m_{t-1} + (1-Î²â‚) g_t     [First moment, momentum]
v_t = Î²â‚‚ v_{t-1} + (1-Î²â‚‚) g_tÂ²    [Second moment, variance]

mÌ‚_t = m_t / (1 - Î²â‚áµ—)             [Bias correction]
vÌ‚_t = v_t / (1 - Î²â‚‚áµ—)

Î¸_t = Î¸_{t-1} - Î· mÌ‚_t / (âˆšvÌ‚_t + Îµ)

Hiperparametreler:
  Î²â‚ = 0.9 (momentum)
  Î²â‚‚ = 0.999 (variance)
  Îµ = 1e-8 (numerical stability)

Her parametreye adaptif LR!
```

#### AdamW
```
Adam'Ä±n L2 dÃ¼zeltilmiÅŸ versiyonu

Î¸_t = Î¸_{t-1} - Î· (mÌ‚_t / âˆšvÌ‚_t + Î» Î¸_{t-1})
                                   â†‘
                            DoÄŸru weight decay

Pratik Standart: AdamW + kÃ¼Ã§Ã¼k Î» (1e-4)
```

### ğŸ“ˆ LR Schedule (Learning Rate ProgramÄ±)

#### ReduceLROnPlateau
```
if val_loss not improving for patience epochs:
    Î· = Î· / factor
    
Tipik: patience=10, factor=10
```

#### Cosine Annealing
```
Î·_t = Î·_min + (Î·_max - Î·_min) Ã— (1 + cos(Ï€t/T)) / 2

  Î·
  â”‚â•²
  â”‚ â•²___
  â”‚     â•²___
  â”‚         â•²___
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ t
  
Smooth decay
```

#### Step Decay
```
Î·_t = Î·_0 Ã— Î³^âŒŠt/sâŒ‹

Î³ = 0.1, s = 30 epoch

Belli epoch'larda LR'Ä± dÃ¼ÅŸÃ¼r
```

#### Warmup (BÃ¼yÃ¼k Modellerde)
```
Ä°lk W epoch'ta LR'Ä± yavaÅŸ yavaÅŸ artÄ±r:

Î·_t = Î·_target Ã— min(1, t/W)

PatlamayÄ± Ã¶nler
```

---

## 6ï¸âƒ£ Tensors, Åekiller ve Autograd

### ğŸ§Š Tensor Nedir?

**TanÄ±m:** Ã‡ok boyutlu sayÄ± dizisi + metadata

```
Tensor = numpy array + dtype + device

x = torch.randn(64, 3, 224, 224, device='mps', dtype=torch.float32)
                 â†‘   â†‘   â†‘    â†‘      â†‘         â†‘
              Batch RGB  H    W   Device   Data type
```

**Boyutlar:**
- 0D: Scalar (5)
- 1D: Vector ([1, 2, 3])
- 2D: Matrix ([[1,2],[3,4]])
- 3D+: Tensor

### ğŸ“ Shape Operations

#### Ã–nemli Ä°ÅŸlemler
```python
x.shape           # BoyutlarÄ± dÃ¶ndÃ¼r
x.view(...)       # Yeniden ÅŸekillendir (memory'de contiguous olmalÄ±)
x.reshape(...)    # View'e benzer ama copy yapabilir
x.permute(...)    # BoyutlarÄ± yer deÄŸiÅŸtir
x.transpose(...)  # Ä°ki boyutu yer deÄŸiÅŸtir
x.squeeze()       # Size=1 boyutlarÄ± kaldÄ±r
x.unsqueeze(dim)  # Yeni boyut ekle
```

**En SÄ±k Hata:**
```
RuntimeError: size mismatch
â†’ print(x.shape) her kritik noktada!
```

### ğŸ“¡ Broadcasting Rules

**Kural:**
1. SaÄŸdan sola hizala
2. Boyutlar eÅŸit VEYA biri 1 olmalÄ±
3. Eksik boyut 1 kabul edilir

**Ã–rnekler:**
```
(3, 1, 5) + (4, 5) â†’ (3, 4, 5) âœ“
  3 1 5
    4 5
  â”€â”€â”€â”€â”€
  3 4 5

(3, 2) + (3, 1) â†’ (3, 2) âœ“
  3 2
  3 1
  â”€â”€â”€
  3 2

(3, 2) + (2, 3) â†’ ERROR âœ—
  3 2
  2 3
  â”€â”€â”€
  X X  (uyuÅŸmuyor!)
```

### ğŸ”„ Computational Graph & Autograd

**Sezgi:** Her iÅŸlem bir "fatura" keser

#### Forward Pass
```
x (requires_grad=True)
  â†“ [iÅŸlem: square]
a = xÂ²
  â†“ [iÅŸlem: multiply]
b = a Ã— 3
  â†“ [iÅŸlem: sum]
L = Î£b

Her iÅŸlem grafiÄŸe kaydedilir
```

#### Backward Pass (Zincir KuralÄ±)
```
L.backward()

âˆ‚L/âˆ‚L = 1
âˆ‚L/âˆ‚b = âˆ‚L/âˆ‚L Ã— âˆ‚L/âˆ‚b = 1 Ã— 1 = 1
âˆ‚L/âˆ‚a = âˆ‚L/âˆ‚b Ã— âˆ‚b/âˆ‚a = 1 Ã— 3 = 3
âˆ‚L/âˆ‚x = âˆ‚L/âˆ‚a Ã— âˆ‚a/âˆ‚x = 3 Ã— 2x = 6x

x.grad = 6x â† SonuÃ§ burada
```

### âš™ï¸ Autograd Detaylar

```python
# 1. Gradient tracking aÃ§Ä±k
x = torch.tensor([1.0], requires_grad=True)

# 2. Forward (graph oluÅŸur)
y = x ** 2

# 3. Backward (gradient hesapla)
y.backward()

# 4. Gradient'i oku
print(x.grad)  # 2x = 2.0

# 5. SÄ±fÄ±rla (yoksa birikir!)
x.grad.zero_()

# 6. GÃ¼ncellemede tracking kapat
with torch.no_grad():
    x -= lr * x.grad
```

**Kritik:** `zero_grad()` unutma â†’ gradient accumulation!

---

## 7ï¸âƒ£ Overfit & Underfit: Bias-Variance

### ğŸ“Š Ä°ki UÃ§

```
Underfit:               Overfit:
  
Train: KÃ¶tÃ¼           Train: MÃ¼kemmel
Val:   KÃ¶tÃ¼           Val:   KÃ¶tÃ¼

Model yetersiz        Model ezber yaptÄ±
```

### ğŸ¯ TeÅŸhis

**Loss EÄŸrileri:**
```
Loss
  â”‚
  â”‚ Underfit:
  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Train
  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Val
  â”‚   (ikisi de yÃ¼ksek)
  â”‚
  â”‚ Overfit:
  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•²___ Train (dÃ¼ÅŸÃ¼k)
  â”‚            â•±â”€â”€ Val (yÃ¼ksek)
  â”‚           â†‘
  â”‚      Overfit baÅŸladÄ±
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Epoch
```

### ğŸ›¡ï¸ Regularization (DÃ¼zenleme)

#### L2 Regularization (Ridge)
```
L_total = L_data + Î»/2 Ã— ||Î¸||Â²
                   â†‘
              Regularization term

Î»: regularization katsayÄ±sÄ± (1e-4, 1e-3, 1e-2)

Etki: BÃ¼yÃ¼k aÄŸÄ±rlÄ±klarÄ± cezalar â†’ sade model
```

#### L1 Regularization (Lasso)
```
L_total = L_data + Î» Ã— ||Î¸||â‚

Etki: BazÄ± aÄŸÄ±rlÄ±klarÄ± TAM SIFIR yapar (sparsity)
     â†’ Ã–zellik seÃ§imi
```

#### Elastic Net
```
L_total = L_data + Î»â‚||Î¸||â‚ + Î»â‚‚||Î¸||Â²

L1 + L2 kombinasyonu
```

#### Dropout (Derin AÄŸlarda)
```
Training:
  Her nÃ¶ron p olasÄ±lÄ±kla "kapanÄ±r"
  
Test:
  TÃ¼m nÃ¶ronlar aktif
  Ã‡Ä±ktÄ±lar p ile scale edilir

Etki: Co-adaptation'Ä± kÄ±rar
      Ensemble effect
```

#### Early Stopping â­
```
best_val_loss = âˆ
patience_counter = 0

for epoch in epochs:
    train()
    val_loss = validate()
    
    if val_loss < best_val_loss:
        best_val_loss = val_loss
        save_model()
        patience_counter = 0
    else:
        patience_counter += 1
        
    if patience_counter >= patience:
        break  # DUR!

En basit ve etkili regularization!
```

### ğŸ“ Bias-Variance Trade-off

**Matematiksel:**
```
E[(y - Å·)Â²] = BiasÂ²(Å·) + Var(Å·) + ÏƒÂ²
               â†‘          â†‘       â†‘
           Systematic Sensitivity  Irreducible
           error      to data     noise
```

**Denge:**
```
Error
  â”‚
  â”‚â•²              â† Total Error
  â”‚ â•²
  â”‚  â•²  â•±        
  â”‚   â•²â•±         â† Bias (underfit bÃ¶lgesi)
  â”‚    â•²         â† Variance (overfit bÃ¶lgesi)
  â”‚     â•²___
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Model Complexity
  Simple      Complex
  
  Underfit  Sweet   Overfit
            Spot
```

---

## 8ï¸âƒ£ Feature Engineering & Scaling

### ğŸ“ Neden Ã–lÃ§ekleme?

**Problem: Ã‡arpÄ±k Loss YÃ¼zeyi**

```
Ã–zellik 1: [0, 1]
Ã–zellik 2: [0, 10000]

Hessian:
H = [  1      0   ]
    [  0   10â¸   ]

Condition number Îº = Î»_max/Î»_min = 10â¸ â†’ KÃ–TÃœ!

GD zikzak yapar:
        Î¸â‚‚
         â†‘
    â•±â•²  â”‚  â•±â•²     â† Ã‡ok dik
   â•±  â•² â”‚ â•±  â•²
  â•±    â•²â”‚â•±    â•²
 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Î¸â‚
```

**Ã‡Ã¶zÃ¼m: Standardization**

```
x' = (x - Î¼) / Ïƒ

SonuÃ§: Î¼=0, Ïƒ=1
Îº â‰ˆ 1 â†’ Yuvarlak yÃ¼zey â†’ GD dÃ¼z iner!
```

### ğŸ“Š Ã–lÃ§ekleme YÃ¶ntemleri

#### Z-Score (Standardization)
```
x' = (x - Î¼) / Ïƒ

Î¼ = sample mean
Ïƒ = sample std

SonuÃ§: Î¼'=0, Ïƒ'=1
Avantaj: Outlier'larÄ± korur
```

#### Min-Max Scaling
```
x' = (x - x_min) / (x_max - x_min)

SonuÃ§: [0, 1] aralÄ±ÄŸÄ±
Dezavantaj: Outlier'a hassas
```

#### Robust Scaling
```
x' = (x - median) / IQR

IQR = Qâ‚ƒ - Qâ‚ (interquartile range)

Avantaj: Outlier'a robust
```

### ğŸ·ï¸ Kategorik DeÄŸiÅŸkenler

#### One-Hot Encoding
```
Renk: ['kÄ±rmÄ±zÄ±', 'mavi', 'yeÅŸil']

     kÄ±rmÄ±zÄ±  mavi  yeÅŸil
     [1,      0,    0]
     [0,      1,    0]
     [0,      0,    1]

Avantaj: SÄ±ra varsayÄ±mÄ± yok
Dezavantaj: High cardinality'de boyut patlamasÄ±
```

#### Label Encoding
```
['kÄ±rmÄ±zÄ±', 'mavi', 'yeÅŸil'] â†’ [0, 1, 2]

Dikkat: SÄ±ra anlamÄ± verir! (mavi > kÄ±rmÄ±zÄ±?)
Tree-based modellerde OK, linear'da dikkat
```

#### Target Encoding
```
Kategori â†’ Mean(target | kategori)

Dikkat: LEAKAGE riski!
Ã‡Ã¶zÃ¼m: Cross-validation ile yap
```

#### Learnable Embeddings
```
Kategori â†’ DÃ¼ÅŸÃ¼k boyutlu dense vektÃ¶r (Ã¶ÄŸrenilir)

'istanbul' â†’ [0.5, -0.3, 0.8, ...]  (d=128)
'ankara'   â†’ [0.2, 0.4, -0.1, ...]

Derin aÄŸlarda standart
```

---

## 9ï¸âƒ£ DeÄŸerlendirme Metrikleri

### ğŸ“ Regresyon Metrikleri

#### MSE / RMSE
```
MSE = (1/N) Î£ (y - Å·)Â²
RMSE = âˆšMSE

Avantaj: Birim anlamlÄ± (RMSE)
Dezavantaj: Outlier'a hassas
```

#### MAE
```
MAE = (1/N) Î£ |y - Å·|

Avantaj: Outlier'a robust
Dezavantaj: TÃ¼rev sÄ±fÄ±rda tanÄ±msÄ±z
```

#### RÂ² (Coefficient of Determination)
```
RÂ² = 1 - SS_res / SS_tot

SS_res = Î£(y - Å·)Â²
SS_tot = Î£(y - È³)Â²

Yorum: RÂ²=0.85 â†’ Model varyansÄ±n %85'ini aÃ§Ä±klÄ±yor
RÂ² < 0 â†’ Model ortalamadan kÃ¶tÃ¼!
```

### ğŸ¯ Classification Metrikleri

#### Confusion Matrix
```
                Predicted
             Pos      Neg
Actual Pos   TP   |   FN
       Neg   FP   |   TN
```

#### Precision & Recall
```
Precision = TP / (TP + FP)
  "Pozitif dediÄŸimin ne kadarÄ± doÄŸru?"

Recall = TP / (TP + FN)
  "GerÃ§ek pozitiflerin ne kadarÄ±nÄ± yakaladÄ±m?"
```

#### F1 Score
```
F1 = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)

Harmonik ortalama
â†’ KÃ¼Ã§Ã¼k deÄŸerlere aÄŸÄ±rlÄ±k verir
```

#### ROC-AUC vs PR-AUC
```
ROC: TPR vs FPR
  â†’ Dengeli sÄ±nÄ±flarda iyi

PR: Precision vs Recall
  â†’ Ä°mbalanced data'da daha bilgilendirici

Ä°mbalanced (Ã¶rn. %1 pozitif):
  â†’ PR-AUC kullan!
```

#### Calibration
```
Model: "%80 olasÄ±lÄ±kla pozitif"
GerÃ§ek: 100 Ã¶rneÄŸin 80'i pozitif mi?

Calibration curve:
  MÃ¼kemmel: y = x doÄŸrusu
  Sapma var: Recalibration gerekli
```

---

## ğŸ”Ÿ Data Leakage (Veri SÄ±zÄ±ntÄ±sÄ±)

### ğŸš¨ TanÄ±m

**Leakage:** Test/validation bilgisinin eÄŸitime sÄ±zmasÄ±

**SonuÃ§:** Yapay yÃ¼ksek performans, production'da felaket

### ğŸ“… Temporal Leakage

```
âŒ YANLIÅ:
  Rastgele split (2019, 2020 karÄ±ÅŸÄ±k)
  â†’ GeleceÄŸi bilerek geÃ§miÅŸi tahmin!

âœ… DOÄRU:
  Zamansal split:
  Train: 2019-01 ~ 2019-12
  Val:   2020-01 ~ 2020-03
  Test:  2020-04 ~ 2020-06
```

### ğŸ”— Target Leakage

```
Hedef: Kredi geri Ã¶denecek mi?

âŒ Ã–zellik: "geri_Ã¶deme_planÄ±"
  â†’ Bu bilgi ancak Ã¶deme BAÅLADIÄINDA var!
  â†’ Tahmin anÄ±nda bilinmez

âœ… Ã–zellik: "gelir", "yaÅŸ", "kredi_skoru"
  â†’ Tahmin anÄ±nda bilinir
```

### ğŸ§® Preprocessing Leakage

```python
# âŒ YANLIÅ
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_all_scaled = scaler.fit_transform(X_all)  # TÃ¼m veri!
X_train, X_test = train_test_split(X_all_scaled)

# âœ… DOÄRU
X_train, X_test = train_test_split(X_all)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)  # Sadece train!
X_test_scaled = scaler.transform(X_test)        # AynÄ± scaler, transform only
```

### ğŸ” Leakage Tespiti

**Checklist:**
```
1. Bu Ã¶zellik tahmin anÄ±nda bilinir mi?
2. Correlation > 0.95 hedefle? (ÅŸÃ¼pheli!)
3. Zaman sÄ±rasÄ±nÄ± bozduk mu?
4. Test'e dokunduk mu (preprocessing'de)?
5. Cross-validation dÃ¼zgÃ¼n mÃ¼? (grup bazlÄ± gerekebilir)
```

---

## 1ï¸âƒ£1ï¸âƒ£ Probabilistik BakÄ±ÅŸ

### ğŸ² Maximum Likelihood Estimation (MLE)

**Fikir:** GÃ¶zlenen verinin olasÄ±lÄ±ÄŸÄ±nÄ± maksimize et

```
Î¸* = argmax_Î¸ p(Data | Î¸)
             â†‘
        Likelihood

Log-likelihood (Ã§alÄ±ÅŸmak kolay):
Î¸* = argmax_Î¸ Î£áµ¢ log p(yáµ¢ | xáµ¢, Î¸)
```

### ğŸ“ MSE'nin Probabilistik KÃ¶keni

**VarsayÄ±m:** Gaussian noise

```
y = f_Î¸(x) + Îµ,  Îµ ~ N(0, ÏƒÂ²)

â†’ p(y|x,Î¸) = N(y; f_Î¸(x), ÏƒÂ²)

Log-likelihood:
log p(y|x,Î¸) = -(y - f_Î¸(x))Â² / (2ÏƒÂ²) + const

Maksimize et:
argmax Î£ log p(yáµ¢|xáµ¢,Î¸)
= argmin Î£ (yáµ¢ - f_Î¸(xáµ¢))Â²
           â†‘
          MSE!
```

**SonuÃ§:** MSE minimize = Gaussian MLE

### ğŸ¯ Cross-Entropy'nin KÃ¶keni

**VarsayÄ±m:** Bernoulli (binary classification)

```
y âˆˆ {0, 1}
p(y=1|x,Î¸) = Ïƒ(f_Î¸(x))

Log-likelihood:
log p(y|x,Î¸) = y log p + (1-y) log(1-p)

Maksimize et:
argmax Î£ [yáµ¢ log páµ¢ + (1-yáµ¢) log(1-páµ¢)]
= argmin -Î£ [yáµ¢ log páµ¢ + (1-yáµ¢) log(1-páµ¢)]
            â†‘
    Binary Cross-Entropy!
```

**SonuÃ§:** BCE minimize = Bernoulli MLE

### ğŸ“Š MAP (Maximum A Posteriori)

**Bayes KuralÄ±:**
```
p(Î¸|D) âˆ p(D|Î¸) Ã— p(Î¸)
  â†‘        â†‘        â†‘
Posterior  Like   Prior
```

**MAP:**
```
Î¸* = argmax_Î¸ p(Î¸|D)
   = argmax_Î¸ [log p(D|Î¸) + log p(Î¸)]
   = argmin_Î¸ [-log p(D|Î¸) - log p(Î¸)]
                    â†‘            â†‘
                  Loss      Regularization!
```

#### L2 = Gaussian Prior
```
Prior: Î¸ ~ N(0, 1/Î»)

log p(Î¸) = -Î»/2 Ã— ||Î¸||Â²

MAP:
argmin [Loss + Î»/2 Ã— ||Î¸||Â²]
                â†‘
            L2 (Ridge)!
```

#### L1 = Laplace Prior
```
Prior: Î¸ ~ Laplace(0, 1/Î»)

log p(Î¸) = -Î» Ã— ||Î¸||â‚

MAP:
argmin [Loss + Î» Ã— ||Î¸||â‚]
                â†‘
            L1 (Lasso)!
```

**Mesaj:**
> Loss ve regularization rastgele deÄŸil,
> probabilistik varsayÄ±mlarÄ±n sonucu!

---

## 1ï¸âƒ£2ï¸âƒ£ SayÄ±sal Stabilite

### ğŸ”´ Problemler

#### 1. NaN / Inf
```
Sebepler:
  - LR Ã§ok bÃ¼yÃ¼k
  - Gradient explosion
  - SayÄ±sal taÅŸma (exp, log)

Belirtiler:
  Loss â†’ NaN
  Parametre â†’ Â±Inf
```

#### 2. Gradient Vanishing
```
Derin aÄŸlarda:
  - KÃ¼Ã§Ã¼k gradientler Ã§arpÄ±lÄ±r
  - SonuÃ§ â†’ 0

Ã‡Ã¶zÃ¼m:
  - ReLU (sigmoid yerine)
  - Batch/LayerNorm
  - Skip connections (Week 2+)
```

#### 3. Gradient Explosion
```
Gradientler bÃ¼yÃ¼r â†’ patlar

Ã‡Ã¶zÃ¼m:
  - Gradient clipping
  - KÃ¼Ã§Ã¼k LR
  - Normalizasyon
```

### ğŸš‘ Ä°lk YardÄ±m ProtokolÃ¼

```
Problem: Training unstable, NaN

1. LR'Ä± yarÄ±ya indir
   â†’ Ã‡oÄŸu zaman dÃ¼zelir

2. Ã–zellikleri standardize et
   â†’ Loss yÃ¼zeyi yuvarlanÄ±r

3. zero_grad() kontrol et
   â†’ Gradient accumulation olmasÄ±n

4. Loss/Metric doÄŸru mu?
   â†’ Regresyon â‰  CE

5. Shape/dtype/device?
   â†’ print(x.shape, x.dtype, x.device)

6. Seed sabitle
   â†’ ReprodÃ¼ksiyon
```

---

## 1ï¸âƒ£3ï¸âƒ£ Deney Disiplini

### ğŸ”¬ Bilimsel YÃ¶ntem

```
1. GÃ–ZLEM
   "Val loss platoda"

2. HÄ°POTEZ
   "LR Ã§ok bÃ¼yÃ¼k olabilir"

3. DENEY PLANI
   "LR'Ä± 0.01 â†’ 0.001 deÄŸiÅŸtir"
   "DiÄŸer her ÅŸey sabit"

4. Ã–LÃ‡ÃœM
   Val loss, train loss, sÃ¼re

5. KARAR
   Ä°yileÅŸti mi? Neden?

6. LOG
   SonuÃ§larÄ± yaz

7. Ä°TERE ET
   Sonraki hipotez
```

### ğŸ“Š Baseline Stratejisi

```
Level 0: Dummy
  - Mean/mode predictor
  - Random guess
  â†’ Baseline oluÅŸtur

Level 1: Simple
  - Linear/Logistic
  â†’ Beat et

Level 2: Standard
  - RF/XGBoost (tabular)
  - Standard CNN (vision)
  â†’ Beat et

Level 3: Custom
  - Domain-specific
  â†’ GerektiÄŸinde
```

### ğŸ“ Ablation Studies

```
Full model: A + B + C + D â†’ 0.85

Test:
  A + B + C     â†’ 0.82  (D'nin katkÄ±sÄ±: +0.03)
  A + B     + D â†’ 0.80  (C'nin katkÄ±sÄ±: +0.05)
  A     + C + D â†’ 0.78  (B'nin katkÄ±sÄ±: +0.07)
      B + C + D â†’ 0.60  (A'nÄ±n katkÄ±sÄ±: +0.25)

SonuÃ§: A en kritik, sonra B, C, D
```

### ğŸ² Hyperparameter Search

#### Random Search
```
LR: log-uniform(1e-5, 1e-1)
L2: log-uniform(1e-6, 1e-2)
batch: choice([32, 64, 128, 256])

N=20 rastgele kombinasyon dene

Avantaj: Grid'den daha verimli (kanÄ±tlanmÄ±ÅŸ)
```

#### Bayesian Optimization
```
1. Ä°lk denemeler (N=5)
2. Gaussian Process fit
3. Acquisition function â†’ Sonraki nokta
4. Tekrar

Avantaj: Az deneyle iyi sonuÃ§
```

---

## 1ï¸âƒ£4ï¸âƒ£ SÃ¶zlÃ¼k: Formal TanÄ±mlar

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TERÄ°M                   TANIM                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Model (f_Î¸)             x â†¦ Å· fonksiyonu         â”‚
â”‚ Parametre (Î¸)           Ã–ÄŸrenilen aÄŸÄ±rlÄ±klar     â”‚
â”‚ Hiperparametre          KullanÄ±cÄ± seÃ§imi (Î·, Î»)  â”‚
â”‚ Loss (L)                Hata fonksiyonu          â”‚
â”‚ Gradient (âˆ‡L)           EÄŸim vektÃ¶rÃ¼             â”‚
â”‚ Learning Rate (Î·)       AdÄ±m bÃ¼yÃ¼klÃ¼ÄŸÃ¼           â”‚
â”‚ Optimizer               GD algoritmasÄ±           â”‚
â”‚ Epoch                   TÃ¼m veriyi bir gÃ¶rme     â”‚
â”‚ Batch                   Mini grup (32-256)       â”‚
â”‚ Overfit                 Train â†“, Val â†‘           â”‚
â”‚ Underfit                Train â†‘, Val â†‘           â”‚
â”‚ Regularization          Penalty term (Î»||Î¸||Â²)   â”‚
â”‚ Early Stopping          Val bazlÄ± durdurma       â”‚
â”‚ Validation              Hiperparametre seÃ§imi    â”‚
â”‚ Test                    Final deÄŸerlendirme      â”‚
â”‚ i.i.d.                  BaÄŸÄ±msÄ±z, Ã¶zdeÅŸ daÄŸÄ±lÄ±m  â”‚
â”‚ Generalization          GÃ¶rmediÄŸine genelleme    â”‚
â”‚ MLE                     Max likelihood           â”‚
â”‚ MAP                     Max a posteriori         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 1ï¸âƒ£5ï¸âƒ£ Neden Lineer Regresyon?

### ğŸ¯ Pedagojik GerekÃ§eler

#### 1. Konveks Problem
```
L(w) = ||y - Xw||Â²

â†’ Tek global minimum
â†’ Local minimum yok
â†’ GD davranÄ±ÅŸÄ± TEMÄ°Z
```

#### 2. Analitik Ã‡Ã¶zÃ¼m Var
```
Normal Equations:
w* = (X^T X)^(-1) X^T y

â†’ GD ile karÅŸÄ±laÅŸtÄ±rabilirsin
â†’ DoÄŸruluÄŸu kontrol edebilirsin
```

#### 3. TÃ¼m Kavramlar Mevcut
```
âœ“ Loss (MSE)
âœ“ Gradient
âœ“ Optimization (GD)
âœ“ Regularization (L2)
âœ“ Overfitting
âœ“ Val/Test split
âœ“ Metrics (RÂ², RMSE)
âœ“ Feature scaling etkisi

â†’ Kamp eÄŸitimi gibi!
```

#### 4. GÃ¶rsel Anlama
```
2D:
  y
  â”‚  â—
  â”‚    â—  â—
  â”‚  â—   â”€â”€â”€â”€ Fit line
  â”‚ â—  â—
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ x

Herkes anlar!
```

#### 5. Transfer Edilebilir
```
Linear Regression'da Ã¶ÄŸrendiÄŸin:

Ã–lÃ§ekleme    â†’ MLP, CNN, Transformer
LR seÃ§imi    â†’ Her model
Overfit      â†’ Her model
Early stop   â†’ Her model
Val split    â†’ Her model

â†’ TEMEL BURADA ATILIR!
```

---

## 1ï¸âƒ£6ï¸âƒ£ Week 0 â†’ Week 1 KÃ¶prÃ¼sÃ¼

### ğŸ“š BugÃ¼n Ne Ã–ÄŸrendik?

**Kavramsal:**
```
âœ“ ML = Veriyle fonksiyon Ã¶ÄŸrenme
âœ“ Train/Val/Test neden ÅŸart
âœ“ Loss = Hata Ã¶lÃ§Ã¼sÃ¼
âœ“ GD = Gradyan takip ederek iniÅŸ
âœ“ Overfit = Ezber (regularization ile Ã¶nle)
âœ“ Ã–lÃ§ekleme = Optimizasyonu kolaylaÅŸtÄ±r
```

**Matematiksel:**
```
âœ“ Î¸* = argmin_Î¸ L(Î¸)
âœ“ Î¸ â† Î¸ - Î·âˆ‡L
âœ“ MSE â† Gaussian MLE
âœ“ CE â† Bernoulli MLE
âœ“ L2 â† Gaussian prior
```

**Pratik:**
```
âœ“ AdamW + kÃ¼Ã§Ã¼k L2 (baÅŸlangÄ±Ã§)
âœ“ Early stopping (overfit Ã¶nleme)
âœ“ Standardization (feature scaling)
âœ“ Val ile ayarla, Test'e dokunma
âœ“ Seed sabitle (repro)
```

### ğŸš€ Week 1'de Ne YapacaÄŸÄ±z?

```
1. Sentetik veri oluÅŸtur
   y = wx + b + Îµ

2. Manuel GD
   â†’ Gradient'i KENDÄ°N hesapla
   â†’ Autograd'Ä± Ã‡IPLAK gÃ¶r

3. nn.Module ile GD
   â†’ PyTorch'un gÃ¼cÃ¼nÃ¼ kullan
   â†’ Workflow'u Ã¶ÄŸren

4. Train/Val split
   â†’ Overfit'i CANLI izle

5. Early stopping
   â†’ Regularization etkisini GÃ–R

6. Metrikler
   â†’ RMSE, RÂ² hesapla, yorumla
```

### âœ… HazÄ±r mÄ±sÄ±n? (Self-Check)

```
â–¡ "Model nedir?" â†’ Parametrik fonksiyon
â–¡ "Loss nedir?" â†’ Hata Ã¶lÃ§Ã¼sÃ¼ (tek sayÄ±)
â–¡ "Gradient nedir?" â†’ EÄŸim vektÃ¶rÃ¼
â–¡ "GD nasÄ±l Ã§alÄ±ÅŸÄ±r?" â†’ Î¸ â† Î¸ - Î·âˆ‡L
â–¡ "Overfit nedir?" â†’ Train iyi, Val kÃ¶tÃ¼
â–¡ "Neden Ã¶lÃ§ekleme?" â†’ Loss yÃ¼zeyi yuvarlanÄ±r
â–¡ "MSE nereden gelir?" â†’ Gaussian MLE
â–¡ "L2 nereden gelir?" â†’ Gaussian prior
â–¡ "Early stopping nedir?" â†’ Val kÃ¶tÃ¼leÅŸince dur
â–¡ "Test ne zaman?" â†’ Sadece final'de

Hepsi âœ“ ise â†’ Week 1'e HAZIRSIN! ğŸ“
```

---

## 1ï¸âƒ£7ï¸âƒ£ Tek Paragraf Ã–zet

> **Makine Ã¶ÄŸrenmesi**, veriyle uyumlu bir **fonksiyonu bulma** sanatÄ±dÄ±r. Bunu, bir **kayÄ±p pusulasÄ±** (loss) yardÄ±mÄ±yla, parametreleri **gradyan adÄ±mlarÄ±yla** (GD) ayarlayarak yaparÄ±z. **DoÄŸrulama kÃ¼mesi** vicdanÄ±mÄ±zdÄ±r: ezberlediÄŸimiz anda bizi durdurur. **DÃ¼zenleme** (L2/L1), **Ã¶lÃ§ekleme**, **doÄŸru metrik** ve **dÃ¼rÃ¼st deney**; gÃ¼venilir sonuÃ§larÄ±n dÃ¶rt ayaÄŸÄ±dÄ±r. Bu temeller yerindeyse, Ã¼stÃ¼ne kuracaÄŸÄ±mÄ±z her modelâ€”lineer, MLP, hatta devasa Transformerâ€”**anlaÅŸÄ±lÄ±r** ve **kontrol edilebilir** olur.

---

## ğŸ“ Sonraki AdÄ±m

### ğŸ“– Okuma RotasÄ±

```
âœ… theory_intro.md (lise)
âœ… theory_core_concepts.md (Ã¼niversite) â† ÅU AN
â¬œ theory_foundations.md (sezgisel detay)
â¬œ theory_mathematical.md (matematiksel derinlik)
â¬œ theory_advanced.md (pratik saha)
â¬œ Setup & Week 1
```

### ğŸ’ª Pratik AlÄ±ÅŸtÄ±rma (30 dk)

**3 Problem Analiz Et:**

```
Problem 1: Regresyon
  - GÃ¶rev: Ev fiyatÄ±
  - Ã–zellikler: mÂ², oda, yaÅŸ
  - Loss: MSE (neden? â†’ Gaussian varsayÄ±mÄ±)
  - Metric: RMSE (birim anlamlÄ±)
  - Regularization: L2 (neden? â†’ Gaussian prior)
  - Split: Rastgele 70/15/15

Problem 2: Dengesiz SÄ±nÄ±flama
  - GÃ¶rev: DolandÄ±rÄ±cÄ±lÄ±k (%1)
  - Loss: BCE + class weight
  - Metric: PR-AUC, F1 (neden? â†’ imbalanced)
  - Regularization: Dropout
  - Split: Stratified (oran koru)

Problem 3: Zaman Serisi
  - GÃ¶rev: SatÄ±ÅŸ tahmini
  - Loss: MAE (neden? â†’ outlier robust)
  - Metric: MAPE
  - Leakage riski: Temporal!
  - Split: Zamansal (geÃ§miÅŸâ†’gelecek)
```

### ğŸ§® Mini Quiz (Self-Test)

```
Q1: MSE neden Gaussian MLE'ye eÅŸit?
A1: Gaussian noise varsayÄ±mÄ±nda log-likelihood
    maksimize etmek = MSE minimize etmek

Q2: L2 regularization ne anlama gelir?
A2: Parametrelere Gaussian prior koyduk
    â†’ KÃ¼Ã§Ã¼k aÄŸÄ±rlÄ±klarÄ± tercih ediyoruz

Q3: Early stopping neden regularization?
A3: EÄŸitimi erken durdurarak aÄŸÄ±rlÄ±klarÄ±n
    bÃ¼yÃ¼mesini engelliyoruz â†’ implicit L2

Q4: Adam neden SGD'den hÄ±zlÄ±?
A4: Her parametreye adaptif LR
    â†’ Dik yÃ¶nlerde hÄ±zlÄ±, yassÄ± yÃ¶nlerde yavaÅŸ
```

---

**ğŸ‰ Tebrikler! Week 0 Core Concepts tamamlandÄ±!**

**Fark:**
- theory_intro.md â†’ GÃ¼nlÃ¼k dil, sÄ±fÄ±r matematik
- theory_core_concepts.md â†’ Formal tanÄ±mlar, hafif matematik, probabilistik temeller

**Åimdi yapabilirsin:**
- Week 1 kodunu ANLAYARAK yaz
- "Neden MSE?" "Neden L2?" â†’ Cevapla
- FormÃ¼llerin nereden geldiÄŸini BÄ°L

**HazÄ±r ol, Week 1 pratik zamanÄ±!** ğŸš€
