# Week 0: Matematiksel Temeller - HocanÄ±n TahtasÄ±

**NovaDev v1.0 - Derin Matematiksel BakÄ±ÅŸ**

> "HÄ±z yok, kopyala-yapÄ±ÅŸtÄ±r yok. Temeli taÅŸ taÅŸ dÃ¶ÅŸeyelim."

---

## ğŸ¯ Bu DÃ¶kÃ¼man HakkÄ±nda

Bu notlar, makine Ã¶ÄŸrenmesinin **matematiksel ve probabilistik temellerini** aÃ§Ä±klar. 
AmaÃ§: "Neden bu formÃ¼l?", "Nereden geldi?" sorularÄ±na **net cevaplar**.

**Seviye:** Orta-Ä°leri (theory_foundations.md'yi okuduktan sonra)
**SÃ¼re:** 90-120 dakika
**Hedef:** "Neden?" sorusuna akÄ±cÄ± cevap verebilmek

---

## 0ï¸âƒ£ Ã‡erÃ§eve: ML GerÃ§ekte Ne Yapar?

### ğŸ¯ Temel Ä°ddia

**Bir fonksiyon arÄ±yoruz.**

```
Girdi (x) â†’ Fonksiyon â†’ Ã‡Ä±ktÄ± (Å· = f_Î¸(x))
```

Burada:
- **x**: Girdi (features)
- **f_Î¸**: Ã–ÄŸrenilen fonksiyon
- **Î¸** (theta): Ayarlanabilir **parametreler**
- **Å·**: Tahmin

### ğŸ“ Matematiksel FormÃ¼lasyon

```
Hedef: Î¸* = argmin_Î¸ E[(y - f_Î¸(x))Â²]
                      â†‘
                 Beklenti (tÃ¼m olasÄ± veri)
```

**Pratikte:** Elimizdeki Ã¶rneklerden yaklaÅŸÄ±k hesaplarÄ±z:

```
Î¸* â‰ˆ argmin_Î¸ (1/N) Î£ L(y_i, f_Î¸(x_i))
                        â†‘
                    Empirical loss
```

### ğŸ’¡ AmaÃ§: GerÃ§ek DÃ¼nya Ä°liÅŸkisini Taklit Etmek

```
GerÃ§ek dÃ¼nya:  y = g(x) + Îµ  (gÃ¼rÃ¼ltÃ¼lÃ¼)
                    â†‘      â†‘
                Bilinmeyen KaÃ§Ä±nÄ±lmaz
                 fonksiyon  gÃ¼rÃ¼ltÃ¼

Bizim model:  Å· = f_Î¸(x)
               â†‘
          YaklaÅŸtÄ±rma
```

### âœ… Bir CÃ¼mle Ã–zet

> **EÄŸitim = HatayÄ± Ã¶lÃ§en bir pusulaya (loss) bakarak parametreleri ayarlama iÅŸidir.**

**BileÅŸenler:**
- **Veri**: Ã–rnekler {(x_i, y_i)}
- **Loss**: HatayÄ± tek sayÄ±ya Ã§evirir L(Î¸)
- **Optimizasyon**: L(Î¸)'yÄ± azaltmak iÃ§in Î¸'yÄ± deÄŸiÅŸtir
- **Genelleme**: Ezber deÄŸil, **gÃ¶rmediÄŸin veride** de iÅŸe yarama

---

## 1ï¸âƒ£ Veri: Neden Kutsal? (Ve NasÄ±l Bozulur)

### ğŸ“Š i.i.d. VarsayÄ±mÄ± (Temelin Temeli)

**i.i.d.** = **i**ndependent and **i**dentically **d**istributed

**AnlamÄ±:** EÄŸitim ve test Ã¶rnekleri **aynÄ± daÄŸÄ±lÄ±mdan** gelmeli.

```
p_train(x, y) = p_test(x, y)
```

**Neden Ã–nemli?**
- EÄŸer daÄŸÄ±lÄ±mlar farklÄ±ysa â†’ "EÄŸitimde iyi, gerÃ§ekte kÃ¶tÃ¼"

### ğŸ”´ i.i.d. Ä°hlalleri (GerÃ§ek Hayatta SÄ±k)

#### 1. Covariate Shift (Kovaryat KaymasÄ±)
```
p(x) deÄŸiÅŸir ama p(y|x) sabit

Ã–rnek: Kamera aÃ§Ä±sÄ± deÄŸiÅŸir ama
        "kedi" tanÄ±mÄ± aynÄ± kalÄ±r
```

**Belirtiler:**
- Test verisi farklÄ± gÃ¶rÃ¼nÃ¼yor
- Ã–zellik daÄŸÄ±lÄ±mlarÄ± shift etmiÅŸ

**Ã‡Ã¶zÃ¼m:**
- Domain adaptation
- Feature standardization
- Data augmentation

#### 2. Concept Drift (Konsept KaymasÄ±)
```
p(y|x) deÄŸiÅŸir ama p(x) sabit

Ã–rnek: "Spam" tanÄ±mÄ± zamanla evriliyor
        ama email format'Ä± aynÄ±
```

**Belirtiler:**
- Model performansÄ± zamanla dÃ¼ÅŸÃ¼yor
- Etiket anlamÄ± deÄŸiÅŸmiÅŸ

**Ã‡Ã¶zÃ¼m:**
- Periodic retraining
- Online learning
- Temporal validation

#### 3. Prior Shift (Ã–nsel KaymasÄ±)
```
p(y) deÄŸiÅŸir (sÄ±nÄ±f oranlarÄ±)

Ã–rnek: Training'de %50 pozitif,
        Production'da %10 pozitif
```

**Belirtiler:**
- Class imbalance production'da farklÄ±
- Metrics yanÄ±ltÄ±cÄ±

**Ã‡Ã¶zÃ¼m:**
- Stratified sampling
- Class weighting
- Calibration

### ğŸ¯ Veri BÃ¶lme Stratejisi

#### Standard Split
```
TÃ¼m Veri (100%)
    â†“
â”œâ”€ Train (70%)      â†’ Parametreler Ã¶ÄŸren
â”œâ”€ Val (15%)        â†’ Hiperparametre ayarla
â””â”€ Test (15%)       â†’ Final deÄŸerlendirme
```

#### Time Series Split
```
[â”€â”€â”€â”€Trainâ”€â”€â”€â”€][â”€Valâ”€][Test]
     GeÃ§miÅŸ    YakÄ±n  Gelecek
                Gelecek
```

**Dikkat:** Zamansal sÄ±zÄ±ntÄ± olmasÄ±n! Future information geÃ§miÅŸe sÄ±zmamalÄ±.

### âš ï¸ Kural: Validation vs Test

```
âŒ YANLIÅ:
for hp in hyperparameters:
    score = evaluate_on_test(hp)  # Test'i kirletiyorsun!
    
âœ… DOÄRU:
for hp in hyperparameters:
    score = evaluate_on_val(hp)   # Val ile seÃ§
final_score = evaluate_on_test(best_hp)  # Test'e bir kez bak
```

### ğŸ” Data Leakage Ã–rnekleri (Saha Deneyimi)

#### Leakage Tipi 1: Temporal
```python
# âŒ YANLIÅ: GeleceÄŸi kullanÄ±yorsun
df['mean_price'] = df.groupby('user')['price'].transform('mean')
# Bu hesap tÃ¼m veriyi kullanÄ±r!

# âœ… DOÄRU: Sadece geÃ§miÅŸi kullan
df['mean_price'] = df.groupby('user')['price'].expanding().mean()
```

#### Leakage Tipi 2: Target Encoding
```python
# âŒ YANLIÅ: Hedef bilgisi Ã¶zelliÄŸe sÄ±zdÄ±
category_means = train['category'].map(
    train.groupby('category')['target'].mean()
)
# Test kategorilerinin target ortalamasÄ± train'den!

# âœ… DOÄRU: Cross-validation ile
from sklearn.model_selection import KFold
# Her fold iÃ§in target encoding yap
```

#### Leakage Tipi 3: Proxy Variables
```
Ã–zellikler: [birim_fiyat, adet, TOPLAM_FÄ°YAT]
Hedef: adet

â†’ TOPLAM_FÄ°YAT / birim_fiyat = adet (sÄ±zÄ±ntÄ±!)
```

**Antidot:**
- Feature correlation matrix kontrol et
- Domain knowledge kullan
- Leakage detection tools (SHAP values kontrol)

---

## 2ï¸âƒ£ Ã–zellikler: Ham Veri â†’ Ã–ÄŸrenilebilir Temsil

### ğŸšï¸ Ã–lÃ§ekleme: Neden Bu Kadar Kritik?

#### Matematiksel AÃ§Ä±klama

KayÄ±p yÃ¼zeyi **Ã¶zellik Ã¶lÃ§eklerine** baÄŸlÄ±:

```
L(w) = Î£ (y - Î£ w_j x_j)Â²

EÄŸer x_1 âˆˆ [0, 1] ve x_2 âˆˆ [0, 1000]:
  â†’ w_1'in etkisi kÃ¼Ã§Ã¼k gÃ¶rÃ¼nÃ¼r
  â†’ w_2'nin etkisi abartÄ±lÄ±r
  â†’ Loss yÃ¼zeyi Ã‡ARPIK (eliptik)
```

**Eliptik YÃ¼zey Problemi:**
```
        w_2
         â†‘
    â•±â•²  â”‚  â•±â•²     â† Ã‡ok dik
   â•±  â•² â”‚ â•±  â•²
  â•±    â•²â”‚â•±    â•²
 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ w_1
        â† Ã‡ok yassÄ±

GD bu yÃ¼zeyde ZÄ°KZAK yapar!
```

**Ã–lÃ§eklenmiÅŸ YÃ¼zey:**
```
        w_2
         â†‘
       â•±â”€â•²        â† Dairevi
      â”‚   â”‚
      â•²â”€â”€â”€â•±
 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ w_1

GD dÃ¼mdÃ¼z iner!
```

### ğŸ“ Ã–lÃ§ekleme YÃ¶ntemleri

#### Z-Score (Standardization)
```
x' = (x - Î¼) / Ïƒ

SonuÃ§: Î¼=0, Ïƒ=1
Avantaj: Outlier'larÄ± korur
KullanÄ±m: Ã‡oÄŸu durumda ilk tercih
```

#### Min-Max Scaling
```
x' = (x - x_min) / (x_max - x_min)

SonuÃ§: [0, 1] aralÄ±ÄŸÄ±
Avantaj: Bounded range
Dezavantaj: Outlier'a hassas
```

#### Robust Scaling
```
x' = (x - median) / IQR

IQR = Q3 - Q1 (interquartile range)
Avantaj: Outlier'a robust
KullanÄ±m: AykÄ±rÄ± deÄŸer Ã§ok varsa
```

### ğŸ¯ Kategorik DeÄŸiÅŸkenler

#### One-Hot Encoding
```
Renk: ['kÄ±rmÄ±zÄ±', 'mavi', 'yeÅŸil']
     â†“
[[1, 0, 0],  # kÄ±rmÄ±zÄ±
 [0, 1, 0],  # mavi
 [0, 0, 1]]  # yeÅŸil
```

**Avantaj:** SÄ±ra varsayÄ±mÄ± yok
**Dezavantaj:** High cardinality'de boyut patlamasÄ±

#### Target Encoding
```
Kategori â†’ Mean(target | kategori)

Ã–rnek:
'istanbul' â†’ 0.85  (bu kategoride ortalama target)
'ankara'   â†’ 0.62
```

**Avantaj:** Tek boyut
**âš ï¸ DÄ°KKAT:** Leakage riski! Cross-validation ile yap

#### Learnable Embeddings
```
Kategori â†’ DÃ¼ÅŸÃ¼k boyutlu vektÃ¶r (Ã¶ÄŸrenilir)

'istanbul' â†’ [0.5, -0.3, 0.8]
'ankara'   â†’ [0.2, 0.4, -0.1]
```

**Avantaj:** Model kendisi Ã¶ÄŸrenir
**KullanÄ±m:** Derin aÄŸlarda, NLP'de

### ğŸ“ Metin Ä°ÅŸleme

#### Klasik: Bag of Words
```
"makine Ã¶ÄŸrenmesi zor" â†’ [1, 1, 0, 1, ...]
                           m  Ã¶  a  z
```

**Dezavantaj:** SÄ±ra bilgisi kaybolur

#### Modern: Pre-trained Embeddings
```
"makine Ã¶ÄŸrenmesi" â†’ BERT/GPT embedding
                     [0.23, -0.45, ..., 0.78]  (768-dim)
```

**Avantaj:** Semantik iliÅŸkiler korunur

### ğŸ”´ Outlier YÃ¶netimi

#### MSE ve Outlier Ä°liÅŸkisi

```
MSE = (1/N) Î£ (y - Å·)Â²
                 â†‘
            Kare alÄ±yor!
```

**BÃ¼yÃ¼k hata â†’ Kare â†’ Ã‡ok bÃ¼yÃ¼k ceza**

**Ã–rnek:**
```
Hatalar: [1, 1, 1, 1, 10]
MSE = (1+1+1+1+100)/5 = 20.8  â† Tek outlier dominates!
MAE = (1+1+1+1+10)/5 = 2.8    â† Daha robust
```

#### Robust Loss: Huber
```
         { (1/2)xÂ²        if |x| â‰¤ Î´
L(x) = {
         { Î´(|x| - Î´/2)   if |x| > Î´

KÃ¼Ã§Ã¼k hata: MSE gibi (smooth)
BÃ¼yÃ¼k hata: MAE gibi (linear)
```

---

## 3ï¸âƒ£ KayÄ±p FonksiyonlarÄ±: Oyunun KuralÄ±

### ğŸ¯ Temel Prensip

> "Ne Ã¶lÃ§Ã¼yorsan ona dÃ¶nÃ¼ÅŸÃ¼rsÃ¼n."

Loss fonksiyonu **neyi optimize ettiÄŸini** tanÄ±mlar.

### ğŸ“ Regresyon Loss'larÄ±

#### Mean Squared Error (MSE)
```
L_MSE = (1/N) Î£ (y_i - Å·_i)Â²
```

**Ã–zellikleri:**
- **Kare**: BÃ¼yÃ¼k hatalarÄ± aÄŸÄ±r cezalandÄ±rÄ±r
- **Smooth**: TÃ¼revi her yerde var
- **Convex**: Tek minimum (linear model'de)

**Ne Zaman Kullan:**
- Normal daÄŸÄ±lÄ±mlÄ± hatalar
- Outlier az
- Standard regression

**Dezavantaj:**
- Outlier'a aÅŸÄ±rÄ± hassas

#### Mean Absolute Error (MAE)
```
L_MAE = (1/N) Î£ |y_i - Å·_i|
```

**Ã–zellikleri:**
- **Linear**: Outlier'a daha toleranslÄ±
- **Robust**: Median predict eder

**Ne Zaman Kullan:**
- Outlier Ã§ok
- Robust tahmin lazÄ±m

**Dezavantaj:**
- KÃ¶ÅŸeli (optimizasyon biraz hassas)
- SÄ±fÄ±rda tÃ¼rev tanÄ±msÄ±z

#### Huber Loss (Hibrit)
```
         { (1/2)(y-Å·)Â²      if |y-Å·| â‰¤ Î´
L_Huber = {
         { Î´|y-Å·| - Î´Â²/2    if |y-Å·| > Î´
```

**Pratikte En Ä°yi Denge:**
- KÃ¼Ã§Ã¼k hata: MSE (smooth)
- BÃ¼yÃ¼k hata: MAE (robust)

### ğŸ² SÄ±nÄ±flandÄ±rma Loss'larÄ±

#### Cross-Entropy (Log Loss)
```
L_CE = -Î£ y_i Ã— log(Å·_i)

Binary: -(y log(p) + (1-y) log(1-p))
```

**Sezgisel AÃ§Ä±klama:**
```
Model %10 der ama gerÃ§ek 1 â†’ BÃ¼yÃ¼k ceza
Model %90 der ve gerÃ§ek 1  â†’ KÃ¼Ã§Ã¼k ceza
Model %99 der ve gerÃ§ek 1  â†’ Ã‡ok kÃ¼Ã§Ã¼k ceza

-log(0.1) = 2.3  â† AÄŸÄ±r
-log(0.9) = 0.1  â† Hafif
-log(0.99) = 0.01 â† Ã‡ok hafif
```

**Ne Zaman Kullan:**
- SÄ±nÄ±flandÄ±rma (her zaman!)
- OlasÄ±lÄ±k tahminleri

#### Focal Loss
```
L_focal = -(1-p)^Î³ Ã— log(p)
             â†‘
        ModÃ¼lasyon faktÃ¶rÃ¼
```

**Ã–zellik:**
- Kolay Ã¶rnekler â†’ DÃ¼ÅŸÃ¼k aÄŸÄ±rlÄ±k
- Zor Ã¶rnekler â†’ YÃ¼ksek aÄŸÄ±rlÄ±k

**Ne Zaman Kullan:**
- Class imbalance
- Hard negative mining

### ğŸ“Š Metrik SeÃ§imi vs Loss SeÃ§imi

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Problem   â”‚     Loss     â”‚   Metric    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Regression  â”‚ MSE/Huber    â”‚ RMSE, MAE   â”‚
â”‚ Binary Cls  â”‚ BCE          â”‚ F1, AUC     â”‚
â”‚ Multi Cls   â”‚ CE           â”‚ Accuracy    â”‚
â”‚ Imbalanced  â”‚ Focal/Weightedâ”‚ PR-AUC, F1  â”‚
â”‚ Ranking     â”‚ Pairwise     â”‚ nDCG, MRR   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Ã–nemli:** Loss optimize edilir, Metric raporlanÄ±r.

---

## 4ï¸âƒ£ Optimizasyon: EÄŸim Neden Yeterli?

### ğŸ—» Loss YÃ¼zeyi Analojisi

```
KayÄ±p fonksiyonu = DaÄŸlÄ±k arazi

L(Î¸)
  â†‘
  â”‚     â•±â•²    â•±â•²
  â”‚   â•±    â•²â•±    â•²
  â”‚ â•±              â•²
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Î¸
  
  BulunduÄŸun nokta: Î¸_current
  Hedef: Vadiye in (minimum)
  Elindeki: Gradient (eÄŸim vektÃ¶rÃ¼)
```

### ğŸ“ Gradient Descent FormÃ¼lÃ¼

```
Î¸_new = Î¸_old - Î· Ã— âˆ‡L(Î¸_old)
         â†‘       â†‘      â†‘
       Mevcut   LR   Gradient
```

**Sezgi:**
- **Gradient**: En dik iniÅŸ yÃ¶nÃ¼
- **LR (Î·)**: AdÄ±m bÃ¼yÃ¼klÃ¼ÄŸÃ¼
- **Negatif**: YukarÄ± deÄŸil aÅŸaÄŸÄ± git

### ğŸšï¸ Learning Rate: Hayat Memat Meselesi

```
LR Ã§ok bÃ¼yÃ¼k (Î· = 1.0):
  Î¸ â”€â”€â”€â†’ â”¼ â†â”€â”€â”€ Î¸'
         â†‘
    Minimum'u aÅŸtÄ±!
    ZÄ±pla zÄ±pla diverge olur

LR uygun (Î· = 0.1):
  Î¸ â”€â”€â”€â†’ Â· â”€â”€â”€â†’ Â· â”€â”€â”€â†’ â—
         â†“   Smooth
        Minimum'a yaklaÅŸ

LR Ã§ok kÃ¼Ã§Ã¼k (Î· = 0.001):
  Î¸ â†’ Â· â†’ Â· â†’ Â· â†’ ...
      Ã‡ok yavaÅŸ, zaman kaybÄ±
```

### ğŸ”„ Mini-Batch Gradient Descent

```
Batch GD:      TÃ¼m veriyi gÃ¶r â†’ GÃ¼ncelle
               âœ“ Stabil
               âœ— YavaÅŸ

Stochastic GD: Her Ã¶rneÄŸi gÃ¶r â†’ GÃ¼ncelle
                âœ“ HÄ±zlÄ±
                âœ— GÃ¼rÃ¼ltÃ¼lÃ¼

Mini-batch GD: 32-256 Ã¶rnek â†’ GÃ¼ncelle
               âœ“ HÄ±z + stabilite dengesi
               âœ“ GPU parallelizmi
               â† PRAKTÄ°K STANDART
```

**GÃ¼rÃ¼ltÃ¼ AvantajÄ±:**
```
GÃ¼rÃ¼ltÃ¼lÃ¼ gradient bazen iyi!
  â†“
Saddle point'lerden kaÃ§abilir
Local minimum'lardan atlar
```

### âš¡ Momentum & Nesterov

#### Standard Momentum
```
v_t = Î² Ã— v_{t-1} + âˆ‡L(Î¸)
Î¸_t = Î¸_{t-1} - Î· Ã— v_t
      â†‘
  GeÃ§miÅŸin ortalamasÄ±
```

**Analoji:** Top vadiden aÅŸaÄŸÄ± yuvarlanÄ±yor
- Ä°vme birikir
- KÃ¼Ã§Ã¼k tepeleri aÅŸabilir
- Oscillation azalÄ±r

**Î² = 0.9:** %90 geÃ§miÅŸ + %10 ÅŸimdi

#### Nesterov Momentum (NAG)
```
"Ã–nce atlayacaÄŸÄ±n yeri tahmin et,
 sonra oradan gradient Ã¶lÃ§"

Î¸_lookahead = Î¸ - Î² Ã— v
v_t = Î² Ã— v_{t-1} + âˆ‡L(Î¸_lookahead)
Î¸_t = Î¸_{t-1} - Î· Ã— v_t
```

**Avantaj:** Daha proaktif, zÄ±plama azalÄ±r

### ğŸ§  Adam / AdamW

#### Adam (Adaptive Moment Estimation)
```
m_t = Î²1 Ã— m_{t-1} + (1-Î²1) Ã— âˆ‡L     [First moment]
v_t = Î²2 Ã— v_{t-1} + (1-Î²2) Ã— (âˆ‡L)Â²  [Second moment]

mÌ‚_t = m_t / (1 - Î²1^t)  [Bias correction]
vÌ‚_t = v_t / (1 - Î²2^t)

Î¸_t = Î¸_{t-1} - Î· Ã— mÌ‚_t / (âˆšvÌ‚_t + Îµ)
                          â†‘
                    Her parametreye
                    adaptif LR
```

**Sezgi:**
- **m**: Momentum (yÃ¶n)
- **v**: Variance (Ã¶lÃ§ek)
- Her parametre kendi LR'Ä±nÄ± alÄ±r

**Hiperparametreler:**
- Î²1 = 0.9 (momentum)
- Î²2 = 0.999 (variance)
- Îµ = 1e-8 (sayÄ±sal stabilite)

#### AdamW (Weight Decay dÃ¼zeltilmiÅŸ)
```
Adam'da L2 dÃ¼zgÃ¼n Ã§alÄ±ÅŸmÄ±yor!

âŒ Adam: Gradient'e L2 ekle
âœ… AdamW: Direkt aÄŸÄ±rlÄ±klarÄ± kÃ¼Ã§Ã¼lt

Î¸_t = Î¸_{t-1} - Î· Ã— (mÌ‚_t / âˆšvÌ‚_t + Î» Ã— Î¸_{t-1})
                                    â†‘
                              DoÄŸru L2
```

### ğŸ“ˆ Learning Rate Schedule

#### Cosine Decay
```
Î·_t = Î·_min + (Î·_max - Î·_min) Ã— (1 + cos(Ï€t/T)) / 2

  Î·
  â”‚â•²
  â”‚ â•²___
  â”‚     â•²___
  â”‚         â•²___
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ epoch
  
  BaÅŸta hÄ±zlÄ±, sonra yumuÅŸak
```

#### Step Decay
```
Î·_t = Î·_0 Ã— Î³^(floor(t/s))

  Î·
  â”‚â”€â”€â”€â”€â”
  â”‚    â””â”€â”€â”€â”€â”
  â”‚         â””â”€â”€â”€â”€â”
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ epoch
  
  Belli epoch'larda dÃ¼ÅŸÃ¼r
```

#### ReduceLROnPlateau
```
if val_loss not improving for N epochs:
    Î· = Î· / 10
```

#### Warmup
```
BaÅŸta kÃ¼Ã§Ã¼k LR â†’ YavaÅŸ yavaÅŸ arttÄ±r

Î·_t = Î·_target Ã— min(1, t / warmup_steps)

Neden? BÃ¼yÃ¼k model'de patlamayÄ± Ã¶nler
```

---

**(Devam theory_mathematical_part2.md'de...)**

---

## ğŸ“š Ara Ã–zet

Bu bÃ¶lÃ¼mde Ã¶ÄŸrendiklerimiz:

âœ… **Matematiksel Ã§erÃ§eve:** ML = fonksiyon arama
âœ… **Veri disiplini:** i.i.d., leakage, bÃ¶lme stratejisi
âœ… **Feature engineering:** Ã–lÃ§ekleme matematiÄŸi
âœ… **Loss fonksiyonlarÄ±:** MSE/MAE/Huber/CE seÃ§imi
âœ… **Optimizasyon:** GD, momentum, Adam derinliÄŸi

**Sonraki bÃ¶lÃ¼m (part2):**
- SayÄ±sal koÅŸullar & curvature
- Probabilistik bakÄ±ÅŸ (MLE, MAP)
- Bias-variance matematik
- Regularization teorisi
- DeÄŸerlendirme metrikleri

---

**Durum:** ğŸ“– Part 1 tamamlandÄ± â†’ Part 2'ye geÃ§
