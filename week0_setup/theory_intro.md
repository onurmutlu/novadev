# Week 0: Makine Ã–ÄŸrenmesine GiriÅŸ - SÄ±fÄ±rdan BaÅŸlangÄ±Ã§

**NovaDev v1.0 - Lise Seviyesi AnlatÄ±m**

> "Kod yok, formÃ¼l minimum, sabÄ±rlÄ± hoca modunda. BittiÄŸinde 'ne yapÄ±yoruz, neden, ne zaman neye bakÄ±yoruz' net olsun."

---

## ğŸ¯ Bu DÃ¶kÃ¼man Kimler Ä°Ã§in?

- âœ… HiÃ§ ML deneyimi yok
- âœ… "Makine Ã¶ÄŸrenmesi" kelimesi soyut geliyor
- âœ… Ã–nce **sezgiyle** anlamak istiyorum
- âœ… MatematiÄŸi sonra Ã¶ÄŸrenirim, Ã¶nce mantÄ±ÄŸÄ± kavramak istiyorum

**Seviye:** Tam BaÅŸlangÄ±Ã§ (Lise)
**SÃ¼re:** 45-60 dakika
**Format:** GÃ¼nlÃ¼k dil, bol benzetme, mini quiz
**Hedef:** "Ah, ha! Demek bu kadar basit!" demek

---

## 0ï¸âƒ£ Makine Ã–ÄŸrenmesi Nedir? (Tek CÃ¼mle)

### ğŸ¯ TanÄ±m

**Makine Ã¶ÄŸrenmesi = Ã–rneklerden (veri) bir kural (fonksiyon) Ã¶ÄŸrenip, yeni durumlarÄ± tahmin etme iÅŸi.**

### ğŸŒŸ GÃ¼nlÃ¼k Hayattan Ã–rnekler

**1. Telefonun Klavyesi**
```
Sen: "Merhaba, nas..."
Klavye: "â†’ nasÄ±lsÄ±n?" (otomatik tamamlama)

NasÄ±l yapÄ±yor?
â†’ Milyonlarca mesajdan "merhaba"dan sonra
  "nasÄ±lsÄ±n" gelme PATERNINI Ã¶ÄŸrenmiÅŸ
```

**2. Netflix Ã–nerileri**
```
Ä°zlediklerin: Aksiyon filmleri, bilim kurgu
Netflix: "Åunu da beÄŸenebilirsin: Inception"

NasÄ±l yapÄ±yor?
â†’ Senin gibi izleyenlerin sonra ne izlediÄŸine bakÄ±yor
  (benzer profil â†’ benzer tercih)
```

**3. Spam Filtresi**
```
Email: "TÄ±kla kazaaaan!!! 1000000$$$"
Gmail: ğŸš« SPAM

NasÄ±l yapÄ±yor?
â†’ Milyonlarca spam Ã¶rneÄŸinden ORTAK Ã¶zellikleri Ã¶ÄŸrenmiÅŸ:
  - Ã‡ok Ã¼nlem (!!!)
  - Para sembolÃ¼ ($$$)
  - AÅŸÄ±rÄ± bÃ¼yÃ¼k rakamlar
```

### ğŸ§© Temel BileÅŸenler

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Girdi (x)                             â”‚
â”‚  â†’ Modele verdiÄŸin bilgi               â”‚
â”‚    Ã–rnek: Ev Ã¶zellikleri (mÂ², oda)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Model (f_Î¸)                           â”‚
â”‚  â†’ Kutu iÃ§indeki fonksiyon             â”‚
â”‚  â†’ Ä°Ã§inde "ayar dÃ¼ÄŸmeleri" var (Î¸)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Ã‡Ä±ktÄ± (Å·)                             â”‚
â”‚  â†’ Modelin tahmini                     â”‚
â”‚    Ã–rnek: Ev fiyatÄ± (500K TL)          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Parametre (Î¸):** Modelin iÃ§ ayarlarÄ± (Ã¶ÄŸrenilen sayÄ±lar)
**AmaÃ§:** Bu ayarlarÄ± Ã¶yle ayarla ki tahminler **doÄŸru** ve **tutarlÄ±** olsun

### ğŸ’¡ AltÄ±n Prensip

> **"Benzer ÅŸeyler benzer sonuÃ§lar verir"**
> 
> Bu prensibi Ã¶ÄŸrenilebilir bir kurala dÃ¶nÃ¼ÅŸtÃ¼rÃ¼yoruz!

---

## 1ï¸âƒ£ Veri: Neden Kutsal?

### ğŸ“Š Veri Nedir?

**Tablo olarak dÃ¼ÅŸÃ¼n:**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Ev No  â”‚  mÂ²  â”‚ Oda â”‚ YaÅŸ  â”‚  Fiyat(TL) â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚    1    â”‚ 100  â”‚  3  â”‚  5   â”‚   400K     â”‚ â† Ã–rnek 1
â”‚    2    â”‚  75  â”‚  2  â”‚ 10   â”‚   300K     â”‚ â† Ã–rnek 2
â”‚    3    â”‚ 150  â”‚  4  â”‚  2   â”‚   700K     â”‚ â† Ã–rnek 3
â”‚   ...   â”‚ ...  â”‚ ... â”‚ ...  â”‚   ...      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
          â†‘      â†‘     â†‘        â†‘
       Ã–zellik Ã–zellik Ã–zellik  Etiket
       (Feature)                (Label)
```

**Terminoloji:**
- **SatÄ±r = Ã–rnek** (bir ev, bir Ã¶ÄŸrenci, bir fotoÄŸraf)
- **SÃ¼tun = Ã–zellik** (metrekare, yaÅŸ, piksel deÄŸeri)
- **Hedef = Etiket** (fiyat, sÄ±nav notu, "kedi"/"kÃ¶pek")

### ğŸ“ Ã–ÄŸrenme TÃ¼rleri

#### 1. GÃ¶zetimli (Supervised)
```
Veri: (Girdi, DoÄŸru Cevap) Ã§iftleri

Ã–rnek: (Ev Ã¶zellikleri, Fiyat)
       (FotoÄŸraf, "Kedi"/"KÃ¶pek")
       
GÃ¶rev: Yeni girdide doÄŸru cevabÄ± tahmin et
```

#### 2. GÃ¶zetimsiz (Unsupervised)
```
Veri: Sadece girdiler (cevap yok)

Ã–rnek: MÃ¼ÅŸteri alÄ±ÅŸveriÅŸleri
       
GÃ¶rev: Benzer gruplarÄ± bul (kÃ¼meleme)
       Anormallikleri tespit et
```

#### 3. PekiÅŸtirmeli (Reinforcement)
```
Durum: Oyun tahtasÄ± / robot sensÃ¶rleri
Aksiyon: Hamle yap
Ã–dÃ¼l: KazandÄ±n mÄ±? Kaybettin mi?

GÃ¶rev: En Ã§ok Ã¶dÃ¼lÃ¼ topla (dene yanÄ±l)
```

**Bu programda:** Ã‡oÄŸunlukla **gÃ¶zetimli Ã¶ÄŸrenme** (supervised)

### ğŸ”„ Neden Train/Val/Test BÃ¶l?

#### ğŸ“– Ã–ÄŸrenci Benzetmesi

```
Bir Ã¶ÄŸrenciyi dÃ¼ÅŸÃ¼n:

âŒ YANLIÅ YOL:
  Ã‡alÄ±ÅŸma: 10 soru â†’ Ezberle
  SÄ±nav: AynÄ± 10 soru
  SonuÃ§: "100 aldÄ±m!" (ama gerÃ§ekte hiÃ§bir ÅŸey bilmiyor)

âœ… DOÄRU YOL:
  Ã‡alÄ±ÅŸma: 100 soru (TRAIN)
  Ara sÄ±nav: 20 yeni soru (VALIDATION) â†’ Ã‡alÄ±ÅŸma stratejisini ayarla
  Final: 30 tamamen yeni soru (TEST) â†’ GerÃ§ek performans
```

#### ğŸ“¦ Veri BÃ¶lme

```
TÃ¼m Veri (1000 Ã¶rnek)
         â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚            â”‚
  â”Œâ”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
  â”‚   TRAIN    â”‚ â”‚  70% (700 Ã¶rnek)
  â”‚ "Ã‡alÄ±ÅŸma"  â”‚ â”‚  â†’ Model burada Ã¶ÄŸrenir
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
    â”‚   VAL    â”‚ â”‚  15% (150 Ã¶rnek)
    â”‚"Ara sÄ±nav"â”‚ â”‚  â†’ AyarlarÄ± seÃ§eriz (LR, L2, vs.)
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
      â”‚  TEST  â”‚â”€â”˜  15% (150 Ã¶rnek)
      â”‚"Final" â”‚    â†’ Tek seferlik gerÃ§ek performans
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### âš ï¸ AltÄ±n Kural

```
âŒ Test'e bakÄ±p ayar yapma!

Neden?
â†’ Test'i "ezberlersin"
â†’ GerÃ§ek dÃ¼nya performansÄ± bilinmez kalÄ±r

âœ… Val'e bakÄ±p ayarla, Test'e sadece BÄ°R KEZ bak
```

### ğŸŒŠ DaÄŸÄ±lÄ±m KaymasÄ± (Distribution Shift)

#### Ä°deal DÃ¼nya
```
Train verisi: Ä°stanbul'da Ã§ekilmiÅŸ fotoÄŸraflar
Test verisi:  Ä°stanbul'da Ã§ekilmiÅŸ fotoÄŸraflar
               â†‘
         AynÄ± koÅŸullar â†’ Model iyi Ã§alÄ±ÅŸÄ±r âœ“
```

#### GerÃ§ek DÃ¼nya
```
Train: Ä°stanbul (gÃ¼neÅŸli, gÃ¼ndÃ¼z)
Test:  Ankara (karlÄ±, akÅŸam)
        â†‘
    FarklÄ± koÅŸullar â†’ Model ÅŸaÅŸÄ±rÄ±r âœ—
```

**Ã–rnekler:**
- **Kovaryat kaymasÄ±:** Kamera modeli deÄŸiÅŸti
- **Konsept kaymasÄ±:** "Spam" tanÄ±mÄ± zamanla evrildi
- **SÄ±nÄ±f oranÄ±:** Training'de %50 spam, gerÃ§ekte %5 spam

### ğŸ• Zaman Serisi Ã–zel Durum

```
âŒ YANLIÅ:
  Train: Rastgele gÃ¼nler
  Test:  Rastgele gÃ¼nler
  
  â†’ GeleceÄŸi bilip geÃ§miÅŸi tahmin edebilir (HILE!)

âœ… DOÄRU:
  Train: [â”€â”€â”€â”€â”€â”€â”€GeÃ§miÅŸâ”€â”€â”€â”€â”€â”€â”€]
  Val:               [â”€YakÄ±n gelecekâ”€]
  Test:                          [â”€Gelecekâ”€]
  
  â†’ Zaman sÄ±rasÄ±nÄ± MUTLAKA koru
```

---

## 2ï¸âƒ£ Model: Kutudaki Fonksiyon

### ğŸ›ï¸ Model = AyarlÄ± Radyo

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    ğŸ“» RADYO                 â”‚
â”‚                              â”‚
â”‚  Frekans dÃ¼ÄŸmesi: [âš«â”€â”€â”€â”€]  â”‚ â† Parametre 1
â”‚  Ses dÃ¼ÄŸmesi:     [â”€â”€â”€âš«â”€]  â”‚ â† Parametre 2
â”‚  Bas dÃ¼ÄŸmesi:     [â”€â”€âš«â”€â”€]  â”‚ â† Parametre 3
â”‚                              â”‚
â”‚  Ã‡Ä±kan ses: ğŸ“¢ "...kÄ±rrr..." â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Hedef: DÃ¼ÄŸmeleri Ã¶yle ayarla ki yayÄ±n NETLEÅSÄ°N

Model = Radyo
Parametreler = DÃ¼ÄŸmeler (Î¸)
EÄŸitim = DÃ¼ÄŸmeleri ayarlama sÃ¼reci
```

### ğŸ§® Matematiksel (Basit)

```
Å· = f_Î¸(x)

Å·: Tahmin (radyodan Ã§Ä±kan ses)
x: Girdi (radyo dalgasÄ±)
Î¸: Parametreler (dÃ¼ÄŸmelerin konumu)
f: Fonksiyon (radyo devresi)
```

### ğŸšï¸ Parametre vs Hiperparametre

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  PARAMETRE (Î¸)                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â†’ Model Ã–ÄRENÄ°R                      â”‚
â”‚  â†’ Veriyle ayarlanÄ±r                  â”‚
â”‚  â†’ Ã–rnek: AÄŸÄ±rlÄ±klar, bias'lar       â”‚
â”‚  â†’ SayÄ±sÄ±: Binler, milyonlar         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  HÄ°PERPARAMETRE                       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â†’ SEN seÃ§ersin                       â”‚
â”‚  â†’ EÄŸitim baÅŸlamadan Ã¶nce kararlaÅŸtÄ±râ”‚
â”‚  â†’ Ã–rnek: Ã–ÄŸrenme hÄ±zÄ±, katman sayÄ±sÄ±â”‚
â”‚  â†’ SayÄ±sÄ±: 5-20 arasÄ±                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Benzetme:**
- **Parametre:** Ã–ÄŸrencinin ezberledikleri
- **Hiperparametre:** Ã‡alÄ±ÅŸma stratejisi (kaÃ§ saat, hangi kitap)

### ğŸ“ˆ Model Kapasitesi

```
Kapasite = Modelin esnekliÄŸi

Ã‡ok DÃ¼ÅŸÃ¼k:              Ã‡ok YÃ¼ksek:
  Basit Ã§izgi             KarmaÅŸÄ±k eÄŸri
  
     â—                       â—
   â—   â—                   â—   â—
  â—â”€â”€â”€â”€â”€â—â—               â— â•±â•² â•±â•²â—
 â—       â—              â—â•±  â•²â•±  â•²â—
         â—                       â—
         
  UNDERFIT               OVERFIT
  (Yetersiz)             (Ezber)
```

**Ä°pucu:** KÃ¼Ã§Ã¼kten baÅŸla â†’ Gerekirse bÃ¼yÃ¼t

---

## 3ï¸âƒ£ Hata NasÄ±l Ã–lÃ§Ã¼lÃ¼r? (Loss = KayÄ±p)

### ğŸ¯ Neden Tek SayÄ±?

```
Model 1: BazÄ± tahminler iyi, bazÄ± kÃ¶tÃ¼
Model 2: BazÄ± tahminler iyi, bazÄ± kÃ¶tÃ¼

Hangisi daha iyi? ğŸ¤”

â†’ Tek bir sayÄ±ya Ã§evir â†’ KarÅŸÄ±laÅŸtÄ±r! âœ“
```

**Loss (KayÄ±p) = Hedefe uzaklÄ±k (tek sayÄ±)**

### ğŸ“ Regresyon Loss'larÄ± (SayÄ± Tahmini)

#### MSE (Mean Squared Error)
```
GerÃ§ek: 100
Tahmin: 90
Hata: 10

MSE = (10)Â² = 100  â† Kare alÄ±yor!

GerÃ§ek: 100
Tahmin: 50
Hata: 50

MSE = (50)Â² = 2500  â† BÃ¼yÃ¼k hata â†’ Ã‡OK AÄIR CEZA
```

**Ã–zellik:** BÃ¼yÃ¼k hatalarÄ± **sert** cezalandÄ±rÄ±r

**Ne Zaman Kullan:**
- Standard regresyon
- AykÄ±rÄ± deÄŸer az
- "BÃ¼yÃ¼k hatalar Ã§ok kÃ¶tÃ¼" diyorsan

#### MAE (Mean Absolute Error)
```
GerÃ§ek: 100
Tahmin: 90
Hata: 10

MAE = |10| = 10

GerÃ§ek: 100
Tahmin: 50
Hata: 50

MAE = |50| = 50  â† Linear ceza (adil)
```

**Ã–zellik:** AykÄ±rÄ± deÄŸerlere **toleranslÄ±**

**Ne Zaman Kullan:**
- AykÄ±rÄ± deÄŸer Ã§ok
- "Her hata eÅŸit Ã¶nemde"

### ğŸ² Classification Loss (SÄ±nÄ±flama)

#### Cross-Entropy (Log Loss)
```
GerÃ§ek: "Kedi" (1)
Model gÃ¼veni: %10 Kedi
â†’ AÄIR CEZA! (YanlÄ±ÅŸ ve Ã§ok emin!)

GerÃ§ek: "Kedi" (1)
Model gÃ¼veni: %90 Kedi
â†’ Hafif ceza (DoÄŸru ve emin)

GerÃ§ek: "Kedi" (1)
Model gÃ¼veni: %99 Kedi
â†’ Ã‡ok hafif ceza (MÃ¼kemmel!)
```

**Analoji - SÄ±nav:**
```
Soru: "2+2 = ?"
Senin cevabÄ±n: "5" + "Kesinlikle eminim!"
Hoca: ğŸ˜¡ AÄŸÄ±r ceza! (YanlÄ±ÅŸ + kendinden emin)

Senin cevabÄ±n: "5" + "Emin deÄŸilim..."
Hoca: ğŸ˜ Orta ceza (YanlÄ±ÅŸ ama ÅŸÃ¼phelisin)
```

### âš–ï¸ Metric vs Loss

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  LOSS                               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â†’ EÄŸitim sÄ±rasÄ±nda OPTÄ°MÄ°ZE edilir â”‚
â”‚  â†’ Matematiksel olarak tÃ¼revlenebilirâ”‚
â”‚  â†’ Ã–rnek: MSE, Cross-Entropy        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  METRIC                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  â†’ Ä°nsan iÃ§in anlamlÄ±               â”‚
â”‚  â†’ Raporlamada kullanÄ±lÄ±r           â”‚
â”‚  â†’ Ã–rnek: Accuracy, F1, RMSE        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 4ï¸âƒ£ KayÄ±p NasÄ±l AzaltÄ±lÄ±r? (Gradient Descent)

### ğŸ—» DaÄŸ Analojisi

```
    â•±â•²    â•±â•²
   â•±  â•²  â•±  â•²
  â•±    â•²â•±    â•²
 â•±      *     â•²  â† Sen buradasÄ±n (sisli daÄŸ)
â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•²

Hedef: AÅŸaÄŸÄ± in (minimum)
Elinde: EÄŸim Ã¶lÃ§er (gradient)
         "Bu yÃ¶nde eÄŸim aÅŸaÄŸÄ± iniyor"
```

### ğŸ“ Gradient Descent (Gradyan Ä°niÅŸi)

```
1. BulunduÄŸun noktada EÄIMI Ã¶lÃ§
   â†“
2. EÄŸimin gÃ¶sterdiÄŸi yÃ¶nde ADIM at
   â†“
3. Yeni noktada tekrar EÄIMI Ã¶lÃ§
   â†“
4. Tekrarla (kayÄ±p artÄ±k deÄŸiÅŸmiyorsa â†’ DUR)
```

**FormÃ¼l (Basit):**
```
Yeni_Parametre = Eski_Parametre - (Ã–ÄŸrenme_HÄ±zÄ± Ã— EÄŸim)
                                      â†‘              â†‘
                                   Ne kadar          Hangi
                                   atlayacak?        yÃ¶ne?
```

### ğŸšï¸ Learning Rate (Ã–ÄŸrenme HÄ±zÄ±)

#### Ã‡ok BÃ¼yÃ¼k LR (0.9)
```
    â•±â•²
   â•±  â•²
  â—â”€â”€â”€â”€â”¼â”€â”€â”€â”€â—  â† ZIK ZAK! Minimum'u aÅŸÄ±yor
      â•²â•±
       â†‘
   Hedef (ama geÃ§iyor)
```

**SonuÃ§:** Patlar, diverge olur, NaN

#### Uygun LR (0.1)
```
    â•±â•²
   â•±  â•²
  â—â”€â”€â†’â—â”€â†’â—â”€â†’â—  â† Smooth iniÅŸ
         â•²â•±
          â†‘
        Minimum'a ulaÅŸtÄ± âœ“
```

**SonuÃ§:** BaÅŸarÄ±lÄ±!

#### Ã‡ok KÃ¼Ã§Ã¼k LR (0.001)
```
    â•±â•²
   â•±  â•²
  â—â†’â—â†’â—â†’â—â†’â—â†’...  â† Ã‡ok yavaÅŸ
         â•²â•±
```

**SonuÃ§:** Saatler sÃ¼rer, sÄ±kÄ±cÄ±

### ğŸ’ Mini-Batch Gradient Descent

```
Batch GD:
  â†’ TÃ¼m veriyi gÃ¶r â†’ Hesapla â†’ GÃ¼ncelle
  â†’ Stabil ama YAVAÅ (1 milyon Ã¶rnek!)

Stochastic GD:
  â†’ Her Ã¶rneÄŸi gÃ¶r â†’ GÃ¼ncelle
  â†’ HÄ±zlÄ± ama GÃœRÃœLTÃœLÃœ (zikzak)

Mini-batch GD:  â­ PRAKTÄ°K!
  â†’ 32-256 Ã¶rnek â†’ Hesapla â†’ GÃ¼ncelle
  â†’ HÄ±z + Stabilite dengesi
  â†’ GPU parallelizasyonu
```

**GÃ¼rÃ¼ltÃ¼ = Bazen Ä°yi:**
```
GÃ¼rÃ¼ltÃ¼lÃ¼ adÄ±mlar:
  â†’ Dar Ã§ukurlardan kaÃ§ar
  â†’ Daha geniÅŸ Ã§ukura oturur
  â†’ Daha iyi genelleme!
```

### ğŸš€ GeliÅŸmiÅŸ OptimizatÃ¶rler

#### Momentum
```
Top vadiden yuvarlanÄ±yor ğŸ€

  â•±â•²   â•±â•²
 â•±  â•² â•±  â•²
â•±    â—    â•²  â† KÃ¼Ã§Ã¼k tepeyi aÅŸar (momentum)
     â†“
    â•±â•²
   â•±  â—  â† Daha derin Ã§ukura ulaÅŸtÄ±!
   â•²â”€â”€â•±
```

**MantÄ±k:** GeÃ§miÅŸ adÄ±mlarÄ± da hesaba kat

#### Adam/AdamW â­
```
Her parametre iÃ§in AYRI hÄ±z ayarÄ±:

Parametre 1: HÄ±zlÄ± deÄŸiÅŸsin â†’ YÃ¼ksek LR
Parametre 2: YavaÅŸ deÄŸiÅŸsin â†’ DÃ¼ÅŸÃ¼k LR

â†’ Adaptif (akÄ±llÄ±)
â†’ Pratikte en Ã§ok kullanÄ±lÄ±r
```

**Pratik ReÃ§ete:**
```
BaÅŸlangÄ±Ã§: AdamW + kÃ¼Ã§Ã¼k L2
          â†‘
    %90 durumda iÅŸe yarar!
```

---

## 5ï¸âƒ£ Tensors: YapÄ± TaÅŸÄ±

### ğŸ“¦ Tensor = GeliÅŸmiÅŸ SayÄ± Tablosu

```
Skalar (0D):     5
VektÃ¶r (1D):    [1, 2, 3]
Matrix (2D):    [[1, 2],
                 [3, 4]]
Tensor (3D+):   [[[1,2],[3,4]],
                 [[5,6],[7,8]]]
```

### ğŸ·ï¸ Tensor Ã–zellikleri

```python
x = torch.randn(64, 3, 224, 224)
                 â†‘   â†‘   â†‘    â†‘
             Batch  RGB  Height Width

Shape:  (64, 3, 224, 224)
Dtype:  torch.float32
Device: mps (Apple GPU)
```

**Shape = En Ã–nemli:**
```
âŒ En sÄ±k hata: Shape uyuÅŸmazlÄ±ÄŸÄ±!

"Expected (10, 5) but got (10, 3)"
         â†‘              â†‘
      Beklenen      Gelen
      
Ã‡Ã¶zÃ¼m: Her adÄ±mda print(x.shape)
```

### ğŸ“¡ Broadcasting (Otomatik GeniÅŸleme)

```
(10, 1) + (1, 5) â†’ (10, 5)

GÃ¶rsel:
  [10]     [1 2 3 4 5]
  [10]  +  [1 2 3 4 5]
  [10]     [1 2 3 4 5]
  ...

  =
  
  [11 12 13 14 15]
  [11 12 13 14 15]
  [11 12 13 14 15]
  ...
```

**Kural (Basit):**
- Boyutlar eÅŸit VEYA biri 1 olmalÄ±
- Otomatik kopyalar

---

## 6ï¸âƒ£ Autograd: Otomatik TÃ¼rev

### ğŸ“ Fatura Analojisi

```
Restoran:
  Ana yemek: 50 TL
  Ä°Ã§ecek:    20 TL
  TatlÄ±:     30 TL
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  KDV (%18): ???  â† Her kalem iÃ§in hesapla
  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
  Toplam:    ???
```

**Autograd = Otomatik muhasebe:**
```
Forward (Ä°leri):
  x â†’ iÅŸlem1 â†’ iÅŸlem2 â†’ iÅŸlem3 â†’ Loss
  
  (FiÅŸleri keserken her iÅŸlem KAYDEDÄ°LÄ°YOR)

Backward (Geri):
  Loss â† "Senin payÄ±n ne?" â† iÅŸlem3
       â† "Senin payÄ±n ne?" â† iÅŸlem2
       â† "Senin payÄ±n ne?" â† iÅŸlem1
  
  (FiÅŸleri tersten topla, herkesin borcunu hesapla)
```

### ğŸ”§ Temel Komutlar

```python
# 1. "Bu parametrenin tÃ¼revini tut"
x = torch.tensor([1.0], requires_grad=True)

# 2. Ä°leri hesap (fiÅŸ kes)
y = x ** 2

# 3. Geri hesap (borÃ§ Ã¶de)
y.backward()

# 4. Gradient'i oku
print(x.grad)  # dy/dx = 2x = 2

# 5. SÄ±fÄ±rla (yeni tur)
x.grad.zero_()
```

### âš ï¸ Ã–nemli: Gradient Birikimi

```python
# âŒ YANLIÅ
for epoch in range(10):
    loss = compute_loss()
    loss.backward()  # Gradient BÄ°RÄ°KÄ°YOR!
    optimizer.step()

# âœ… DOÄRU
for epoch in range(10):
    optimizer.zero_grad()  # Ã–nce temizle!
    loss = compute_loss()
    loss.backward()
    optimizer.step()
```

**Neden?**
```
backward() â†’ ADD yapar (ekler)
           â†’ SET yapmaz (ayarlamaz)
           
Temizlemezsen â†’ SÃ¼rekli bÃ¼yÃ¼r â†’ Patlama!
```

---

## 7ï¸âƒ£ Overfit/Underfit: AltÄ±n Denge

### ğŸ“Š ÃœÃ§ Senaryo

#### 1. Underfit (Yetersiz Ã–ÄŸrenme)
```
Train Loss: YÃ¼ksek ğŸ“ˆ
Val Loss:   YÃ¼ksek ğŸ“ˆ

Neden?
  â†’ Model Ã§ok basit
  â†’ Yeterince Ã¶ÄŸrenmemiÅŸ
  
Ã‡Ã¶zÃ¼m:
  âœ“ Daha karmaÅŸÄ±k model
  âœ“ Daha uzun eÄŸitim
  âœ“ Daha iyi Ã¶zellikler
```

#### 2. Overfit (Ezber)
```
Train Loss: Ã‡ok dÃ¼ÅŸÃ¼k ğŸ“‰
Val Loss:   YÃ¼ksek ğŸ“ˆ

Neden?
  â†’ Model ezberledi
  â†’ Yeni veriyi genelleyemiyor
  
Ã‡Ã¶zÃ¼m:
  âœ“ Regularization (L2, dropout)
  âœ“ Early stopping
  âœ“ Daha fazla veri
  âœ“ Data augmentation
```

#### 3. Good Fit (Ä°DEAL) â­
```
Train Loss: DÃ¼ÅŸÃ¼k ğŸ“‰
Val Loss:   DÃ¼ÅŸÃ¼k ğŸ“‰

Durum: MÃ¼kemmel denge! âœ“
```

### ğŸ“ˆ Loss EÄŸrileri

```
Loss
  â”‚
  â”‚ Train â”€â”€â”€â”€â”€â”€â•²___________
  â”‚              â•²
  â”‚               â•²  
  â”‚ Val   â”€â”€â”€â”€â”€â”€â”€â”€â”€â•²____â•±â”€â”€â”€  â† Overfit baÅŸladÄ±!
  â”‚                 â†‘
  â”‚            Bu noktada DUR
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Epoch
                    â†‘
              Early stopping
```

### ğŸ›¡ï¸ Regularization Cephanesi

#### 1. L2 (Weight Decay)
```
"AÄŸÄ±rlÄ±klarÄ± KÃœÃ‡ÃœK tut"

BÃ¼yÃ¼k aÄŸÄ±rlÄ±k = AÅŸÄ±rÄ± hassas model = Overfit
KÃ¼Ã§Ã¼k aÄŸÄ±rlÄ±k = Sade model = Genelleme âœ“
```

#### 2. L1 (Lasso)
```
"BazÄ± aÄŸÄ±rlÄ±klarÄ± TAM SIFIR yap"

â†’ Ã–zellik seÃ§imi (gereksizleri atar)
```

#### 3. Dropout (Derin AÄŸlarda)
```
EÄŸitim sÄ±rasÄ±nda rastgele nÃ¶ronlarÄ± "Ã¶ldÃ¼r"

â†’ Model tek nÃ¶rona baÄŸÄ±mlÄ± olamaz
â†’ Daha robust (dayanÄ±klÄ±)
```

#### 4. Early Stopping â­
```
Val loss kÃ¶tÃ¼leÅŸmeye baÅŸladÄ± mÄ±? â†’ DUR!

â†’ En basit regularization
â†’ Pratikte Ã§ok etkili
```

---

## 8ï¸âƒ£ Ã–lÃ§ekleme: Neden Ã–nemli?

### ğŸ¢ Ã‡arpÄ±k YÃ¼zey Problemi

```
Ã–zellik 1: 0-1 arasÄ±
Ã–zellik 2: 0-10,000 arasÄ±

Loss YÃ¼zeyi:

        Ã–z2
         â†‘
    â•±â•²  â”‚  â•±â•²     â† Ã‡ok dik (Ã–z2 hassas)
   â•±  â•² â”‚ â•±  â•²
  â•±    â•²â”‚â•±    â•²
 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Ã–z1
        â† Ã‡ok yassÄ± (Ã–z1 etkisiz)

Gradient Descent bu yÃ¼zeyde ZÄ°KZAK yapar!
```

### âœ¨ Ã–lÃ§ekleme SonrasÄ±

```
Ã–zellik 1: Ortalama=0, Std=1
Ã–zellik 2: Ortalama=0, Std=1

Loss YÃ¼zeyi:

        Ã–z2
         â†‘
       â•±â”€â•²        â† Dairevi (dengeli)
      â”‚   â”‚
      â•²â”€â”€â”€â•±
 â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Ã–z1

Gradient Descent DÃœMDÃœZ iner! âœ“
```

### ğŸ“ Standardization (Z-Score)

```
x' = (x - mean) / std

Ã–rnek:
  YaÅŸlar: [20, 30, 40, 50, 60]
  Mean: 40
  Std: 14.14
  
  Ã–lÃ§eklenmiÅŸ: [-1.41, -0.71, 0, 0.71, 1.41]
               â†‘
           Ortalama=0, Std=1
```

**Ne Zaman:** Ã‡oÄŸu durumda ilk tercih!

---

## 9ï¸âƒ£ DeÄŸerlendirme Metrikleri

### ğŸ“ Regresyon Metrikleri

#### RMSE (Root Mean Squared Error)
```
Tahmin: [100, 200, 300]
GerÃ§ek: [110, 190, 310]
Hata:   [10,  10,  10]

MSE = (10Â² + 10Â² + 10Â²) / 3 = 33.33
RMSE = âˆš33.33 = 5.77

Birim: AynÄ± (TL, mÂ², vs.) â†’ Yorumlanabilir!
```

#### MAE (Mean Absolute Error)
```
Tahmin: [100, 200, 300]
GerÃ§ek: [110, 190, 310]
Hata:   [10,  10,  10]

MAE = (|10| + |10| + |10|) / 3 = 10

Daha robust (aykÄ±rÄ± deÄŸerlere toleranslÄ±)
```

### ğŸ¯ Classification Metrikleri

#### Confusion Matrix
```
                 Tahmin
              Pos      Neg
GerÃ§ek Pos    90   |   10    â† FN (False Negative) "KaÃ§Ä±rdÄ±k"
       Neg    20   |  880    â† TN (True Negative) "DoÄŸru red"
              â†‘
            FP (False Positive) "YanlÄ±ÅŸ alarm"
```

#### Precision (Ä°sabet)
```
Precision = TP / (TP + FP)
          = 90 / (90 + 20)
          = 0.82

"Pozitif dediÄŸimizin %82'si gerÃ§ekten pozitif"
```

#### Recall (Yakalama)
```
Recall = TP / (TP + FN)
       = 90 / (90 + 10)
       = 0.90

"GerÃ§ek pozitiflerin %90'Ä±nÄ± yakaladÄ±k"
```

#### F1 Score (Denge)
```
F1 = 2 Ã— (Precision Ã— Recall) / (Precision + Recall)
   = 2 Ã— (0.82 Ã— 0.90) / (0.82 + 0.90)
   = 0.86

Ä°kisinin harmonik ortalamasÄ± (dengeli)
```

### âš ï¸ Accuracy TuzaÄŸÄ±

```
Veri: %99 negatif, %1 pozitif

Aptal Model: "Hep negatif de"
Accuracy: %99  â† YANILTI!
Recall: 0      â† HiÃ§ pozitif yakalayamÄ±yor!

Ã‡Ã¶zÃ¼m: Dengesiz veride Precision/Recall/F1 kullan
```

---

## ğŸ”Ÿ Data Leakage (SÄ±zÄ±ntÄ±)

### ğŸš¨ En Sinsi Hata

**TanÄ±m:** EÄŸitimde olmamasÄ± gereken bilgi **sÄ±zdÄ±**

### ğŸ“… Temporal Leakage (Zaman SÄ±zÄ±ntÄ±sÄ±)

```
âŒ YANLIÅ:
  2020 verileri â†’ Train
  2019 verileri â†’ Test
  
  â†’ GELECEÄÄ° bilip GEÃ‡MÄ°ÅÄ° tahmin ediyorsun! (HILE)

âœ… DOÄRU:
  2019 verileri â†’ Train
  2020 verileri â†’ Test
  
  â†’ GeÃ§miÅŸten geleceÄŸi tahmin (gerÃ§ekÃ§i)
```

### ğŸ”— Target Leakage (Hedef SÄ±zÄ±ntÄ±sÄ±)

```
Hedef: Kredi geri Ã¶denecek mi? (Evet/HayÄ±r)

Ã–zellikler:
  - YaÅŸ âœ“
  - Gelir âœ“
  - Kredi skoru âœ“
  - Geri_Ã¶deme_planÄ± âœ— â† SIKINTI!
                        (Bu hedefe Ã§ok yakÄ±n!)

Neden?
â†’ Geri Ã¶deme planÄ± ancak Ã¶deme BAÅLADIÄINDA bilinir
â†’ Tahmin anÄ±nda bu bilgi YOK!
```

### ğŸ§® Normalization Leakage

```python
# âŒ YANLIÅ: TÃ¼m veriyi kullanarak normalize et
mean = all_data.mean()  # Train + Test!
std = all_data.std()
normalized = (all_data - mean) / std

# âœ… DOÄRU: Sadece train'den Ã¶ÄŸren
mean = train_data.mean()  # Sadece train
std = train_data.std()
train_norm = (train_data - mean) / std
test_norm = (test_data - mean) / std  # AynÄ± mean/std kullan
```

### ğŸ” Leak Tespiti

```
Sorular:
1. Bu Ã¶zellik tahmin ANI'nda bilinir mi?
2. Bu Ã¶zellik hedefle "fazla" iliÅŸkili mi? (correlation > 0.95)
3. Zaman sÄ±rasÄ±nÄ± bozdum mu?
4. Test verisine hiÃ§ dokundum mu?

EÄŸer herhangi biri "HayÄ±r" â†’ Leakage olabilir!
```

---

## 1ï¸âƒ£1ï¸âƒ£ Deney Disiplini

### ğŸ”¬ Bilimsel YÃ¶ntem

```
1. GÃ–ZLEM
   "Val loss platoya ulaÅŸtÄ±"
   
2. HÄ°POTEZ
   "LR Ã§ok bÃ¼yÃ¼k olabilir"
   
3. DENEY
   "LR'Ä± yarÄ±ya indireyim"
   
4. Ã–LÃ‡ÃœM
   "Val loss deÄŸiÅŸti mi?"
   
5. SONUÃ‡
   "Evet, dÃ¼zeldi" â†’ Hipotez doÄŸru âœ“
   "HayÄ±r" â†’ BaÅŸka hipotez dene
   
6. Ã–ÄRENÄ°M
   "LR hassasiyeti yÃ¼ksekmiÅŸ"
```

### ğŸ“Š Baseline Stratejisi

```
Level 0: En Basit (Dummy)
  Regresyon: Ortalama tahmin et
  SÄ±nÄ±flama: En sÄ±k sÄ±nÄ±fÄ± seÃ§
  
  SonuÃ§: Accuracy %65
  â†“

Level 1: Basit Model
  Linear Regression / Logistic Regression
  
  SonuÃ§: Accuracy %72 (+7%)
  â†“

Level 2: Standard
  Random Forest / XGBoost
  
  SonuÃ§: Accuracy %85 (+13%)
  â†“

Level 3: KarmaÅŸÄ±k (Gerekiyorsa)
  Neural Network / Custom
  
  SonuÃ§: Accuracy %87 (+2%, pahalÄ±!)
```

**Kural:** Her seviyeyi **beat et**, sonra geÃ§!

### ğŸ“ Experiment Log Åablonu

```markdown
## Deney #005 - 2025-10-06

### Hipotez
"L2=0.001 ekleyince overfit azalacak"

### Setup
- Model: LinearRegression
- LR: 0.01
- L2: 0.001  â† DEÄÄ°ÅTÄ°RÄ°LEN
- Batch: 32
- Seed: 42

### Baseline (Deney #004)
Train: 0.05, Val: 0.15 (overfit!)

### SonuÃ§
Train: 0.08 (+0.03, beklenen)
Val: 0.10 (-0.05, âœ“ dÃ¼zeldi!)

### Karar
âœ“ L2 etkili
â†’ SÄ±rada: Early stopping ekle
```

---

## 1ï¸âƒ£2ï¸âƒ£ En SÄ±k 10 Hata & Ã‡Ã¶zÃ¼m

### ğŸ› Hata KataloÄŸu

#### 1. Learning Rate Felaketi
```
Belirti: Loss â†’ NaN veya sonsuz
Sebep: LR Ã§ok bÃ¼yÃ¼k
Ã‡Ã¶zÃ¼m: LR'Ä± 10x dÃ¼ÅŸÃ¼r (0.01 â†’ 0.001)
```

#### 2. Shape UyumsuzluÄŸu
```
Belirti: "RuntimeError: size mismatch (10,5) vs (10,3)"
Sebep: Katmanlar uyuÅŸmuyor
Ã‡Ã¶zÃ¼m: Her katmanda print(x.shape)
```

#### 3. Device Mismatch
```
Belirti: "Expected MPS tensor but got CPU"
Sebep: BazÄ± tensor'ler farklÄ± cihazda
Ã‡Ã¶zÃ¼m: Hepsini aynÄ± device'a taÅŸÄ± (.to(device))
```

#### 4. Gradient Unutma
```
Belirti: Training unstable, loss zÄ±plÄ±yor
Sebep: optimizer.zero_grad() unutulmuÅŸ
Ã‡Ã¶zÃ¼m: Her iterasyon baÅŸÄ±nda zero_grad()
```

#### 5. Val = Test KarÄ±ÅŸtÄ±rma
```
Belirti: Test'te beklenmedik dÃ¼ÅŸÃ¼k performans
Sebep: Val'e bakÄ±p ayar yaptÄ±n (ezberledin)
Ã‡Ã¶zÃ¼m: Test'e sadece BÄ°R KEZ bak
```

#### 6. Ã–lÃ§ekleme Unutma
```
Belirti: YakÄ±nsama Ã§ok yavaÅŸ, LR hassas
Sebep: Ã–zellikler normalize edilmemiÅŸ
Ã‡Ã¶zÃ¼m: StandardScaler kullan
```

#### 7. Data Leakage
```
Belirti: Train mÃ¼kemmel, test felaket
Sebep: Test bilgisi train'e sÄ±zmÄ±ÅŸ
Ã‡Ã¶zÃ¼m: Temporal split, normalization dikkat
```

#### 8. Dengesiz SÄ±nÄ±fta Accuracy
```
Belirti: %99 accuracy ama iÅŸe yaramaz
Sebep: SÄ±nÄ±f oranÄ± %1/%99
Ã‡Ã¶zÃ¼m: Precision/Recall/F1 kullan
```

#### 9. Seed Yok
```
Belirti: Her Ã§alÄ±ÅŸtÄ±rmada farklÄ± sonuÃ§
Sebep: Seed sabitlenmemiÅŸ
Ã‡Ã¶zÃ¼m: set_seed(42) en baÅŸta
```

#### 10. Overfit Ä°ÅŸareti KaÃ§Ä±rma
```
Belirti: Train â†“â†“, Val â†‘â†‘
Sebep: Erken durdurma yok
Ã‡Ã¶zÃ¼m: Early stopping ekle
```

### ğŸš‘ Acil Durum Kiti (Ä°lk YardÄ±m)

```
Problem: Kod Ã§alÄ±ÅŸmÄ±yor!

Checklist:
1. â˜ LR Ã§ok bÃ¼yÃ¼k mÃ¼? â†’ YarÄ±ya indir
2. â˜ Ã–zellikler Ã¶lÃ§ekli mi? â†’ Standardize et
3. â˜ zero_grad() var mÄ±? â†’ Ekle
4. â˜ Shape'ler uyumlu mu? â†’ YazdÄ±r
5. â˜ Device aynÄ± mÄ±? â†’ Kontrol et
6. â˜ Loss doÄŸru mu? â†’ Regresyon â‰  CE
```

---

## 1ï¸âƒ£3ï¸âƒ£ Mini Quiz (Kendini Test Et!)

### ğŸ¯ Soru 1: Overfit Tespiti
```
Train Loss: 0.01
Val Loss:   0.50

Durum: ?
Ã‡Ã¶zÃ¼m: ?

Cevap: Overfit! (Ezber)
Ã‡Ã¶zÃ¼m: L2 ekle, early stopping, daha fazla veri
```

### ğŸ¯ Soru 2: Accuracy TuzaÄŸÄ±
```
Veri: %95 negatif, %5 pozitif
Model: "Hep negatif"
Accuracy: %95

Sorun: ?

Cevap: Model hiÃ§ pozitif yakalayamÄ±yor! (Recall=0)
Ã‡Ã¶zÃ¼m: F1, Precision/Recall kullan
```

### ğŸ¯ Soru 3: Loss PatlamasÄ±
```
Epoch 1: Loss = 2.5
Epoch 2: Loss = 5.8
Epoch 3: Loss = NaN

Sebep: ?

Cevap: LR Ã§ok bÃ¼yÃ¼k
Ã‡Ã¶zÃ¼m: LR'Ä± 10x dÃ¼ÅŸÃ¼r
```

### ğŸ¯ Soru 4: YavaÅŸ YakÄ±nsama
```
1000 epoch sonra hala dÃ¼zelmiyor
LR hassas (biraz artÄ±rÄ±nca patlar)

Sebep: ?

Cevap: Ã–lÃ§ekleme yapÄ±lmamÄ±ÅŸ, condition number yÃ¼ksek
Ã‡Ã¶zÃ¼m: StandardScaler kullan
```

---

## 1ï¸âƒ£4ï¸âƒ£ SÃ¶zlÃ¼k: Cep KartÄ±

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  TERÄ°M              AÃ‡IKLAMA                 â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Model              Girdiâ†’Ã‡Ä±ktÄ± fonksiyonu   â”‚
â”‚  Parametre (Î¸)      Model'in Ã¶ÄŸrenen sayÄ±lar â”‚
â”‚  Hiperparametre     Senin seÃ§tiklerin (LR)   â”‚
â”‚  Loss               Hata miktarÄ± (tek sayÄ±)  â”‚
â”‚  Gradient           EÄŸim (hangi yÃ¶ne?)       â”‚
â”‚  Learning Rate      AdÄ±m bÃ¼yÃ¼klÃ¼ÄŸÃ¼           â”‚
â”‚  Optimizer          Gradient descent algoritmâ”‚
â”‚  Epoch              TÃ¼m veriyi bir kez gÃ¶rme â”‚
â”‚  Batch              Mini grup (32-256)       â”‚
â”‚  Overfit            Ezber (train iyi, val kÃ¶tÃ¼)
â”‚  Underfit           Yetersiz Ã¶ÄŸrenme         â”‚
â”‚  Regularization     Ezberi frenle (L2, dropout)
â”‚  Early Stopping     Val bozulunca dur        â”‚
â”‚  Validation         Ara sÄ±nav (ayar seÃ§)     â”‚
â”‚  Test               Final (tek seferlik)     â”‚
â”‚  Precision          Ä°sabet oranÄ±             â”‚
â”‚  Recall             Yakalama oranÄ±           â”‚
â”‚  F1                 Precision+Recall dengesi â”‚
â”‚  Data Leakage       Test bilgisi sÄ±zdÄ±       â”‚
â”‚  Seed               Rastgelelik sabitleyici  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 1ï¸âƒ£5ï¸âƒ£ Neden Lineer Regresyon ile BaÅŸlÄ±yoruz?

### ğŸ¯ Pedagojik Avantajlar

#### 1. Basit ve Temiz
```
y = wâ‚€ + wâ‚Ã—xâ‚ + wâ‚‚Ã—xâ‚‚

â†’ Tek global minimum (convex)
â†’ Optimizasyon davranÄ±ÅŸÄ± NET gÃ¶rÃ¼nÃ¼r
â†’ Debug kolay
```

#### 2. TÃ¼m Kavramlar Var
```
âœ“ Loss (MSE)
âœ“ Gradient
âœ“ Optimization (GD)
âœ“ Regularization (L2)
âœ“ Overfitting
âœ“ Val/Test split
âœ“ Metrics (RÂ², RMSE)

â†’ Kamp eÄŸitimi gibi! Her beceri burada Ã¶ÄŸrenilir
```

#### 3. GÃ¶rsel Anlama Kolay
```
2D grafikte:
  
  Fiyat
    â”‚  â—
    â”‚    â—  â—
    â”‚  â—   â”€â”€â”€â”€ Fit line
    â”‚ â—  â—
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ mÂ²
    
"Ã‡izgiyi noktalara uydur" â†’ Herkes anlÄ±yor!
```

#### 4. Kavramlar Transfer Eder
```
Linear Regression'da Ã¶ÄŸrendiÄŸin:

Ã–lÃ§ekleme    â†’ MLP'de de ÅŸart
LR seÃ§imi    â†’ CNN'de de kritik
Overfit      â†’ Transformer'da da sorun
Early stop   â†’ Her yerde kullanÄ±lÄ±r

â†’ Temel burada SAÄLAM atÄ±lÄ±r!
```

---

## 1ï¸âƒ£6ï¸âƒ£ Week 0 BugÃ¼n: Ne YapmalÄ±?

### ğŸ“ Pratik AlÄ±ÅŸtÄ±rma (30 dk)

**3 Problem Yaz, Analiz Et:**

#### Problem 1: Regresyon
```
GÃ¶rev: Ev fiyatÄ± tahmini
Ã–zellikler: mÂ², oda sayÄ±sÄ±, yaÅŸ
Hedef: Fiyat (TL)

Analiz:
  - Loss: MSE (sayÄ±sal tahmin)
  - Metric: RMSE (yorumlanabilir)
  - Risk: AykÄ±rÄ± deÄŸer (lÃ¼ks villa)
  - Split: Rastgele 70/15/15
```

#### Problem 2: Dengesiz SÄ±nÄ±flama
```
GÃ¶rev: Kredi kartÄ± dolandÄ±rÄ±cÄ±lÄ±ÄŸÄ±
Ã–zellikler: Ä°ÅŸlem tutarÄ±, zaman, lokasyon
Hedef: DolandÄ±rÄ±cÄ±lÄ±k mÄ±? (Evet/HayÄ±r)
Denge: %99 normal, %1 dolandÄ±rÄ±cÄ±lÄ±k

Analiz:
  - Loss: Cross-Entropy (+ class weight)
  - Metric: F1, Recall (yakalama Ã¶nemli!)
  - Risk: Accuracy yanÄ±ltÄ±r (%99 aptal model)
  - Split: Stratified (oranÄ± koru)
```

#### Problem 3: Zaman Serisi
```
GÃ¶rev: SatÄ±ÅŸ tahmini
Ã–zellikler: GeÃ§miÅŸ satÄ±ÅŸ, mevsim, promosyon
Hedef: YarÄ±nki satÄ±ÅŸ

Analiz:
  - Loss: MAE (outlier'a robust)
  - Metric: MAPE (yÃ¼zde hata)
  - Risk: Temporal leakage (gelecek sÄ±zmasÄ±n!)
  - Split: Zamansal (geÃ§miÅŸâ†’gelecek)
```

### âœ… Self-Check (Kendini Kontrol Et)

```
Week 0 bittiÄŸinde cevap verebiliyor musun?

â–¡ "Model nedir?" â†’ AyarlÄ± fonksiyon
â–¡ "Loss nedir?" â†’ Hata miktarÄ± (tek sayÄ±)
â–¡ "Gradient nedir?" â†’ EÄŸim (hangi yÃ¶ne)
â–¡ "Overfit nedir?" â†’ Ezber (train iyi, val kÃ¶tÃ¼)
â–¡ "Neden train/val/test?" â†’ Ezber Ã¶nlemek
â–¡ "Neden Ã¶lÃ§ekleme?" â†’ Loss yÃ¼zeyini yuvarlatmak
â–¡ "Leakage nedir?" â†’ Test bilgisi sÄ±zdÄ±
â–¡ "Precision vs Recall?" â†’ Ä°sabet vs Yakalama
â–¡ "Neden LR kÃ¼Ã§Ã¼k olmalÄ±?" â†’ PatlamayÄ± Ã¶nlemek

Hepsi âœ“ ise â†’ Week 1'e hazÄ±rsÄ±n! ğŸš€
```

---

## 1ï¸âƒ£7ï¸âƒ£ Tek Paragraf Ã–zet (HafÄ±zana KazÄ±nsÄ±n)

> **Makine Ã¶ÄŸrenmesi**, veriye bakÄ±p bir **kural** bulma iÅŸidir. Bu kuralÄ±, hatamÄ±zÄ± tek sayÄ±ya indirgeyen **kayÄ±p** ile Ã¶lÃ§er, **gradyan adÄ±mlarÄ±** ile parametreleri dÃ¼zeltiriz. **DoÄŸrulama kÃ¼mesi** bize "ezberledin mi?" diye sorar; gerekirse **erken durdurur** ve **dÃ¼zenleriz**. **Ã–lÃ§ekleme**, **doÄŸru metrik**, **dÃ¼rÃ¼st veri ayrÄ±mÄ±** ve **kÃ¼Ã§Ã¼k ama sÃ¼rekli deneyler** baÅŸarÄ±yÄ± garanti eder. Temel bu; Ã¼stÃ¼ne her ÅŸeyi inÅŸa edebiliriz.

---

## ğŸ“ Sonraki AdÄ±m

### ğŸ“š Okuma SÄ±rasÄ±

```
âœ… Bu dÃ¶kÃ¼man (theory_intro.md)
   â””â”€ Sezgi oturdu âœ“

â¬œ theory_foundations.md
   â””â”€ Biraz daha detay + gÃ¶rsel

â¬œ theory_mathematical.md
   â””â”€ Matematiksel derinlik (opsiyonel)

â¬œ Kurulum & Setup
   â””â”€ PyTorch MPS test

â¬œ Week 1: Linear Regression
   â””â”€ PRATIK!
```

### ğŸš€ HazÄ±r mÄ±sÄ±n?

```bash
cd /Users/onur/code/novadev-protocol
source .venv/bin/activate

# Week 1'e baÅŸla:
python week1_tensors/linreg_manual.py
```

**BaÅŸarÄ± Kriteri:**
> Week 1'de kod yazarken:
> "Aha! Gradient burasÄ±!"
> "Ä°ÅŸte overfit belirtisi!"
> diyebilmen.

---

**ğŸ‰ Tebrikler! Week 0 Introduction tamamlandÄ±!**

**Åimdi:** Kod yazmadan Ã¶nce **zihnindeKÄ° model** oluÅŸtu.
**Sonra:** Kodu yazdÄ±ÄŸÄ±nda "neden bÃ¶yle?" bileceksin.
**SonuÃ§:** Daha az hata, daha hÄ±zlÄ± Ã¶ÄŸrenme, daha derin anlayÄ±ÅŸ!

**HazÄ±r ol, Week 1 geliyor!** ğŸ’ª

