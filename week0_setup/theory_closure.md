# Week 0 â€” KapanÄ±ÅŸ DÃ¶kÃ¼manÄ±

**NovaDev v1.0 - Self-Assessment & Final Check**

> "Kendi cÃ¼mlelerimle, kendi anlayÄ±ÅŸÄ±mla. Week 0'Ä± gerÃ§ekten Ã¶zÃ¼msedim mi?"

---

## ğŸ¯ Bu DÃ¶kÃ¼man HakkÄ±nda

**AmaÃ§:** Week 0 sonunda temel kavramlarÄ±n **kendi cÃ¼mlelerimle** netleÅŸtiÄŸini gÃ¶stermek

**Format:** 
- âœ… Son kontrol listesi (soru-cevap)
- ğŸ§  Mini-Ã¶dev Ã§Ã¶zÃ¼mleri
- ğŸ“Š Tablo formatÄ±nda karÅŸÄ±laÅŸtÄ±rmalar
- ğŸ“ Kendime notlar

**KullanÄ±m:**
- Week 0 bittiÄŸinde oku
- CevaplarÄ± kendi kelimelerinle yaz
- BoÅŸluk varsa geri dÃ¶n ilgili theory'ye
- Week 1'e geÃ§meden Ã¶nce rahat hisset

---

## âœ… Son Kontrol Listesi â€” CevaplarÄ±m

### 1ï¸âƒ£ Train / Validation / Test (Fark ve AmaÃ§)

#### Train Set
```
AmaÃ§: Model burada Ã–ÄRENÄ°R
Ne yapar: AÄŸÄ±rlÄ±klar bu veri Ã¼zerinde optimize edilir
Î¸ â† Î¸ - Î·âˆ‡L (train data)

Analoji: Ders Ã§alÄ±ÅŸma dÃ¶nemi
```

#### Validation Set
```
AmaÃ§: AYAR SEÃ‡ + Erken Durdur
Ne yapar: 
  - Hiperparametre seÃ§imi (LR, L2, epoch, batch)
  - Early stopping sinyali
  - Model bu veriyi Ã–ÄRENMEz, sadece performans Ã¶lÃ§er

Analoji: Ara sÄ±nav (Ã§alÄ±ÅŸma stratejini ayarla)
```

#### Test Set
```
AmaÃ§: FÄ°NAL DEÄERLENDÄ°RME
Ne yapar: GerÃ§ek dÃ¼nya performansÄ±nÄ± TEK SEFERDE gÃ¶ster
ASLA: Test'e bakÄ±p ayar yapma!

Analoji: Final sÄ±navÄ± (bir kez, dokunma)
```

#### Neden AyÄ±rÄ±yoruz?

**Ana Sebep:** Ezberi (overfit) yakalamak ve gerÃ§ek performansÄ± Ã¶lÃ§mek

```
Senario 1: AyrÄ±m yok
  â†’ Model train'i ezberler
  â†’ GerÃ§ek dÃ¼nyada felaket

Senario 2: Sadece train/test
  â†’ Test'e bakarak ayar yaparsÄ±n
  â†’ Test'i "ezberlersin" (indirect)

Senario 3: Train/Val/Test âœ“
  â†’ Val ile ayar yap
  â†’ Test sadece final rapor
  â†’ DÃ¼rÃ¼st deÄŸerlendirme
```

---

### 2ï¸âƒ£ Loss SeÃ§imi (Ne Zaman Hangisi?)

#### Regresyon Loss'larÄ±

**MSE (Mean Squared Error)**
```
FormÃ¼l: L = (1/N) Î£ (y - Å·)Â²

Ne zaman:
  âœ“ Normal daÄŸÄ±lÄ±mlÄ± hatalar
  âœ“ BÃ¼yÃ¼k hatalar gerÃ§ekten kÃ¶tÃ¼
  âœ“ Veri temiz, aykÄ±rÄ± az
  âœ“ Ä°lk tercih (standard)

Ã–zellik:
  âœ“ BÃ¼yÃ¼k hatalarÄ± AÄIR cezalar
  âœ— AykÄ±rÄ± deÄŸerlere HASSAS

Probabilistik kÃ¶ken:
  Gaussian noise â†’ MSE (MLE)
```

**MAE (Mean Absolute Error)**
```
FormÃ¼l: L = (1/N) Î£ |y - Å·|

Ne zaman:
  âœ“ AykÄ±rÄ± deÄŸer Ã‡OK
  âœ“ Robust tahmin gerekli
  âœ“ TÃ¼m hatalar eÅŸit Ã¶nemde

Ã–zellik:
  âœ“ AykÄ±rÄ±lara DAYANIKLI
  âœ— SÄ±fÄ±rda tÃ¼rev tanÄ±msÄ±z (kÃ¶ÅŸeli)

Median predict eder
```

**Huber Loss**
```
FormÃ¼l: 
  KÃ¼Ã§Ã¼k hata: MSE gibi (smooth)
  BÃ¼yÃ¼k hata: MAE gibi (robust)

Ne zaman:
  âœ“ DENGE arÄ±yorum
  âœ“ Hem smooth hem robust istiyorum
  
EN Ä°YÄ° PRATÄ°K DENGE!
```

#### SÄ±nÄ±flama Loss'larÄ±

**Cross-Entropy (CE)**
```
Binary: -(y log p + (1-y) log(1-p))
Multi-class: -Î£ y_k log p_k

Ne zaman:
  âœ“ SÄ±nÄ±flama (her zaman!)
  âœ“ OlasÄ±lÄ±k tahmini istiyorum

Ã–zellik:
  YanlÄ±ÅŸ sÄ±nÄ±fa YÃœKSEK GÃœVENde
  â†’ Ekstra AÄIR ceza

Probabilistik kÃ¶ken:
  Bernoulli/Categorical â†’ CE (MLE)
```

#### Ã–zet ReÃ§ete

```
Regresyon:
  Temiz veri â†’ MSE
  AykÄ±rÄ± Ã§ok â†’ MAE
  Denge â†’ Huber

SÄ±nÄ±flama:
  Cross-Entropy (standart)
  Ä°mbalanced â†’ + class weight / Focal Loss
```

---

### 3ï¸âƒ£ Learning Rate (LR) â€” TanÄ±m ve Semptomlar

#### TanÄ±m

```
Î¸_new = Î¸_old - Î· Ã— âˆ‡L
                 â†‘
            Learning Rate
            (AdÄ±m bÃ¼yÃ¼klÃ¼ÄŸÃ¼)
```

**Analoji:** DaÄŸdan inerken attÄ±ÄŸÄ±n adÄ±mÄ±n boyu

#### Semptomlar Tablosu

**LR Ã‡ok BÃ¼yÃ¼k**
```
Belirtiler:
  âŒ Loss ZIPlar (testere diÅŸi)
  âŒ NaN / Inf
  âŒ AÄŸÄ±rlÄ±k normlarÄ± patlar
  âŒ Val metrikleri kaotik

Grafik:
  Loss
    â”‚ â•±â•²â•±â•²â•±â•²
    â”‚â•±      â•²â•±â•²
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Epoch

Ä°lk YardÄ±m:
  1. LR'Ä± yarÄ±ya indir (0.01 â†’ 0.001)
  2. Ã–zellik Ã¶lÃ§ekle (StandardScaler)
  3. L2 ekle (weight decay)
  4. Gradient clipping (gerekirse)
```

**LR Biraz BÃ¼yÃ¼k**
```
Belirtiler:
  âš ï¸ YakÄ±nsÄ±yor ama salÄ±nÄ±m var
  âš ï¸ Val istikrarsÄ±z
  âš ï¸ Sweet spot'u bulmuyor

Grafik:
  Loss
    â”‚â•²   â•±â•²
    â”‚ â•² â•±  â•²â•±
    â”‚  â•²â•±
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Epoch

Ã‡Ã¶zÃ¼m:
  1. LR'Ä± azalt (0.01 â†’ 0.005)
  2. ReduceLROnPlateau kullan
  3. Cosine decay dene
```

**LR Uygun âœ“**
```
Belirtiler:
  âœ… Train dÃ¼ÅŸÃ¼yor (smooth)
  âœ… Val dÃ¼zenli iyileÅŸiyor
  âœ… AÅŸÄ±rÄ± salÄ±nÄ±m yok

Grafik:
  Loss
    â”‚â•²___
    â”‚    â•²___
    â”‚        â•²___
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Epoch

Aksiyon:
  1. Bu ayarÄ± KAYDET
  2. Early stopping eÅŸiÄŸi tanÄ±mla
  3. Week 1'de kullan
```

**LR Ã‡ok KÃ¼Ã§Ã¼k**
```
Belirtiler:
  â±ï¸ Ã‡ok YAVAÅ
  â±ï¸ Loss azalÄ±yor ama aÄŸÄ±r Ã§ekim
  â±ï¸ Epoch sayÄ±sÄ± yetersiz

Grafik:
  Loss
    â”‚â•²
    â”‚ â•²
    â”‚  â•²
    â”‚   â•²  (hala inmekte...)
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Epoch

Ã‡Ã¶zÃ¼m:
  1. LR'Ä± artÄ±r (0.0001 â†’ 0.001)
  2. Warmup + decay dene
  3. Epoch sayÄ±sÄ±nÄ± artÄ±r
```

#### Ä°lk YardÄ±m ProtokolÃ¼

```
1. LR'Ä± YARIYLA indir
   â†’ %70 durumda dÃ¼zelir

2. Ã–zellikleri STANDARTLAÅTIR
   â†’ Loss yÃ¼zeyi yuvarlanÄ±r

3. L2 EKLE (weight decay)
   â†’ PatlamayÄ± yumuÅŸatÄ±r

4. zero_grad() KONTROL
   â†’ Gradient accumulation olmasÄ±n

5. Loss/Metric DOÄRU mu?
   â†’ Regresyon â‰  CE

6. Shape/Dtype/Device?
   â†’ print(x.shape, x.dtype, x.device)
```

---

### 4ï¸âƒ£ Overfit vs Underfit â€” Sinyal ve Ä°lk YardÄ±m

#### Underfit (Ã–ÄŸrenemedi)

**Belirtiler:**
```
Train Loss: YÃœKSEK ğŸ“ˆ
Val Loss:   YÃœKSEK ğŸ“ˆ

Model yetersiz, Ã¶ÄŸrenememiÅŸ
```

**Grafiksel:**
```
Loss
  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Train (yÃ¼ksek)
  â”‚
  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Val (yÃ¼ksek)
  â”‚
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Epoch

Ä°kisi de kÃ¶tÃ¼ â†’ UNDERFIT
```

**Ã‡Ã¶zÃ¼mler:**
```
1. Kapasite â†‘
   - Daha karmaÅŸÄ±k model
   - Daha fazla katman/nÃ¶ron

2. Daha uzun eÄŸitim
   - Epoch artÄ±r
   - LR schedule dÃ¼zelt

3. Daha iyi Ã¶zellikler
   - Feature engineering
   - Daha iyi temsil

4. LR ayarÄ±
   - Ã‡ok kÃ¼Ã§Ã¼kse artÄ±r
   - Optimizer deÄŸiÅŸtir (AdamW)
```

#### Overfit (Ezber)

**Belirtiler:**
```
Train Loss: Ã‡OK DÃœÅÃœK ğŸ“‰
Val Loss:   YÃœKSEK ğŸ“ˆ

Model ezberlemiÅŸ, genelleyemiyor
```

**Grafiksel:**
```
Loss
  â”‚
  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•²___ Train (mÃ¼kemmel)
  â”‚            â•²
  â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•±â”€â”€ Val (kÃ¶tÃ¼leÅŸiyor)
  â”‚             â†‘
  â”‚        Overfit baÅŸladÄ±
  â”‚            (burda dur!)
  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Epoch
```

**Ã‡Ã¶zÃ¼mler (Regularization):**
```
1. Early Stopping â­
   Val loss kÃ¶tÃ¼leÅŸince DUR
   â†’ En basit ve etkili

2. L2 (Weight Decay)
   Î» = 1e-4, 1e-3, 1e-2
   â†’ AÄŸÄ±rlÄ±klarÄ± kÃ¼Ã§Ã¼k tut

3. Daha Ã§ok veri
   â†’ En iyi Ã§Ã¶zÃ¼m (varsa)

4. Data Augmentation
   â†’ Yapay veri Ã§eÅŸitliliÄŸi

5. Dropout (derin aÄŸlarda)
   p = 0.5 (hidden), 0.2 (input)
   â†’ NÃ¶ron baÄŸÄ±mlÄ±lÄ±ÄŸÄ±nÄ± kÄ±r

6. Kapasite â†“
   â†’ Model basitleÅŸtir (son Ã§are)
```

#### Val EÄŸrisi KuralÄ±

```
Train â†“â†“ ama Val â†‘â†‘ â†’ OVERFIT

Aksiyon:
  1. Hemen early stopping
  2. En iyi val checkpoint'e dÃ¶n
  3. Regularization artÄ±r
  4. Bir daha aynÄ± noktayÄ± geÃ§me
```

---

### 5ï¸âƒ£ Tensor Refleksi â€” Shape / Dtype / Device KontrolÃ¼

#### Shape KontrolÃ¼

**Neden Ã–nemli:**
```
En sÄ±k hata: Shape mismatch!
"RuntimeError: size mismatch (10,5) vs (10,3)"
```

**Kontrol NoktalarÄ±:**
```python
# Input
print(f"Input: {x.shape}")  # (64, 784)

# Layer output
print(f"After layer1: {h1.shape}")  # (64, 128)
print(f"After layer2: {h2.shape}")  # (64, 64)
print(f"Output: {y.shape}")  # (64, 10)

# Loss computation
print(f"Target: {target.shape}")  # (64,) or (64, 10)
```

**AlÄ±ÅŸkanlÄ±k:**
```
Her katman sonrasÄ±:
  assert h.shape == expected_shape
  
Kritik noktalarda:
  logger.info(f"Shape checkpoint: {x.shape}")
```

#### Dtype KontrolÃ¼

**Kurallar:**
```
Float iÅŸlemler:
  âœ“ Loss hesabÄ±: float32
  âœ“ Aktivasyonlar: float32
  âœ“ AÄŸÄ±rlÄ±klar: float32

Int iÅŸlemler:
  âœ“ SÄ±nÄ±f etiketleri: int64 (long)
  âœ“ Index'ler: int64

âŒ KarÄ±ÅŸÄ±rsa: Sessiz hata veya yavaÅŸlama
```

**Ã–rnek:**
```python
# âŒ YANLIÅ
target = torch.tensor([0, 1, 2], dtype=torch.float32)
loss = F.cross_entropy(logits, target)  # HATA!

# âœ… DOÄRU
target = torch.tensor([0, 1, 2], dtype=torch.long)
loss = F.cross_entropy(logits, target)  # OK
```

#### Device KontrolÃ¼

**Kurallar:**
```
TÃ¼m tensor'ler AYNI cihazda:
  âœ“ Model: device='mps'
  âœ“ Input: device='mps'
  âœ“ Target: device='mps'

âŒ KarÄ±ÅŸÄ±rsa:
  - CPU'da kalan tensor â†’ Sessiz YAVAÅLAMA
  - FarklÄ± device'lar â†’ RuntimeError
```

**Best Practice:**
```python
# BaÅŸta device belirle
device = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')

# Model
model = MyModel().to(device)

# Her batch
for x, y in dataloader:
    x = x.to(device)
    y = y.to(device)
    
    # Training...
```

#### Gradient AkÄ±ÅŸÄ±

**Kritik Kontrol:**
```python
# Her iterasyon
optimizer.zero_grad()  # Temizle
loss.backward()        # Hesapla
optimizer.step()       # GÃ¼ncelle

# âŒ zero_grad() unutulursa:
# Gradientler BÄ°RÄ°KÄ°R â†’ Patlama!
```

#### Checklist (Her Yeni Kod)

```
â–¡ Shape'leri logladÄ±m mÄ±?
â–¡ Dtype'lar doÄŸru mu? (float loss, long target)
â–¡ Device'lar aynÄ± mÄ±? (hepsi MPS/CPU/CUDA)
â–¡ zero_grad() her iterasyonda mÄ±?
â–¡ requires_grad=True parametrelerde mi?
â–¡ no_grad() gÃ¼ncellemede mi?
```

---

## ğŸ§  Mini-Ã–dev â€” Cevaplar

### A) Tek CÃ¼mlelik TanÄ±mlar

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ TERÄ°M           â”‚ TEK CÃœMLE TANIM               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Model           â”‚ Girdiyi Ã§Ä±ktÄ±ya dÃ¶nÃ¼ÅŸtÃ¼ren,   â”‚
â”‚                 â”‚ parametreleri ayarlanabilir    â”‚
â”‚                 â”‚ FONKSIYON (f_Î¸)               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Parametre       â”‚ Modelin Ã–ÄRENÄ°LEN iÃ§ sayÄ±larÄ± â”‚
â”‚                 â”‚ (aÄŸÄ±rlÄ±klar, bias'lar)        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Hiperparametre  â”‚ EÄŸitim sÃ¼recinin KULLANICI    â”‚
â”‚                 â”‚ SEÃ‡Ä°MÄ° ayarlarÄ± (LR, L2,      â”‚
â”‚                 â”‚ batch, katman sayÄ±sÄ±)         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Loss (KayÄ±p)    â”‚ Tahminin YANLIÅLIÄINI tek     â”‚
â”‚                 â”‚ sayÄ±da Ã¶lÃ§en fonksiyon        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Optimizer       â”‚ KayÄ±p azalacak ÅŸekilde        â”‚
â”‚                 â”‚ parametreleri GÃœNCELLEYEN     â”‚
â”‚                 â”‚ algoritma (SGD, AdamW)        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Regularization  â”‚ Ezberi (OVERFIT) FRENLEYEN    â”‚
â”‚                 â”‚ teknikler (L2, early stopping,â”‚
â”‚                 â”‚ dropout, augmentation)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### B) ÃœÃ§ Problem: DoÄŸru Metrik & OlasÄ± Leakage

#### Problem 1: Ev FiyatÄ± Tahmini (Regresyon)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ METRIK                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Birincil: RMSE                              â”‚
â”‚   â†’ Birim anlamlÄ± (TL cinsinden)            â”‚
â”‚                                             â”‚
â”‚ Ä°kincil: MAE                                â”‚
â”‚   â†’ AykÄ±rÄ±lara robust                       â”‚
â”‚                                             â”‚
â”‚ RÂ² (opsiyonel)                              â”‚
â”‚   â†’ Varyans aÃ§Ä±klama oranÄ±                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LOSS                                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Temel: MSE                                  â”‚
â”‚ AykÄ±rÄ± varsa: Huber / MAE                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LEAKAGE RÄ°SKLERÄ°                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ âŒ "Ä°lan sonrasÄ±" bilgiler                  â”‚
â”‚    â†’ PazarlÄ±k detayÄ±, satÄ±ÅŸ tarihi          â”‚
â”‚                                             â”‚
â”‚ âŒ BÃ¶lgesel ortalama etiketten tÃ¼retme      â”‚
â”‚    â†’ "Mahalledeki ortalama fiyat" Ã¶zelliÄŸi  â”‚
â”‚    â†’ EÄŸer etiket = fiyat ise SIZINTI!       â”‚
â”‚                                             â”‚
â”‚ âŒ Gelecek bilgi                            â”‚
â”‚    â†’ "6 ay sonraki mahalle deÄŸeri"          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DOÄRU AYIRMA                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Standard: Rastgele split (70/15/15)         â”‚
â”‚                                             â”‚
â”‚ CoÄŸrafi kayma varsa:                        â”‚
â”‚   â†’ BÃ¶lge bazlÄ± split                       â”‚
â”‚   â†’ Train: Ä°stanbul                         â”‚
â”‚   â†’ Test: Ankara (distribution shift test) â”‚
â”‚                                             â”‚
â”‚ Zaman serisi ise:                           â”‚
â”‚   â†’ Temporal split (geÃ§miÅŸâ†’gelecek)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Problem 2: DolandÄ±rÄ±cÄ±lÄ±k Tespiti (Dengesiz SÄ±nÄ±flama)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ METRIK                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Birincil: Recall â­                         â”‚
â”‚   â†’ DolandÄ±rÄ±cÄ±lÄ±k kaÃ§Ä±rmak PAHALI          â”‚
â”‚   â†’ "GerÃ§ek pozitifin %'sini yakaladÄ±k?"    â”‚
â”‚                                             â”‚
â”‚ Ä°kincil: Precision                          â”‚
â”‚   â†’ YanlÄ±ÅŸ alarm maliyeti                   â”‚
â”‚                                             â”‚
â”‚ Denge: F1 Score                             â”‚
â”‚   â†’ Precision + Recall harmonik ort.        â”‚
â”‚                                             â”‚
â”‚ EÄŸri: PR-AUC â­                             â”‚
â”‚   â†’ Ä°mbalanced data'da ROC-AUC'dan iyi     â”‚
â”‚                                             â”‚
â”‚ âŒ KULLANMA: Accuracy                       â”‚
â”‚   â†’ %99 negatif varsa yanÄ±ltÄ±r!             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LOSS                                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Cross-Entropy + Class Weight                â”‚
â”‚   â†’ AzÄ±nlÄ±k sÄ±nÄ±fa aÄŸÄ±rlÄ±k ver              â”‚
â”‚                                             â”‚
â”‚ Alternatif: Focal Loss                      â”‚
â”‚   â†’ Kolay Ã¶rnekleri down-weight             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LEAKAGE RÄ°SKLERÄ°                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ âŒ Etiket kurallarÄ±ndan tÃ¼retilen Ã¶zelliklerâ”‚
â”‚    â†’ "Manuel inceleme sonucu" flag'i        â”‚
â”‚    â†’ EÄŸer etiket bununla oluÅŸturulduysa!    â”‚
â”‚                                             â”‚
â”‚ âŒ MÃ¼dahale sonrasÄ± bilgi                   â”‚
â”‚    â†’ "Hesap bloklandÄ±" durumu               â”‚
â”‚    â†’ "KullanÄ±cÄ± ÅŸikayet etti"               â”‚
â”‚                                             â”‚
â”‚ âŒ Gelecek davranÄ±ÅŸ                         â”‚
â”‚    â†’ "30 gÃ¼n sonra chargeback oldu"         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DOÄRU AYIRMA                                â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Stratified Split â­                         â”‚
â”‚   â†’ SÄ±nÄ±f oranÄ±nÄ± KORU                      â”‚
â”‚   â†’ Train: %1 pozitif                       â”‚
â”‚   â†’ Val: %1 pozitif                         â”‚
â”‚   â†’ Test: %1 pozitif                        â”‚
â”‚                                             â”‚
â”‚ Zaman kaymasÄ± varsa:                        â”‚
â”‚   â†’ Temporal split                          â”‚
â”‚   â†’ DolandÄ±rÄ±cÄ±lÄ±k pattern'leri deÄŸiÅŸir     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Problem 3: SatÄ±ÅŸ Tahmini (Zaman Serisi)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ METRIK                                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Birincil: MAE / RMSE                        â”‚
â”‚   â†’ MAE: Outlier'a robust                   â”‚
â”‚   â†’ RMSE: BÃ¼yÃ¼k hata Ã¶nemliyse              â”‚
â”‚                                             â”‚
â”‚ Ä°kincil: MAPE (Mean Abs % Error)           â”‚
â”‚   â†’ YÃ¼zde hata (karÅŸÄ±laÅŸtÄ±rÄ±labilir)        â”‚
â”‚   â†’ Dikkat: y=0 ise tanÄ±msÄ±z!               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LOSS                                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ MSE veya MAE                                â”‚
â”‚   â†’ Outlier Ã§oksa MAE tercih                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LEAKAGE RÄ°SKLERÄ° âš ï¸ EN YÃœKSEK!             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ âŒ GELECEÄÄ° geÃ§miÅŸe karÄ±ÅŸtÄ±rmak (EN SÄ°K!)   â”‚
â”‚    â†’ Gelecek ay kampanyasÄ±                  â”‚
â”‚    â†’ Gelecek haftaki stok durumu            â”‚
â”‚    â†’ Future rolling mean/std                â”‚
â”‚                                             â”‚
â”‚ âŒ Normalization'da leak                    â”‚
â”‚    â†’ TÃ¼m veriden mean/std                   â”‚
â”‚    â†’ DOÄRUSU: Sadece geÃ§miÅŸten Ã¶ÄŸren        â”‚
â”‚                                             â”‚
â”‚ âŒ Feature engineering'de leak              â”‚
â”‚    â†’ "Bu ayÄ±n sonu toplam satÄ±ÅŸ" Ã¶zelliÄŸi   â”‚
â”‚    â†’ AyÄ±n sonu henÃ¼z gelmedi!               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ DOÄRU AYIRMA â­ KRÄ°TÄ°K                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Temporal Split (MUTLAK)                     â”‚
â”‚   â†’ Train: 2019-01 ~ 2019-12                â”‚
â”‚   â†’ Val:   2020-01 ~ 2020-03                â”‚
â”‚   â†’ Test:  2020-04 ~ 2020-06                â”‚
â”‚                                             â”‚
â”‚ Rolling Window                              â”‚
â”‚   â†’ Her tahmin iÃ§in: geÃ§miÅŸ â†’ gelecek       â”‚
â”‚   â†’ Asla geriye bakma!                      â”‚
â”‚                                             â”‚
â”‚ Expanding Window                            â”‚
â”‚   â†’ Train seti zamanla bÃ¼yÃ¼r                â”‚
â”‚   â†’ Ama asla gelecek dahil edilmez          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### C) LR'Ä± 10Ã— BÃ¼yÃ¼tÃ¼rsen / 10Ã— KÃ¼Ã§Ã¼ltÃ¼rsen

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   DURUM      â”‚        SEMPTOM             â”‚      HIZLI TEÅHÄ°S             â”‚          Ã‡Ã–ZÃœM              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ LR Ã—10       â”‚ âŒ Loss ZIPlar             â”‚ AÄŸÄ±rlÄ±k normlarÄ± artÄ±yor      â”‚ 1. LR'Ä± yarÄ±ya/onda bire    â”‚
â”‚ (Ã‡OK BÃœYÃœK)  â”‚ âŒ NaN / Inf               â”‚ Gradient'lar patlÄ±yor         â”‚ 2. L2 ekle (weight decay)   â”‚
â”‚              â”‚ âŒ Val metrikleri kaotik   â”‚ Loss grafik testere diÅŸi      â”‚ 3. Gradient clipping        â”‚
â”‚              â”‚ âŒ AÄŸÄ±rlÄ±klar aÅŸÄ±rÄ± bÃ¼yÃ¼r  â”‚ Diverge ediyor                â”‚ 4. Ã–zellikleri Ã¶lÃ§ekle      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ LR Ã·10       â”‚ â±ï¸ Loss Ã‡OK YAVAÅ azalÄ±yorâ”‚ AynÄ± epoch'ta belirgin        â”‚ 1. LR'Ä± kademeli artÄ±r      â”‚
â”‚ (Ã‡OK KÃœÃ‡ÃœK)  â”‚ â±ï¸ EÄŸitim sÃ¼rÃ¼yor ama     â”‚ iyileÅŸme yok                  â”‚ 2. LR warmup + decay dene   â”‚
â”‚              â”‚    ilerlemiyor             â”‚ Gradient'lar Ã§ok kÃ¼Ã§Ã¼k        â”‚ 3. Epoch sayÄ±sÄ±nÄ± artÄ±r     â”‚
â”‚              â”‚ â±ï¸ AÄŸÄ±r Ã§ekim              â”‚ Momentum yok                  â”‚ 4. AdamW dene (adaptif LR)  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Biraz BÃ¼yÃ¼k  â”‚ âš ï¸ YakÄ±nsÄ±yor ama salÄ±nÄ±m â”‚ LR azaltÄ±nca stabil oluyor    â”‚ 1. LR'Ä± azalt               â”‚
â”‚              â”‚ âš ï¸ Val istikrarsÄ±z        â”‚ Sweet spot bulamÄ±yor          â”‚ 2. ReduceLROnPlateau        â”‚
â”‚              â”‚ âš ï¸ Converge edemiyor      â”‚ Oscillation var               â”‚ 3. Cosine decay             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ TAM Ä°SABET âœ“ â”‚ âœ… Train dÃ¼ÅŸÃ¼yor (smooth) â”‚ EÄŸriler sakin                 â”‚ 1. Bu ayarÄ± KAYDET          â”‚
â”‚              â”‚ âœ… Val dÃ¼zenli iyileÅŸiyor â”‚ AÅŸÄ±rÄ± salÄ±nÄ±m yok             â”‚ 2. Early stopping eÅŸiÄŸi     â”‚
â”‚              â”‚ âœ… Stabil convergence     â”‚ Sweet spot'ta!                â”‚    tanÄ±mla                  â”‚
â”‚              â”‚                            â”‚                               â”‚ 3. Week 1'de kullan         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Grafiksel KarÅŸÄ±laÅŸtÄ±rma:**

```
LR Ã—10 (Ã‡ok BÃ¼yÃ¼k):
  Loss
    â”‚ â•±â•²â•±â•²â•±â•²â•±â•²
    â”‚â•±        â•²â•±
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Epoch
    Testere diÅŸi, diverge

LR Ã·10 (Ã‡ok KÃ¼Ã§Ã¼k):
  Loss
    â”‚â•²
    â”‚ â•²
    â”‚  â•²
    â”‚   â•²  (hala inmekte)
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Epoch
    AÄŸÄ±r Ã§ekim

LR Tam (âœ“):
  Loss
    â”‚â•²___
    â”‚    â•²___
    â”‚        â•²___
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Epoch
    Smooth, stabil
```

---

## ğŸ“ Ek Notlar (Kendime)

### Ã–lÃ§ekleme (Standardization)

```
x' = (x - Î¼) / Ïƒ

Neden?
  â†’ Loss yÃ¼zeyini YUVARLAR
  â†’ GD zikzak yapmaz
  â†’ LR hassasiyeti azalÄ±r
  â†’ Convergence HIZLANIR

Ne zaman?
  â†’ Gradyan tabanlÄ± modellerde ÅAR T
  â†’ Linear, MLP, CNN, Transformer
  â†’ Tree-based'de gereksiz (RF, XGBoost)
```

### Early Stopping = Implicit Regularizer

```
Matematiksel:
  GD her step: w_t = w_0 - Î·t Ã— Î£âˆ‡L
  
  t kÃ¼Ã§Ã¼k â†’ w kÃ¼Ã§Ã¼k â†’ Implicit L2
  t bÃ¼yÃ¼k â†’ w bÃ¼yÃ¼k â†’ Overfit risk

Pratik:
  â†’ En gÃ¼Ã§lÃ¼ regularization
  â†’ Kolay implement
  â†’ %90 durumda yeterli
  
Hiperparametre:
  patience = 10  (10 epoch iyileÅŸme yoksa dur)
```

### DoÄŸru Metrik = Ä°ÅŸ Hedefi

```
Dengesiz SÄ±nÄ±f:
  âŒ Accuracy (yanÄ±ltÄ±r)
  âœ… Precision/Recall/F1
  âœ… PR-AUC

Regresyon:
  âœ… RMSE (birim anlamlÄ±)
  âœ… MAE (robust)
  âš ï¸ RÂ² (dikkatli yorumla)

EÅŸik SeÃ§imi:
  â†’ Ä°ÅŸ maliyetine gÃ¶re
  â†’ Precision vs Recall dengesi
  â†’ ROC curve'de optimal nokta
```

### Test Set KutsaldÄ±r

```
âŒ ASLA:
  - Test'e bakÄ±p ayar yapma
  - Test performansÄ± kÃ¶tÃ¼yse yeniden dene
  - Test'i "development" iÃ§in kullanma

âœ… SADECE:
  - Val ile ayarla
  - Val'de memnunsan
  - Test'e BÄ°R KEZ bak
  - Final rapor

Neden?
  â†’ Test overfitting'i Ã¶nle
  â†’ DÃ¼rÃ¼st deÄŸerlendirme
  â†’ GerÃ§ek dÃ¼nya performansÄ±
```

---

## ğŸ¯ Week 0 BaÅŸarÄ± Kriterleri

### Self-Check (Hepsine âœ“ koyabilir misin?)

```
â–¡ Train/Val/Test farkÄ±nÄ± aÃ§Ä±klayabiliyorum
â–¡ MSE/MAE/Huber ne zaman kullanÄ±lÄ±r biliyorum
â–¡ LR semptomlarÄ±nÄ± tanÄ±yabiliyorum
â–¡ Overfit/Underfit'i teÅŸhis edip Ã§Ã¶zebiliyorum
â–¡ Shape/Dtype/Device kontrolÃ¼nÃ¼ alÄ±ÅŸkanlÄ±k yaptÄ±m
â–¡ Data leakage tespiti yapabiliyorum
â–¡ DoÄŸru metrik seÃ§ebiliyorum (problem tipine gÃ¶re)
â–¡ Early stopping implement edebilirim
â–¡ L2 regularization'Ä±n ne iÅŸe yaradÄ±ÄŸÄ±nÄ± biliyorum
â–¡ Probabilistik kÃ¶kenleri anlÄ±yorum (MSEâ†Gaussian, L2â†Prior)

Hepsi âœ“ ise â†’ Week 0 BAÅARILI! ğŸ“
```

### Week 1 Hedefi

```
Lineer Regresyon:
  1. Sentetik veri oluÅŸtur (y = wx + b + Îµ)
  2. Manuel GD implement (autograd'sÄ±z)
  3. nn.Module ile training
  4. Train/Val split
  5. Early stopping
  6. L2 regularization
  7. Metrik: Val MSE < 0.5

Hedef:
  âœ“ Kod yazarken "neden" bilmek
  âœ“ Her satÄ±rÄ± theory'ye baÄŸlamak
  âœ“ Debug yaparken sistematik dÃ¼ÅŸÃ¼nmek
```

---

## ğŸ’¬ SonuÃ§

Week 0'nÄ±n hedefi olan temel kavramlarÄ± Ã¶zÃ¼msedim:

```
âœ… Model = Parametrik fonksiyon (f_Î¸)
âœ… Loss = Hata Ã¶lÃ§Ã¼sÃ¼ (MSE, CE, ...)
âœ… Optimizer = GÃ¼ncelleme algoritmasÄ± (GD, Adam)
âœ… Train/Val/Test = DÃ¼rÃ¼st deÄŸerlendirme sistemi
âœ… Overfit/Underfit = Ezber vs Yetersizlik
âœ… Tensor = Shape + Dtype + Device
âœ… LR = Hayat memat meselesi
âœ… Regularization = Overfit frenleyici
```

**YarÄ±n Week 1:**
- Lineer regresyon (theory â†’ practice)
- Bu temelleri KOD'a dÃ¶kme
- Val MSE < 0.5 hedefi
- Nedenini aÃ§Ä±klayarak!

**HazÄ±rÄ±m! ğŸ’ª**

---

