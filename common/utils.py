"""Shared utility functions across all weeks."""

import time
from collections.abc import Callable
from functools import wraps
from pathlib import Path
from typing import Any

import torch


def timer(func: Callable) -> Callable:
    """Decorator to measure function execution time."""

    @wraps(func)
    def wrapper(*args, **kwargs):
        start = time.perf_counter()
        result = func(*args, **kwargs)
        end = time.perf_counter()
        print(f"â±ï¸  {func.__name__} took {end - start:.4f}s")
        return result

    return wrapper


def get_device() -> torch.device:
    """
    Get the best available device for PyTorch.

    Priority: CUDA > MPS (Metal for M-series Macs) > CPU
    """
    if torch.cuda.is_available():
        device = torch.device("cuda")
        print(f"âœ… Using CUDA: {torch.cuda.get_device_name(0)}")
    elif torch.backends.mps.is_available():
        device = torch.device("mps")
        print("âœ… Using MPS (Metal Performance Shaders)")
    else:
        device = torch.device("cpu")
        print("âš ï¸  Using CPU (consider GPU for faster training)")
    return device


def set_seed(seed: int = 42) -> None:
    """Set random seed for reproducibility."""
    import random

    import numpy as np

    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)
    # MPS determinism is limited, best effort
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


def count_parameters(model: torch.nn.Module) -> int:
    """Count trainable parameters in a model."""
    return sum(p.numel() for p in model.parameters() if p.requires_grad)


def save_checkpoint(
    model: torch.nn.Module,
    optimizer: torch.optim.Optimizer,
    epoch: int,
    loss: float,
    path: Path | str,
) -> None:
    """Save model checkpoint."""
    path = Path(path)
    path.parent.mkdir(parents=True, exist_ok=True)

    checkpoint = {
        "epoch": epoch,
        "model_state_dict": model.state_dict(),
        "optimizer_state_dict": optimizer.state_dict(),
        "loss": loss,
    }
    torch.save(checkpoint, path)
    print(f"ðŸ’¾ Checkpoint saved: {path}")


def load_checkpoint(
    model: torch.nn.Module,
    optimizer: torch.optim.Optimizer | None,
    path: Path | str,
    device: torch.device,
) -> dict[str, Any]:
    """Load model checkpoint."""
    path = Path(path)
    if not path.exists():
        raise FileNotFoundError(f"Checkpoint not found: {path}")

    checkpoint = torch.load(path, map_location=device)
    model.load_state_dict(checkpoint["model_state_dict"])

    if optimizer is not None:
        optimizer.load_state_dict(checkpoint["optimizer_state_dict"])

    print(f"ðŸ“‚ Checkpoint loaded: {path} (epoch {checkpoint['epoch']})")
    return checkpoint


def print_section(title: str) -> None:
    """Print a formatted section header."""
    print(f"\n{'=' * 60}")
    print(f"  {title}")
    print(f"{'=' * 60}\n")
