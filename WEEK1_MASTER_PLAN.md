# üöÄ WEEK 1 ‚Äî MASTER PLAN (AI + CRYPTO, Paralel Sprint)

> **Hedef:** 5 g√ºnde hem AI hem Crypto hattƒ±nda **√ßalƒ±≈üan, √∂l√ß√ºlen, raporlanan** bir paket √ßƒ±karmak.  
> **Ritim:** T ‚Üí P ‚Üí X (Teori ‚Üí Pratik ‚Üí √úr√ºn).  
> **Prensip:** Read-only, testnet-first, idempotent, schema-driven, p95<1s.

---

## üìã ƒ∞√ßindekiler

1. [Haftalƒ±k Hedefler (DoD)](#haftalƒ±k-hedefler-definition-of-done)
2. [G√ºnl√ºk Detaylƒ± Plan](#g√ºnl√ºk-detaylƒ±-plan)
3. [Test & Kalite Kriterleri](#test--kalite-w1)
4. [Komut Kartƒ±](#komut-kartƒ±-cheatsheet)
5. [√áƒ±ktƒ± Klas√∂rleri](#√ßƒ±ktƒ±-klas√∂rleri)
6. [Risk Y√∂netimi](#risk--√∂nlem)
7. [Metrik Takibi](#skor-tahtasƒ±-takip-edilecek-metrikler)
8. [PR/Issue ≈ûablonlarƒ±](#prissue-≈üablonlarƒ±)
9. [Stretch Goals](#stretch-goals-opsiyonel)
10. [Kapanƒ±≈ü Kriteri](#kapanƒ±≈ü-kriteri)

---

## üéØ Haftalƒ±k Hedefler (Definition of Done)

### AI Hattƒ±: Linear Regression from Scratch

#### Teori (T)
- [ ] Gradient Descent manuel implementasyon anla≈üƒ±ldƒ±
- [ ] `nn.Module` API'si kullanƒ±mƒ± √∂ƒürenildi
- [ ] Train/Val split mantƒ±ƒüƒ± oturdu
- [ ] Early Stopping + L2 regularization prensipleri kavrandƒ±

#### Pratik (P)
- [ ] Manuel GD + `nn.Module` implementasyonu tamamlandƒ±
- [ ] Train/Val split uygulandƒ± (80/20)
- [ ] **Early Stopping** + **L2** regularization entegre edildi
- [ ] **Val MSE ‚â§ 0.50** (sentetik veri √ºzerinde, 1000 sample)
- [ ] Loss eƒürileri: `outputs/ai/w1_loss_curve.png` olu≈üturuldu
- [ ] Ablation study: LR sweep + L2 on/off + Early Stopping on/off

#### √úr√ºn (X)
- [ ] **Tests:** `pytest tests/test_linreg.py -q` ye≈üil (coverage ‚â• 70%)
- [ ] **Model checkpoint:** `outputs/ai/best_model.pt` kaydedildi
- [ ] **√ñzet rapor:** `reports/ai/w1_summary.md` (sonu√ß + ablation + grafikler)
- [ ] Kod temiz (ruff check ge√ßti)

**Ba≈üarƒ± Metrikleri:**
- Val MSE: **‚â§ 0.50** (hedef: 0.40-0.45)
- Convergence: **< 200 epoch**
- Test coverage: **‚â• 70%**

---

### Crypto Hattƒ±: Collector Loop + API Performance

#### Teori (T)
- [ ] Collector loop architecture (30s polling)
- [ ] Idempotent ingest + tail rescan patterns
- [ ] Cache optimization strategies (LRU+TTL)
- [ ] API performance targets (p95 < 1s)

#### Pratik (P)
- [ ] **Collector loop**: 30s polling, idempotent + tail rescan √ßalƒ±≈üƒ±yor
- [ ] **/wallet/{addr}/report** p95 **< 1s** (warm cache)
- [ ] **Schema v1** kontratƒ±na %100 uyum (validator ye≈üil)
- [ ] **Load test:** 100 req, c=10, error=0, p95<1s
- [ ] 5 test c√ºzdanƒ± i√ßin rapor √ºretildi

#### √úr√ºn (X)
- [ ] **Collector service:** Systemd/supervisor ile daemonize
- [ ] **API metrics:** p50/p95/p99 + cache hit ratio logged
- [ ] **√ñzet rapor:** `reports/crypto/w1_metrics.md` (perf + cache + notes)
- [ ] **Wallet reports:** `reports/crypto/wallets/<addr>.json` (5 c√ºzdan)

**Ba≈üarƒ± Metrikleri:**
- API p95 (warm cache): **< 1s** (hedef: 0.5-0.8s)
- Cache hit ratio: **> 70%** (hedef: 80-85%)
- Error rate: **0%**
- Collector lag: **< 100 blocks**

---

## üóìÔ∏è G√ºnl√ºk Detaylƒ± Plan

### **D1 ‚Äî Pazartesi: Kickoff & Baz √áizgisi**

**Hedef:** ƒ∞lk ko≈üu + baseline metrics + environment setup

#### AI (2-3 saat)

**Sabah (09:00-11:00):**
```bash
# 1. Workspace setup
cd week1_tensors
mkdir -p outputs/ai reports/ai

# 2. Sentetik veri olu≈ütur
python -c "
import torch
torch.manual_seed(42)
X = torch.randn(1000, 5)
w_true = torch.tensor([1.5, -2.0, 0.5, 3.0, -1.0])
b_true = 0.5
y = X @ w_true + b_true + torch.randn(1000) * 0.1
torch.save({'X': X, 'y': y}, 'data/synthetic_data.pt')
print('‚úÖ Sentetik veri hazƒ±r:', X.shape, y.shape)
"

# 3. ƒ∞lk training run
python linreg_manual.py --lr 0.01 --epochs 100 --seed 42

# 4. Loss curve kontrol√º
ls -lh outputs/ai/w1_loss_curve.png
```

**√áƒ±ktƒ±lar:**
- `data/synthetic_data.pt` - sentetik dataset
- `outputs/ai/w1_loss_curve.png` - ilk loss eƒürisi
- `reports/day1_ai.txt` - initial MSE deƒüeri

**Ba≈üarƒ± Kriteri:**
- [ ] Training tamamlandƒ± (hata vermedi)
- [ ] Loss curve d√º≈ü√ºyor (overfitting yok)
- [ ] Final train MSE < 1.0

---

#### Crypto (2-3 saat)

**√ñƒüleden Sonra (14:00-17:00):**
```bash
# 1. Environment check
make c.health
# Expected: RPC latency < 300ms, chain tip sync'd

# 2. Test wallets listesi olu≈ütur
cat > data/wallets_w1.txt <<EOF
0xd8dA6BF26964aF9D7eEd9e03E53415D37aA96045
0x0000000000000000000000000000000000000000
0x1111111111111111111111111111111111111111
0x2222222222222222222222222222222222222222
0x3333333333333333333333333333333333333333
EOF

# 3. Backfill last 5k blocks (idempotent)
make c.capture.idem BACKFILL=5000
# Expected: ~10-15 minutes, depends on RPC

# 4. DuckDB verification
duckdb onchain.duckdb -c "
SELECT 
  COUNT(*) AS total_transfers,
  MIN(block_number) AS min_block,
  MAX(block_number) AS max_block,
  COUNT(DISTINCT tx_hash) AS unique_txs
FROM transfers;
"

# 5. API start
make crypto.api
# Expected: Uvicorn starts on :8000

# 6. Baseline p95 measurement (cold cache)
WALLET=$(head -1 data/wallets_w1.txt)
hey -n 50 -c 5 "http://localhost:8000/wallet/$WALLET/report?hours=24" \
  | tee reports/day1_baseline_p95.txt

# 7. Warm cache test
hey -n 50 -c 5 "http://localhost:8000/wallet/$WALLET/report?hours=24" \
  | tee reports/day1_warm_p95.txt
```

**√áƒ±ktƒ±lar:**
- `data/wallets_w1.txt` - test wallet list
- `onchain.duckdb` - populated database (5k blocks)
- `reports/day1_baseline_p95.txt` - cold cache performance
- `reports/day1_warm_p95.txt` - warm cache performance

**Ba≈üarƒ± Kriteri:**
- [ ] Ingest completed without errors
- [ ] Database has > 0 transfers
- [ ] API returns valid JSON
- [ ] Baseline p95 measured (no target yet)

---

**G√ºnl√ºk √ñzet Rapor:**
```bash
# reports/day1.md
cat > reports/day1.md <<EOF
# Day 1 ‚Äî Kickoff & Baseline

## AI
- ‚úÖ Sentetik veri: 1000 samples, 5 features
- ‚úÖ ƒ∞lk training run: Train MSE = X.XX
- ‚úÖ Loss curve: outputs/ai/w1_loss_curve.png
- üìä Baseline: Train MSE = X.XX (hedef: < 1.0)

## Crypto
- ‚úÖ RPC health: latency = XXX ms
- ‚úÖ Ingest: 5k blocks, XXX transfers
- ‚úÖ API start: localhost:8000
- üìä Baseline p95 (cold): XXX ms
- üìä Baseline p95 (warm): XXX ms
- Cache hit ratio: X%

## Notlar
- [ ] AI: LR sweep yapƒ±lacak (D2)
- [ ] Crypto: Cache tuning ba≈ülayacak (D2)

## Riskler
- None (kickoff g√ºn√º)
EOF
```

---

### **D2 ‚Äî Salƒ±: Stabilizasyon & Mod√ºlerlik**

**Hedef:** Mod√ºler kod + Val split + Cache optimization ba≈ülangƒ±cƒ±

#### AI (3-4 saat)

```bash
# 1. nn.Module implementation
python linreg_module.py \
  --lr 0.01 \
  --l2 0.001 \
  --early_stop_patience 10 \
  --epochs 200 \
  --val_split 0.2 \
  --seed 42

# 2. Model checkpoint verification
python -c "
import torch
model = torch.load('outputs/ai/best_model.pt')
print('Model params:', model)
"

# 3. Train/Val MSE report
python scripts/eval_model.py \
  --model outputs/ai/best_model.pt \
  --data data/synthetic_data.pt \
  > reports/day2_ai_metrics.txt
```

**√áƒ±ktƒ±lar:**
- `week1_tensors/linreg_module.py` - nn.Module implementation
- `outputs/ai/best_model.pt` - checkpointed model
- `outputs/ai/w1_train_val_curve.png` - train/val split curves
- `reports/day2_ai_metrics.txt` - MSE summary

**Ba≈üarƒ± Kriteri:**
- [ ] Val MSE < 0.6 (improvement from baseline)
- [ ] Early stopping triggered (patience=10)
- [ ] Model checkpoint loads successfully

---

#### Crypto (3-4 saat)

```bash
# 1. Cache configuration tuning
export NOVA_CACHE_TTL=120  # 2 minutes
export NOVA_CACHE_CAPACITY=4096

# Restart API
pkill -f uvicorn
make crypto.api

# 2. Test all 5 wallets
while read wallet; do
  echo "Testing $wallet..."
  curl -s "http://localhost:8000/wallet/$wallet/report?hours=24" \
    | jq '.meta.generated_at, .tx_count' \
    | tee -a reports/day2_wallet_results.txt
  sleep 2
done < data/wallets_w1.txt

# 3. Cache hit ratio measurement
for i in {1..20}; do
  WALLET=$(shuf -n 1 data/wallets_w1.txt)
  curl -s "http://localhost:8000/wallet/$WALLET/report?hours=24" > /dev/null
  sleep 1
done

# Check health endpoint for cache stats
curl -s localhost:8000/healthz | jq '{
  cache_size,
  cache_hit_rate,
  uptime_s
}' | tee reports/day2_cache_stats.txt

# 4. Schema validation for all wallets
while read wallet; do
  echo "Validating $wallet..."
  python crypto/w0_bootstrap/report_json.py \
    --wallet $wallet \
    --hours 24 \
    --validate \
    || echo "VALIDATION FAILED: $wallet"
done < data/wallets_w1.txt
```

**√áƒ±ktƒ±lar:**
- `reports/day2_wallet_results.txt` - 5 wallet outputs
- `reports/day2_cache_stats.txt` - cache metrics
- Cache hit ratio measurement

**Ba≈üarƒ± Kriteri:**
- [ ] All 5 wallets return valid JSON
- [ ] Schema validation passes for all
- [ ] Cache hit ratio > 50%
- [ ] No 5xx errors

---

**G√ºnl√ºk √ñzet Rapor:**
```bash
cat > reports/day2.md <<EOF
# Day 2 ‚Äî Stabilization & Modularity

## AI
- ‚úÖ nn.Module implementation: linreg_module.py
- ‚úÖ Train/Val split: 800/200
- ‚úÖ Early Stopping: triggered at epoch XXX
- üìä Train MSE = X.XX, Val MSE = X.XX
- ‚úÖ Model checkpoint: best_model.pt

## Crypto
- ‚úÖ Cache tuning: TTL=120s, capacity=4096
- ‚úÖ 5 wallets tested: all valid JSON
- ‚úÖ Schema validation: 5/5 pass
- üìä Cache hit ratio: XX%
- üìä p95 (sample): XXX ms

## Notlar
- [ ] AI: LR sweep tomorrow (D3)
- [ ] Crypto: Load test tomorrow (D3)

## Riskler
- None
EOF
```

---

### **D3 ‚Äî √áar≈üamba: Performans & Ablation**

**Hedef:** Comprehensive testing + ablation study + performance optimization

#### AI (3-4 saat)

```bash
# 1. LR sweep
for lr in 0.1 0.05 0.01 0.005 0.001; do
  echo "Testing LR=$lr..."
  python linreg_module.py \
    --lr $lr \
    --l2 0.001 \
    --early_stop_patience 10 \
    --epochs 200 \
    --val_split 0.2 \
    --seed 42 \
    --output_prefix "lr_${lr}"
done

# 2. Ablation: L2 on/off
python linreg_module.py --lr 0.01 --l2 0.0 --output_prefix "no_l2"
python linreg_module.py --lr 0.01 --l2 0.001 --output_prefix "with_l2"

# 3. Ablation: Early Stopping on/off
python linreg_module.py --lr 0.01 --l2 0.001 --early_stop_patience 0 --output_prefix "no_early"
python linreg_module.py --lr 0.01 --l2 0.001 --early_stop_patience 10 --output_prefix "with_early"

# 4. Generate ablation report
python scripts/ablation_report.py \
  --results_dir outputs/ai \
  --output reports/ai/w1_ablation.md
```

**√áƒ±ktƒ±lar:**
- `outputs/ai/lr_*_loss_curve.png` - LR sweep curves
- `outputs/ai/no_l2_vs_with_l2.png` - L2 comparison
- `outputs/ai/no_early_vs_with_early.png` - Early stopping comparison
- `reports/ai/w1_ablation.md` - comprehensive ablation report

**Ba≈üarƒ± Kriteri:**
- [ ] Best LR identified (lowest Val MSE)
- [ ] L2 impact quantified
- [ ] Early stopping benefit demonstrated
- [ ] Val MSE < 0.55

---

#### Crypto (3-4 saat)

```bash
# 1. Load test (cold cache)
pkill -f uvicorn
rm -f /tmp/cache.pkl  # If file-based cache
make crypto.api
sleep 5

WALLET=$(head -1 data/wallets_w1.txt)
hey -n 200 -c 20 "http://localhost:8000/wallet/$WALLET/report?hours=24" \
  | tee reports/day3_cold_load.txt

# 2. Load test (warm cache)
hey -n 200 -c 20 "http://localhost:8000/wallet/$WALLET/report?hours=24" \
  | tee reports/day3_warm_load.txt

# 3. Concurrent wallet load
seq 1 50 | xargs -P 10 -I {} sh -c '
  WALLET=$(shuf -n 1 data/wallets_w1.txt)
  curl -s "http://localhost:8000/wallet/$WALLET/report?hours=24" > /dev/null
'

# Check cache stats
curl -s localhost:8000/healthz | jq

# 4. TTL/Capacity tuning experiment
for TTL in 60 120 300; do
  for CAP in 1024 2048 4096; do
    echo "Testing TTL=$TTL, CAP=$CAP..."
    export NOVA_CACHE_TTL=$TTL
    export NOVA_CACHE_CAPACITY=$CAP
    pkill -f uvicorn
    make crypto.api
    sleep 5
    
    hey -n 100 -c 10 "http://localhost:8000/wallet/$WALLET/report?hours=24" \
      | grep "95%" \
      | tee -a reports/day3_cache_tuning.txt
  done
done

# 5. Identify optimal config
cat reports/day3_cache_tuning.txt | sort -k2 -n | head -1
```

**√áƒ±ktƒ±lar:**
- `reports/day3_cold_load.txt` - cold cache load test
- `reports/day3_warm_load.txt` - warm cache load test
- `reports/day3_cache_tuning.txt` - TTL/capacity experiment
- `reports/crypto/w1_perf_summary.md` - performance analysis

**Ba≈üarƒ± Kriteri:**
- [ ] p95 (warm cache) < 1s ‚úÖ
- [ ] Error rate = 0%
- [ ] Cache hit ratio > 70%
- [ ] Optimal TTL/capacity identified

---

**G√ºnl√ºk √ñzet Rapor:**
```bash
cat > reports/day3.md <<EOF
# Day 3 ‚Äî Performance & Ablation

## AI
- ‚úÖ LR sweep: tested {0.1, 0.05, 0.01, 0.005, 0.001}
- ‚úÖ Best LR: X.XX (Val MSE = X.XX)
- ‚úÖ L2 ablation: improvement = X.XX MSE
- ‚úÖ Early stopping ablation: saves XX epochs
- üìä Best Val MSE = X.XX (target: < 0.55)

## Crypto
- ‚úÖ Load test (cold): p95 = XXX ms
- ‚úÖ Load test (warm): p95 = XXX ms ‚úÖ (< 1s)
- ‚úÖ Cache hit ratio: XX% (> 70%)
- ‚úÖ Error rate: 0%
- ‚úÖ Optimal config: TTL=XXX, CAP=XXXX

## Notlar
- [ ] AI: Final cleanup tomorrow (D4)
- [ ] Crypto: Error scenarios tomorrow (D4)

## Riskler
- None
EOF
```

---

### **D4 ‚Äî Per≈üembe: Sertle≈ütirme & Kenar Durumlar**

**Hedef:** Error handling + edge cases + code quality

#### AI (2-3 saat)

```bash
# 1. Code cleanup
ruff check week1_tensors/ --fix
ruff format week1_tensors/

# 2. Test suite
pytest tests/test_linreg.py -v --cov=week1_tensors --cov-report=term-missing

# 3. Edge cases
python scripts/test_edge_cases.py

# 4. Documentation
python scripts/generate_ai_summary.py > reports/ai/w1_summary.md
```

**√áƒ±ktƒ±lar:**
- Clean code (ruff pass)
- Test coverage report
- `reports/ai/w1_summary.md` draft

**Ba≈üarƒ± Kriteri:**
- [ ] Ruff: 0 errors
- [ ] Tests: all pass
- [ ] Coverage ‚â• 70%

---

#### Crypto (3-4 saat)

```bash
# 1. Error scenario: RPC 429
# Simulate by reducing rate limits (if possible) or manual trigger
python crypto/tests/simulate_429.py
# Follow Runbook R1 to recover
# Log: reports/day4_error_429.txt

# 2. Error scenario: Reorg
# Simulate by clearing last 20 blocks and re-ingesting
duckdb onchain.duckdb -c "
DELETE FROM transfers WHERE block_number > (SELECT MAX(block_number) - 20 FROM transfers);
UPDATE scan_state SET last_scanned_block = last_scanned_block - 20;
"
make c.capture.idem BACKFILL=30
# Verify: no duplicates, no gaps
./crypto/scripts/data_quality_check.sh onchain.duckdb
# Log: reports/day4_error_reorg.txt

# 3. Error scenario: Bad window (10k limit)
# Test with very large window
curl -s "http://localhost:8000/wallet/0x0000000000000000000000000000000000000000/report?hours=720"
# Should handle gracefully or return error
# Log: reports/day4_error_window.txt

# 4. Contract tests
pytest tests/contract/test_report_schema.py -v

# 5. Schema validation CI
python crypto/w0_bootstrap/report_json.py \
  --wallet $(head -1 data/wallets_w1.txt) \
  --hours 24 \
  --validate \
  || exit 1
```

**√áƒ±ktƒ±lar:**
- `reports/day4_error_*.txt` - error scenario logs
- Contract test results
- Data quality check pass

**Ba≈üarƒ± Kriteri:**
- [ ] All error scenarios recovered successfully
- [ ] Contract tests pass
- [ ] Data quality checks pass
- [ ] No regressions introduced

---

**G√ºnl√ºk √ñzet Rapor:**
```bash
cat > reports/day4.md <<EOF
# Day 4 ‚Äî Hardening & Edge Cases

## AI
- ‚úÖ Code cleanup: ruff pass
- ‚úÖ Tests: XX/XX pass (coverage = XX%)
- ‚úÖ Edge cases tested
- ‚úÖ w1_summary.md draft ready

## Crypto
- ‚úÖ Error scenario 1: RPC 429 ‚Üí recovered via Runbook R1
- ‚úÖ Error scenario 2: Reorg ‚Üí tail rescan successful
- ‚úÖ Error scenario 3: Large window ‚Üí handled gracefully
- ‚úÖ Contract tests: 12/12 pass
- ‚úÖ Data quality: all checks pass

## Notlar
- [ ] Tomorrow (D5): Final run + ship

## Riskler
- None
EOF
```

---

### **D5 ‚Äî Cuma: Ship Day**

**Hedef:** Final run + documentation + release

#### AI (2 saat)

```bash
# 1. Final training run with best params
python linreg_module.py \
  --lr 0.01 \
  --l2 0.001 \
  --early_stop_patience 10 \
  --epochs 200 \
  --val_split 0.2 \
  --seed 42 \
  --output_prefix "final"

# 2. Verify Val MSE ‚â§ 0.50
python -c "
import torch
checkpoint = torch.load('outputs/ai/final_best_model.pt')
print(f'Val MSE: {checkpoint[\"val_mse\"]:.4f}')
assert checkpoint['val_mse'] <= 0.50, 'Val MSE target not met!'
print('‚úÖ Val MSE target met!')
"

# 3. Finalize summary report
python scripts/generate_ai_summary.py \
  --include_graphs \
  --include_ablation \
  > reports/ai/w1_summary.md

# 4. Archive outputs
tar -czf outputs/ai/w1_final.tar.gz outputs/ai/*.png outputs/ai/*.pt
```

**√áƒ±ktƒ±lar:**
- `outputs/ai/final_best_model.pt` - final model checkpoint
- `reports/ai/w1_summary.md` - complete summary with graphs
- `outputs/ai/w1_final.tar.gz` - archived outputs

**Ba≈üarƒ± Kriteri:**
- [x] Val MSE ‚â§ 0.50 ‚úÖ
- [x] Summary report complete
- [x] All graphs included

---

#### Crypto (3 saat)

```bash
# 1. Generate reports for all 5 wallets
mkdir -p reports/crypto/wallets
while read wallet; do
  echo "Generating report for $wallet..."
  python crypto/w0_bootstrap/report_json.py \
    --wallet $wallet \
    --hours 24 \
    --output "reports/crypto/wallets/${wallet}.json" \
    --validate
done < data/wallets_w1.txt

# 2. Final performance measurement
WALLET=$(head -1 data/wallets_w1.txt)
hey -n 200 -c 20 "http://localhost:8000/wallet/$WALLET/report?hours=24" \
  > reports/day5_final_perf.txt

# Extract metrics
cat reports/day5_final_perf.txt | grep -E "(p50|p95|p99)"

# 3. Generate w1_metrics.md
python scripts/generate_crypto_summary.py \
  --perf_file reports/day5_final_perf.txt \
  --cache_stats "$(curl -s localhost:8000/healthz)" \
  > reports/crypto/w1_metrics.md

# 4. Data quality final check
./crypto/scripts/data_quality_check.sh onchain.duckdb \
  | tee reports/day5_data_quality.txt

# 5. Archive
tar -czf reports/crypto/w1_wallets.tar.gz reports/crypto/wallets/
```

**√áƒ±ktƒ±lar:**
- `reports/crypto/wallets/<addr>.json` - 5 wallet reports
- `reports/crypto/w1_metrics.md` - performance summary
- `reports/day5_final_perf.txt` - final load test results
- `reports/day5_data_quality.txt` - quality check results

**Ba≈üarƒ± Kriteri:**
- [x] 5 wallets reports generated ‚úÖ
- [x] p95 < 1s ‚úÖ
- [x] Cache hit ratio > 70% ‚úÖ
- [x] Error rate = 0% ‚úÖ
- [x] w1_metrics.md complete

---

#### Release (1 saat)

```bash
# 1. Create WEEK1_CLOSEOUT.md
cat > reports/WEEK1_CLOSEOUT.md <<EOF
# Week 1 Closeout ‚Äî AI + Crypto Sprint

## üéØ Hedefler & Ba≈üarƒ±

### AI: Linear Regression from Scratch
- ‚úÖ Val MSE: **X.XX** (hedef: ‚â§ 0.50)
- ‚úÖ Convergence: XX epochs (hedef: < 200)
- ‚úÖ Tests: XX/XX pass (coverage: XX%)
- ‚úÖ Ablation: LR=X.XX, L2=X.XXX optimal

### Crypto: Collector + API Performance
- ‚úÖ API p95 (warm): **XXX ms** (hedef: < 1s)
- ‚úÖ Cache hit ratio: **XX%** (hedef: > 70%)
- ‚úÖ Error rate: **0%**
- ‚úÖ Load test: 200 req, 0 errors

## üìä Metrikler

| Kategori   | Metrik          | Hedef     | Ger√ßekle≈üen | Durum |
| ---------- | --------------- | --------- | ----------- | ----- |
| AI         | Val MSE         | ‚â§ 0.50    | X.XX        | ‚úÖ    |
| AI         | Convergence     | < 200 ep  | XX          | ‚úÖ    |
| Crypto     | API p95 (warm)  | < 1s      | XXX ms      | ‚úÖ    |
| Crypto     | Cache hit ratio | > 70%     | XX%         | ‚úÖ    |
| Crypto     | Error rate      | 0%        | 0%          | ‚úÖ    |

## üì¶ Deliverables

### AI
- ‚úÖ Code: \`week1_tensors/linreg_module.py\`
- ‚úÖ Model: \`outputs/ai/final_best_model.pt\`
- ‚úÖ Report: \`reports/ai/w1_summary.md\`
- ‚úÖ Graphs: \`outputs/ai/*.png\`

### Crypto
- ‚úÖ Collector: \`crypto/collector/\`
- ‚úÖ API: \`crypto/service/\`
- ‚úÖ Reports: \`reports/crypto/wallets/*.json\`
- ‚úÖ Metrics: \`reports/crypto/w1_metrics.md\`

## üîó Links

- [AI Summary](./ai/w1_summary.md)
- [Crypto Metrics](./crypto/w1_metrics.md)
- [Wallet Reports](./crypto/wallets/)
- [Daily Reports](./day1.md) ... [day5.md](./day5.md)

## üöÄ Next Steps (Week 2)

### AI
- [ ] Logistic Regression (binary classification)
- [ ] Multi-class classification
- [ ] Cross-entropy loss

### Crypto
- [ ] Price integration (CoinGecko API)
- [ ] USD value calculation
- [ ] Historical snapshots

## üìù Lessons Learned

### What Went Well
- Clear daily goals
- Parallel AI + Crypto tracks
- Automated testing
- Performance targets met

### What Could Improve
- [Add specific improvements]

### Surprises
- [Add surprising findings]
EOF

# 2. Update CHANGELOG
cat >> CHANGELOG.md <<EOF

## [1.2.0-w1] - $(date +%Y-%m-%d)

### AI Track
- Linear Regression from scratch (manual GD + nn.Module)
- Train/Val split + Early Stopping + L2 regularization
- Val MSE achieved: X.XX (target: ‚â§ 0.50)

### Crypto Track
- Collector loop (30s polling, idempotent + tail rescan)
- API performance optimized (p95 < 1s warm cache)
- 5 wallet reports generated
- Cache hit ratio: XX%

### Infrastructure
- Test coverage improved (‚â• 70%)
- Performance testing automated
- Error handling hardened
EOF

# 3. Git tag
git add -A
git commit -m "feat(w1): Complete Week 1 Sprint - AI LR (MSE=X.XX) + Crypto API (p95=XXXms)

AI Track:
- Linear Regression from scratch
- Val MSE: X.XX (target: ‚â§ 0.50) ‚úÖ
- Convergence: XX epochs
- Ablation study complete

Crypto Track:
- API p95 (warm): XXX ms (< 1s) ‚úÖ
- Cache hit ratio: XX% (> 70%) ‚úÖ
- Error rate: 0%
- 5 wallet reports generated

Deliverables:
- reports/WEEK1_CLOSEOUT.md
- reports/ai/w1_summary.md
- reports/crypto/w1_metrics.md
- outputs/ai/final_best_model.pt
- reports/crypto/wallets/*.json
"

git tag -a v1.2.0-w1 -m "Week 1 Release: AI + Crypto Sprint Complete"
git push && git push --tags

echo "üéâ Week 1 Complete! Tag: v1.2.0-w1"
```

---

## üß™ Test & Kalite (W1)

### AI Tests
```bash
# Unit tests
pytest tests/test_linreg.py -v

# Coverage
pytest tests/test_linreg.py --cov=week1_tensors --cov-report=html

# Linting
ruff check week1_tensors/
ruff format week1_tensors/ --check
```

**Target:**
- Tests: **all pass**
- Coverage: **‚â• 70%**
- Ruff: **0 errors**

### Crypto Tests
```bash
# Unit tests
pytest tests/unit/ -v

# Contract tests (schema)
pytest tests/contract/ -v

# Integration tests
pytest tests/integration/ -v

# Load test
hey -n 200 -c 20 "http://localhost:8000/wallet/$W/report?hours=24"
```

**Target:**
- Tests: **all pass**
- Schema validation: **100%**
- Load test error rate: **0%**
- p95 (warm cache): **< 1s**

---

## üß∞ Komut Kartƒ± (Cheatsheet)

### AI Quick Commands
```bash
# Training
python linreg_module.py --lr 0.01 --l2 0.001 --early_stop_patience 10

# Evaluation
python scripts/eval_model.py --model outputs/ai/best_model.pt

# Tests
pytest tests/test_linreg.py -q

# Lint
ruff check week1_tensors/ --fix
```

### Crypto Quick Commands
```bash
# Health check
make c.health
curl -s localhost:8000/healthz | jq

# Ingest (backfill 5k blocks)
make c.capture.idem BACKFILL=5000

# API start
make crypto.api

# Single wallet report
WALLET=0xd8dA6BF26964aF9D7eEd9e03E53415D37aA96045
curl -s "http://localhost:8000/wallet/$WALLET/report?hours=24" | jq

# Load test
hey -n 200 -c 20 "http://localhost:8000/wallet/$WALLET/report?hours=24"

# Schema validation
python crypto/w0_bootstrap/report_json.py --wallet $WALLET --hours 24 --validate

# Data quality
./crypto/scripts/data_quality_check.sh onchain.duckdb
```

### Combined Workflow
```bash
# Morning routine
make c.health                    # Check RPC
make c.capture.idem BACKFILL=100 # Catch up
make crypto.api                  # Start API
pytest tests/ -q                 # Run all tests

# Evening routine
git add -A
git commit -m "day X: progress update"
tar -czf backups/day$(date +%d).tar.gz outputs/ reports/
```

---

## üì¶ √áƒ±ktƒ± Klas√∂rleri

```
novadev-protocol/
‚îú‚îÄ‚îÄ outputs/
‚îÇ   ‚îî‚îÄ‚îÄ ai/
‚îÇ       ‚îú‚îÄ‚îÄ w1_loss_curve.png
‚îÇ       ‚îú‚îÄ‚îÄ w1_train_val_curve.png
‚îÇ       ‚îú‚îÄ‚îÄ w1_ablation.png
‚îÇ       ‚îú‚îÄ‚îÄ final_best_model.pt
‚îÇ       ‚îî‚îÄ‚îÄ w1_final.tar.gz
‚îÇ
‚îú‚îÄ‚îÄ reports/
‚îÇ   ‚îú‚îÄ‚îÄ day1.md
‚îÇ   ‚îú‚îÄ‚îÄ day2.md
‚îÇ   ‚îú‚îÄ‚îÄ day3.md
‚îÇ   ‚îú‚îÄ‚îÄ day4.md
‚îÇ   ‚îú‚îÄ‚îÄ day5.md
‚îÇ   ‚îú‚îÄ‚îÄ WEEK1_CLOSEOUT.md
‚îÇ   ‚îú‚îÄ‚îÄ ai/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ w1_summary.md
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ w1_ablation.md
‚îÇ   ‚îî‚îÄ‚îÄ crypto/
‚îÇ       ‚îú‚îÄ‚îÄ w1_metrics.md
‚îÇ       ‚îú‚îÄ‚îÄ wallets/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ 0xd8dA....json
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ 0x0000....json
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ ... (5 total)
‚îÇ       ‚îî‚îÄ‚îÄ w1_wallets.tar.gz
‚îÇ
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ synthetic_data.pt
‚îÇ   ‚îî‚îÄ‚îÄ wallets_w1.txt
‚îÇ
‚îî‚îÄ‚îÄ logs/
    ‚îî‚îÄ‚îÄ api/
        ‚îî‚îÄ‚îÄ YYYY-MM-DD.jsonl
```

---

## ‚ö†Ô∏è Risk ‚Üí √ñnlem

### AI Risks

| Risk | Symptom | Mitigation |
|------|---------|------------|
| **Overfitting** | Train MSE ‚Üì, Val MSE ‚Üë | Increase L2, reduce epochs, early stopping |
| **Underfitting** | Both MSE high | Increase LR, more epochs, reduce L2 |
| **Divergence** | Loss ‚Üí NaN or ‚àû | Reduce LR, check data normalization |
| **Slow convergence** | Loss plateau | Increase LR, check gradients |

### Crypto Risks

| Risk | Symptom | Mitigation |
|------|---------|------------|
| **RPC 429** | Rate limit errors | AIMD window ‚Üì, token bucket ‚Üë, backoff+jitter |
| **RPC Timeout** | Slow responses | Retry logic, backup RPC, reduce window size |
| **DB Lock** | Write errors | API read-only, single writer process |
| **Cache Stampede** | p95 spike after restart | Warmup script, single-flight pattern |
| **Schema Drift** | Validation fails | Schema-check CI, version strategy |
| **Memory Leak** | RSS grows | Profile, fix leaks, restart schedule |

### General Risks

| Risk | Symptom | Mitigation |
|------|---------|------------|
| **Time Crunch** | Tasks not finished | Prioritize DoD items, defer stretch goals |
| **Scope Creep** | Too many features | Stick to master plan, defer to W2 |
| **Integration Issues** | Tests fail | Incremental integration, CI checks |

---

## üìä Skor Tahtasƒ± (Takip Edilecek Metrikler)

### AI Metrics

| Metrik | Baseline (D1) | Target (D5) | Actual (D5) | Status |
|--------|---------------|-------------|-------------|--------|
| Train MSE | 0.80 | < 0.45 | X.XX | ‚è≥ |
| Val MSE | 0.85 | ‚â§ 0.50 | X.XX | ‚è≥ |
| Convergence (epochs) | 150 | < 200 | XX | ‚è≥ |
| Test Coverage | 50% | ‚â• 70% | XX% | ‚è≥ |

### Crypto Metrics

| Metrik | Baseline (D1) | Target (D5) | Actual (D5) | Status |
|--------|---------------|-------------|-------------|--------|
| API p95 (cold cache) | 2.5s | N/A | XXX ms | ‚è≥ |
| API p95 (warm cache) | 1.2s | < 1s | XXX ms | ‚è≥ |
| Cache hit ratio | 30% | > 70% | XX% | ‚è≥ |
| Error rate | 2% | 0% | X% | ‚è≥ |
| Collector lag (blocks) | 200 | < 100 | XX | ‚è≥ |

### Quality Metrics

| Metrik | Target | Actual | Status |
|--------|--------|--------|--------|
| Tests Pass | 100% | XX% | ‚è≥ |
| Ruff Errors | 0 | X | ‚è≥ |
| Schema Validation | 100% | XX% | ‚è≥ |
| Data Quality Checks | All pass | X/5 | ‚è≥ |

---

## üß© PR/Issue ≈ûablonlarƒ±

### Pull Request Template

```markdown
# Week 1 Sprint: [AI/Crypto] [Feature]

## üéØ Objective
[Brief description of what this PR accomplishes]

## üìä Metrics
- **AI:** Val MSE = X.XX (target: ‚â§ 0.50)
- **Crypto:** p95 = XXX ms (target: < 1s)
- **Tests:** XX/XX pass
- **Coverage:** XX%

## üîÑ Changes
- [ ] Feature 1
- [ ] Feature 2
- [ ] Tests added/updated
- [ ] Documentation updated

## üß™ Testing
```bash
# AI
pytest tests/test_linreg.py -v

# Crypto
pytest tests/contract/ tests/integration/ -v
hey -n 100 -c 10 "http://localhost:8000/wallet/$W/report?hours=24"
```

## üì∏ Screenshots
[Attach graphs, performance charts, etc.]

## ‚ö†Ô∏è Risks
- [Any risks or concerns]

## üîó Related
- Closes #X
- Related to #Y
```

### Issue Templates

#### AI Issue
```markdown
# [W1-AI] Feature/Bug Name

## Description
[Detailed description]

## Expected Behavior
[What should happen]

## Actual Behavior
[What actually happens]

## Steps to Reproduce
1. Step 1
2. Step 2

## Metrics
- Train MSE: X.XX
- Val MSE: X.XX

## Acceptance Criteria
- [ ] Criterion 1
- [ ] Criterion 2
```

#### Crypto Issue
```markdown
# [W1-CRYPTO] Feature/Bug Name

## Description
[Detailed description]

## Current Behavior
[What's happening now]

## Desired Behavior
[What should happen]

## Metrics
- p95: XXX ms
- Cache hit ratio: XX%
- Error rate: X%

## Acceptance Criteria
- [ ] Criterion 1
- [ ] Criterion 2
```

---

## üå± Stretch Goals (Opsiyonel)

### AI Stretch Goals
- [ ] Implement batch gradient descent
- [ ] Add momentum optimizer
- [ ] Learning rate scheduler
- [ ] Feature normalization
- [ ] Cross-validation (k-fold)

### Crypto Stretch Goals
- [ ] Prometheus `/metrics` endpoint
- [ ] Grafana dashboard
- [ ] Canary deployment (A/B test)
- [ ] Dockerfile + docker-compose
- [ ] Horizontal scaling (load balancer)
- [ ] Price API integration (CoinGecko)
- [ ] USD value in reports

### Infrastructure Stretch Goals
- [ ] Pre-commit hooks
- [ ] GitHub Actions CI/CD
- [ ] Code coverage badges
- [ ] Automated deployment script
- [ ] Monitoring alerts (PagerDuty/Slack)

---

## ‚úÖ Kapanƒ±≈ü Kriteri (Cuma 18:00 TR Time)

### AI Track DoD
- [ ] Val MSE ‚â§ 0.50 ‚úÖ
- [ ] Tests all pass (coverage ‚â• 70%)
- [ ] Model checkpoint saved (`outputs/ai/final_best_model.pt`)
- [ ] Summary report complete (`reports/ai/w1_summary.md`)
- [ ] Graphs generated (loss curves, ablation)
- [ ] Code clean (ruff pass)

### Crypto Track DoD
- [ ] API p95 < 1s (warm cache) ‚úÖ
- [ ] Load test: 100 req, error=0
- [ ] Cache hit ratio > 70%
- [ ] 5 wallet reports generated (`reports/crypto/wallets/*.json`)
- [ ] Metrics report complete (`reports/crypto/w1_metrics.md`)
- [ ] Schema validation: 100% pass
- [ ] Data quality checks: all pass

### Release DoD
- [ ] `WEEK1_CLOSEOUT.md` complete
- [ ] CHANGELOG updated
- [ ] Git tag created: `v1.2.0-w1`
- [ ] All commits pushed
- [ ] Tag pushed

### Verification Commands
```bash
# AI
python -c "
import torch
checkpoint = torch.load('outputs/ai/final_best_model.pt')
assert checkpoint['val_mse'] <= 0.50, f'Val MSE {checkpoint[\"val_mse\"]:.4f} > 0.50'
print('‚úÖ AI DoD met: Val MSE =', checkpoint['val_mse'])
"

# Crypto
WALLET=$(head -1 data/wallets_w1.txt)
P95=$(hey -n 100 -c 10 "http://localhost:8000/wallet/$WALLET/report?hours=24" 2>&1 | grep "95%" | awk '{print $2}')
echo "p95: $P95"
if (( $(echo "$P95 < 1.0" | bc -l) )); then
  echo "‚úÖ Crypto DoD met: p95 < 1s"
else
  echo "‚ùå Crypto DoD NOT met: p95 >= 1s"
fi

# Release
git tag | grep v1.2.0-w1 && echo "‚úÖ Release DoD met: tag exists"
```

---

## üìù Notlar

### Timezone
- T√ºm saatler **TR (Europe/Istanbul)** lokal planlandƒ±
- Veride **UTC** kullanƒ±lƒ±r (code standardƒ±)
- Timestamp format: ISO 8601 with `Z` suffix

### Security
- **Private keys:** NEVER in code/config
- **Read-only:** API ve analysis only
- **Testnet:** Sepolia √∂ncelikli
- **Environment:** `.env` files in `.gitignore`

### Documentation
- G√ºnl√ºk raporlar: `reports/dayX.md`
- Weekly closeout: `reports/WEEK1_CLOSEOUT.md`
- API docs: `crypto/API_GUIDE.md`
- Troubleshooting: `crypto/docs/w0_bootstrap/10_tahta_troubleshoot_runbooks.md`

### Communication
- Daily standup: 09:00 TR (optional)
- End-of-day summary: 18:00 TR
- Blockers: immediate Slack/Discord ping
- Wins: celebrate in team channel!

---

**Version:** 1.0  
**Last Updated:** 2025-10-06  
**Status:** Active (Week 1)  
**Next:** Week 2 Planning (Logistic Regression + Price Integration)

---

üéØ **Let's ship it!** Week 1 begins Monday 09:00 TR. Ready? üöÄ

